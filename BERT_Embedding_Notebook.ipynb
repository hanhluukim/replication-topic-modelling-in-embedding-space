{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_Embedding_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN9waX5wCQSwDsUjBSmdC9h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanhluukim/replication-topic-modelling-in-embedding-space/blob/main/BERT_Embedding_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEt5DyniPBcW",
        "outputId": "c6d08f65-23b5-4bb7-c237-2ce02be24a5c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 8.2 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 18.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 44.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stop-words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-I7OQILPvGv",
        "outputId": "502f2f25-7d6a-4fdf-90a3-837de19459e6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stop-words\n",
            "  Downloading stop-words-2018.7.23.tar.gz (31 kB)\n",
            "Building wheels for collected packages: stop-words\n",
            "  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stop-words: filename=stop_words-2018.7.23-py3-none-any.whl size=32911 sha256=7e2febffa6082f77f90ac03536c8bf5382d6a007fd03e03730e0e13843564fa7\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/86/b2/277b10b1ce9f73ce15059bf6975d4547cc4ec3feeb651978e9\n",
            "Successfully built stop-words\n",
            "Installing collected packages: stop-words\n",
            "Successfully installed stop-words-2018.7.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using bert-model to create static the word-embedding\n",
        "# https://datascience.stackexchange.com/questions/85566/how-pre-trained-bert-model-generates-word-embeddings-for-out-of-vocabulary-words\n",
        "\n",
        "from distutils.util import split_quoted\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "\n",
        "#with open('src/stops.txt', 'r') as f:\n",
        "#    stops = f.read().split('\\n')\n",
        "\n",
        "from stop_words import get_stop_words\n",
        "stop_words = get_stop_words('en')\n",
        "    \n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "                                  )\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "8EJnBCguQeWE",
        "outputId": "1f467d36-86c0-435c-a63c-a5a361ba52bc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0d8bff545512>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mnewsgroups_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m model = BertModel.from_pretrained('bert-base-uncased',\n\u001b[1;32m     25\u001b[0m                                   \u001b[0moutput_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Whether the model returns all hidden-states.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1768\u001b[0m             raise EnvironmentError(\n\u001b[0;32m-> 1769\u001b[0;31m                 \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1770\u001b[0m                 \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m                 \u001b[0;34mf\"Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'bert-base-uncased'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'bert-base-uncased' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX21MYTEObtF"
      },
      "outputs": [],
      "source": [
        "# read train_data from 20newsgroups\n",
        "def read_raw_documents():\n",
        "    raw_documents = []\n",
        "    raw_labels = []\n",
        "    for i in range(0,len(newsgroups_train.data)):\n",
        "        raw_documents.append(newsgroups_train.data[i])\n",
        "        raw_labels.append(newsgroups_train.target[i])\n",
        "    return raw_documents, raw_labels\n",
        "\n",
        "def simple_preprocess(raw_documents):\n",
        "    def only_letters(tested_string):\n",
        "        for letter in tested_string:\n",
        "            if letter not in \"abcdefghijklmnopqrstuvwxyz\":\n",
        "                return False\n",
        "        return True\n",
        "    def clean_doc_for_bert(doc): \n",
        "        doc = doc.replace(\">\",\"\").lower()\n",
        "        word_list = word_tokenize(doc) #only using empty space and punctation for tokenization\n",
        "        cleaned = []\n",
        "        for w in word_list:\n",
        "            if w not in stop_words:\n",
        "                if w in string.punctuation or only_letters(w): #using only character from punctation and alpha characters\n",
        "                    if w in string.punctuation or len( set(w) ) > 1: #punctation with len 1 allowed but alpha word must be longer then 1\n",
        "                        cleaned.append( w)\n",
        "        return \" \".join(cleaned), cleaned  #save doc in string and in token-list         \n",
        "       \n",
        "    cleaned_documents = []\n",
        "    for doc in raw_documents:\n",
        "        doc_in_string, doc_in_token_list = clean_doc_for_bert(doc)\n",
        "        cleaned_documents.append(doc_in_string)\n",
        "    return cleaned_documents\n",
        "\n",
        "def transform_to_sentences_with_labels():\n",
        "    # we will not use labels\n",
        "    sentences_with_labels = []\n",
        "    return sentences_with_labels\n",
        "\n",
        "def fine_tune_bert():\n",
        "    # should to be trained?\n",
        "    # no, because for topic modelling, that is usupervised problem. We just find topics for the documents\n",
        "    # topic modelling no targets\n",
        "    return True\n",
        "\n",
        "def transform_to_sentences(docs): #no labels\n",
        "    data_as_sentences = []\n",
        "    for doc in docs:\n",
        "      for sent in doc.split(\". \"): #make sentences\n",
        "        updated_sent = \" \".join([t for t in sent.split(\" \") if len(t) > 1])\n",
        "        if len(updated_sent.split(\" \")) > 1:\n",
        "            data_as_sentences.append(updated_sent)\n",
        "        else:\n",
        "            if updated_sent not in data_as_sentences:\n",
        "                data_as_sentences.append(updated_sent)\n",
        "    return data_as_sentences\n",
        "\n",
        "def split_long_sentence(splitted_sent, given_len):\n",
        "    subsents = []\n",
        "   #for i in range(0,len(splitted_sent), given_len):\n",
        "    i=0\n",
        "    while i < len(splitted_sent): \n",
        "        if i == 0:\n",
        "            sub = \" \".join(splitted_sent[i:i+given_len])\n",
        "            subsents.append(sub)\n",
        "            i = i + given_len\n",
        "        if i!=0:\n",
        "            j = i + given_len - 5 #windown 5\n",
        "            if j + given_len <= len(splitted_sent):\n",
        "                sub = \" \".join(splitted_sent[j:j + given_len])\n",
        "                subsents.append(sub)\n",
        "            else:\n",
        "                sub = \" \".join(splitted_sent[j:])\n",
        "                if len(sub)>1:\n",
        "                    subsents.append(sub)\n",
        "            i = j + given_len\n",
        "    return subsents\n",
        "\n",
        "def handle_long_sentences(sentences, given_len):\n",
        "    # overlapped splitting sentence windown 5\n",
        "    subsents = []\n",
        "    deleted_long_sents = []\n",
        "    for sent in sentences:\n",
        "        splitted_sent = sent.split(\" \")\n",
        "        if len(splitted_sent) > given_len:\n",
        "          long_sent_subsents = split_long_sentence(splitted_sent, given_len)\n",
        "          subsents.extend(long_sent_subsents)\n",
        "          deleted_long_sents.append(sent)\n",
        "    # update sentences: remove and add subsents\n",
        "    for del_sent in deleted_long_sents:\n",
        "        sentences.remove(del_sent)\n",
        "    for add_sent in subsents:\n",
        "        sentences.append(add_sent)\n",
        "    return sentences\n",
        "\n",
        "def create_marked_senteces(sentences):\n",
        "    return ['[CLS] ' + sent.strip() + ' [SEP]' for sent in sentences]\n",
        "def save_sents_to_txt(shorted_sentences):\n",
        "    with open(r'./bert_sentences.txt', 'w') as fp:\n",
        "      for sent in shorted_sentences:\n",
        "          # write each item on a new line\n",
        "          fp.write(f'{sent} \\n')\n",
        "      print('saving sentences from bert-processing')\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Datenvorbereitung**"
      ],
      "metadata": {
        "id": "AJoz6F0DVHX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"reading data:...\")\n",
        "raw_documents, _ = read_raw_documents()\n",
        "print(len(raw_documents))\n",
        "print(\"preprocess data:...\")\n",
        "preprocessed_docs = simple_preprocess(raw_documents)\n",
        "print(len(preprocessed_docs))\n",
        "print(\"transform to sentences:...\")\n",
        "sentences = transform_to_sentences(preprocessed_docs)\n",
        "print(\"split sentences to 128 tokens:...\")\n",
        "shorted_sentences =  handle_long_sentences(sentences, 128)\n",
        "shorted_sentences = create_marked_senteces(shorted_sentences)\n",
        "# write sentences to txt files\n",
        "with open(r'bert_sentences.txt', 'w') as fp:\n",
        "    for sent in shorted_sentences:\n",
        "        # write each item on a new line\n",
        "        fp.write(f'{sent} \\n')\n",
        "    print('saving sentences from bert-processing')\n",
        "print(\"finished: ...\")\n",
        "\n"
      ],
      "metadata": {
        "id": "lBqLI7sVQPQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embeddings**"
      ],
      "metadata": {
        "id": "ktZVaf0YVDIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast, BertTokenizer\n",
        "tokenizer_ = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased',\n",
        "                                  output_hidden_states = True, # Whether the model returns all hidden-states.\n",
        "                                  )\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "scUleQQQXbJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Information nach dem Tokenizer**"
      ],
      "metadata": {
        "id": "28n_S3YdYngR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer_for_a_sent(sent, tokenizer):\n",
        "  this_sent_tokenizer = tokenizer(sent)\n",
        "  # index of token in the vocabulary\n",
        "  indexed_tokens = this_sent_tokenizer.input_ids\n",
        "  segments_ids = [1] * len(indexed_tokens)\n",
        "  # Convert inputs to PyTorch tensors\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "  return tokens_tensor, segments_tensors"
      ],
      "metadata": {
        "id": "VJ4k-5etWruS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reform_token_embeddings_of_sentence(full_outputs):\n",
        "  hidden_states = outputs[2]\n",
        "  token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "  token_embeddings = torch.squeeze(token_embeddings, dim=1) # size= (n_hidden_layers, n_tokens, 768)\n",
        "  token_embeddings = token_embeddings.permute(1,0,2) # size= (n_tokens, n_hidden_layers, 768)\n",
        "  return token_embeddings \n",
        "\n",
        "def get_token_embeddings(reformed_token_embeddings):\n",
        "  # using sum four last layers\n",
        "  token_vecs_sum = []\n",
        "  for token in token_embeddings: \n",
        "    sum_vec = torch.sum(token[-4:], dim=0)\n",
        "    token_vecs_sum.append(sum_vec)\n",
        "    print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))\n",
        "  return token_vecs_sum # size: n_tokens: 768"
      ],
      "metadata": {
        "id": "vZUb5gfJpK8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def find_belonged_embeddings_of_token(token, tokenized_text, embeddings_outputs):\n",
        "  belonged_embeddings = []\n",
        "  belonged_indices = []\n",
        "  for idx, t in enumerate(tokenized_text):\n",
        "    if t.strip(\"#\") in token:\n",
        "      belonged_indices.append(i)\n",
        "  for idx in belonged_indices:\n",
        "    belonged_embeddings.append(embeddings_outputs[i])\n",
        "  return belonged_embeddings\n",
        "\"\"\"\n",
        "def get_belonging_embeddings_of_word(word_id, tokenized_indices, tokens_embeddings):\n",
        "  belongging_embeddings_of_word = []\n",
        "  for idx, tokenizer_idx in enumerate(tokenized_indices):\n",
        "    if tokenizer_idx == word_id:\n",
        "      belongging_embeddings_of_word.append(tokens_embeddings[idx])\n",
        "  return belongging_embeddings_of_word\n",
        "\n",
        "def get_word_embedding(belonging_embeddings=None, methode=\"mean\"):\n",
        "  return torch.mean(belonging_embeddings, dim=0)\n",
        "\n",
        "def get_all_word_embeddings_in_sent(original_sent, sent_tokens_ids, sent_outputs_tokens_embeddings):\n",
        "    # todo what is one word in multiple position in sentence\n",
        "    # remove the special token ids from sent_tokens_ids\n",
        "    def need_to_update(sent_tokens_ids):\n",
        "      return sent_tokens_ids\n",
        "    sent_tokens_id = need_to_update(sent_tokens_ids)\n",
        "    splitted_words_of_sent = original_sent.split(\" \")\n",
        "    # make ids constitent \n",
        "    sent_words_ids = { i+1:word for i, word in enumerate(splitted_words_of_sent)}\n",
        "    sent_words_embeddings = {}\n",
        "    unique_tokens_ids_list = list(set(sent_tokens_ids)) #neead remove the id of special tokens, which are not in the original sentence\n",
        "    if len(sent_words_ids.keys()) == len(unique_tokens_ids_list):\n",
        "      for unique_token_id in unique_tokens_ids_list:\n",
        "        original_word = sent_words_ids[unique_token_id]\n",
        "        word_belonging_embeddings = get_belonging_embeddings_of_word(unique_token_id, sent_tokens_ids, sent_outputs_tokens_embeddings)\n",
        "        sent_words_embeddings[original_word] = get_word_embedding(word_belonging_embeddings)\n",
        "      return sent_words_embeddings\n",
        "    else:\n",
        "      return False\n",
        "\"\"\"\n",
        "def get_word_embeddings_from_all_senteneces_for_a_word(vocab, data_sentences_in_embeddings):\n",
        "  sum_embeddings = None\n",
        "  counts = None\n",
        "  for word in vocab:\n",
        "    for i in range(data_sentences_in_embeddings):\n",
        "      embeddings_in_sent = data_sentences_in_embeddings[i]\n",
        "\n",
        "\n",
        "  return torch.mean(word_embeddings_from_all_senteneces)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "blyG2KaZWn2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embeddings_using_tokenizer():\n",
        "  return True\n",
        "def embeddings_using_tokenizerfast():\n",
        "  return True"
      ],
      "metadata": {
        "id": "UAUi-Upbf41x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex_sent = shorted_sentences[0]\n",
        "print(ex_sent)\n",
        "\n",
        "# using BertTokenizer\n",
        "tokens = tokenizer_.tokenize(ex_sent)\n",
        "print(tokens)\n",
        "print(len(tokens))\n",
        "indexed_tokens = tokenizer_.convert_tokens_to_ids(tokens)\n",
        "print(indexed_tokens)\n",
        "segments_ids = [1] * len(tokens)\n",
        "print(segments_ids)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor, segments_tensors)\n",
        "    hidden_states = outputs[2]\n",
        "    print(len(hidden_states))\n",
        "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "    print(token_embeddings.size())\n",
        "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
        "    print(token_embeddings.size())\n",
        "    #print(hidden_states.shape)\n",
        "# using BertTokenizerFast\n",
        "    reformed = reform_token_embeddings_of_sentence(outputs)\n",
        "    sent_tokens_embeddings = get_token_embeddings(reformed)"
      ],
      "metadata": {
        "id": "i327MIAIYxvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizerfast = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "#tokens = t('word embeddings are vectors', add_special_tokens=False, return_attention_mask=False, return_token_type_ids=False)\n",
        "#print(tokens.word_ids())\n",
        "tokenized_text = tokenizerfast(sent)\n",
        "# index of token in the vocabulary\n",
        "indexed_tokens = tokenized_text.input_ids\n",
        "\n",
        "segments_ids = [1] * len(tokenized_text)\n",
        "# Convert inputs to PyTorch tensors\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "#print(tokenized_text.input_ids)\n",
        "#print(tokenized_text.token_type_ids)\n",
        "#print(tokenized_text.word_ids())"
      ],
      "metadata": {
        "id": "uz7IQKoQgQny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,len(sentences)):\n",
        "  sent = shorted_sentences[i]\n",
        "  tokens_tensor, segments_tensors = tokenizer_for_a_sent(sent)\n",
        "  \"\"\"\n",
        "  tk = tokenizer(sent)\n",
        "  print(tk.input_ids)\n",
        "  print(tk.token_type_ids)\n",
        "  print(tk.word_ids())\n",
        "  print(len(tk.word_ids()))\n",
        "  print(tokenizer.tokenize(sent))\n",
        "  print(len(tokenizer.tokenize(sent)))\n",
        "  print(\"---------------------------------------\")\n",
        "  \"\"\"\n",
        "  with torch.no_grad():\n",
        "    outputs = model(tokens_tensor, segments_tensors)\n",
        "    hidden_states = outputs[2]\n",
        "    #print(hidden_states)\n",
        "    print(len(hidden_states))\n",
        "    print(hidden_states[0].shape)"
      ],
      "metadata": {
        "id": "9OOiDwYfcnv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GbPW9ebCdfbI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}