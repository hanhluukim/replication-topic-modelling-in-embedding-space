{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading texts: ...\n",
      "train-size after loading: 11314\n",
      "test-size after loading: 7532\n",
      "finished load!\n",
      "start: preprocessing: ...\n",
      "finised: preprocessing!\n",
      "vocab-size in df: 8496\n",
      "validation-size ist: 0.01\n",
      "start creating vocabulary ...\n",
      "length of the vocabulary: 8496\n",
      "sample ten words of the vocabulary: ['defense', 'reputation', 'discusses', 'warnings', 'believes', 'increased', 'inaccurate', 'wam', 'cmu', 'decisions']\n",
      "length word2id list: 8496\n",
      "length id2word list: 8496\n",
      "finished: creating vocabulary\n",
      "train-size-after-all: 11214\n",
      "test-size-after-all: 7532\n",
      "validation-size-after-all: 100\n",
      "test-size-after-all: 11214\n",
      "test-indices-length: 11214\n",
      "test-size-after-all: 100\n",
      "test-indices-length: 100\n",
      "test-size-after-all: 7532\n",
      "test-indices-length: 7532\n",
      "length train-documents-indices : 1150368\n",
      "length of the vocabulary: 8496\n",
      "\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>defense</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reputation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>discusses</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>warnings</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>believes</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>scratches</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>unreasonable</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>sophisticated</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>tobias</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>hamer</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  id\n",
       "0         defense   0\n",
       "1      reputation   1\n",
       "2       discusses   2\n",
       "3        warnings   3\n",
       "4        believes   4\n",
       "..            ...  ..\n",
       "95      scratches  95\n",
       "96   unreasonable  96\n",
       "97  sophisticated  97\n",
       "98         tobias  98\n",
       "99          hamer  99\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# einige Paketten wurden für Visualisierung gebraucht\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#import umap.umap_ as umap\n",
    "import time\n",
    "#import plotly.express as px\n",
    "#from sklearn import cluster\n",
    "#from sklearn import metrics\n",
    "\n",
    "from src.prepare_dataset import TextDataLoader\n",
    "# init TextDataLoader für die Datenquelle 20 News Groups\n",
    "# Daten abrufen vom Sklearn, tokenisieren und besondere Charaktern entfernen\n",
    "textsloader = TextDataLoader(source=\"20newsgroups\", train_size=None, test_size=None)\n",
    "textsloader.load_tokenize_texts(\"20newsgroups\")\n",
    "# Vorverarbeitung von Daten mit folgenden Schritten:\n",
    "textsloader.preprocess_texts(length_one_remove=True, punctuation_lower = True, stopwords_filter = True)\n",
    "# Daten zerlegen für Train, Test und Validation. Erstellen Vocabular aus dem Trainset\n",
    "min_df= 30\n",
    "textsloader.split_and_create_voca_from_trainset(max_df=0.7, min_df=min_df, stopwords_remove_from_voca=True)\n",
    "\n",
    "# Erstellen BOW-Repräsentation für ETM Modell\n",
    "for_lda_model = False\n",
    "word2id, id2word, train_set, test_set, val_set = textsloader.create_bow_and_savebow_for_each_set(for_lda_model=for_lda_model)\n",
    "# show for samples: 100 word2id and id2 word\n",
    "word2id_df_100 = pd.DataFrame()\n",
    "word2id_df_100['word'] = list(word2id.keys())[:100]\n",
    "word2id_df_100['id'] = list(word2id.values())[:100]\n",
    "word2id_df_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary after prprocessing ist: 8496\n",
      "Size of train set: 11214\n",
      "Size of val set: 100\n",
      "Size of test set: 7532\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text-after-preprocessing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>biochem nwu jackson swimming pool defense nntp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apollo hp red herring police state usa nntp po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hades coos dartmouth brian hughes installing r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jaeger buphy bu gregg jaeger rushdie boston un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>king eng umd doug boom computer design lab mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>physics ca campbell pc windows os unix reply p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ncr jim sharp parts information distribution w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>sera zuma serdar argic nazi germany armenians ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>chips astro temple charlie mathew bible resear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>loss ece cmu doug loss crazy electrical comput...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text-after-preprocessing\n",
       "0   biochem nwu jackson swimming pool defense nntp...\n",
       "1   apollo hp red herring police state usa nntp po...\n",
       "2   hades coos dartmouth brian hughes installing r...\n",
       "3   jaeger buphy bu gregg jaeger rushdie boston un...\n",
       "4   king eng umd doug boom computer design lab mar...\n",
       "..                                                ...\n",
       "95  physics ca campbell pc windows os unix reply p...\n",
       "96  ncr jim sharp parts information distribution w...\n",
       "97  sera zuma serdar argic nazi germany armenians ...\n",
       "98  chips astro temple charlie mathew bible resear...\n",
       "99  loss ece cmu doug loss crazy electrical comput...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kontrollieren die Größen von verschiedenen Datensätzen\n",
    "print(f'Size of the vocabulary after prprocessing ist: {len(textsloader.vocabulary)}')\n",
    "print(f'Size of train set: {len(train_set[\"tokens\"])}')\n",
    "print(f'Size of val set: {len(val_set[\"tokens\"])}')\n",
    "print(f'Size of test set: {len(test_set[\"test\"][\"tokens\"])}')\n",
    "\n",
    "# re-erstellen von Dokumenten nach der Vorverarbeitungen. Die Dokumenten sind in Wörtern und werden für Word-Embedding Training benutzt\n",
    "docs_tr, docs_t, docs_v = textsloader.get_docs_in_words_for_each_set()\n",
    "train_docs_df = pd.DataFrame()\n",
    "train_docs_df['text-after-preprocessing'] = [' '.join(doc) for doc in docs_tr[:100]]\n",
    "train_docs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training file for embedding\n",
    "# options: cbow, skipgram and bert-embedding\n",
    "# returns: word-embedding for each word in the vocabulary\n",
    "# inputs: train-documents in words and the vocabulary (?)\n",
    "\n",
    "import gensim\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from numpy import dot, argmax, indices\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def normalize(v):\n",
    "    norm=np.linalg.norm(v)\n",
    "    if norm==0:\n",
    "        norm=np.finfo(v.dtype).eps\n",
    "    return norm\n",
    "\n",
    "def unit_vec(vector):\n",
    "      veclen = np.sqrt(np.sum(vector ** 2))\n",
    "      #other_veclen = normalize(vector)\n",
    "      #print(f'veclen: {veclen}')\n",
    "      #print(f'other-veclen: {other_veclen}')\n",
    "      return vector/veclen\n",
    "def get_consine_similarity_2(vector1, vector2):\n",
    "      #vector unit length so that just use dot product\n",
    "      #print(np.array(vector1).shape)\n",
    "      vector1 = unit_vec(np.array(vector1))\n",
    "      vector2 = unit_vec(np.array(vector2))\n",
    "      #print(f'veclen after normalization: {normalize(vector1)}')\n",
    "      return dot(vector1, vector2)\n",
    "\n",
    "def get_consine_similarity(vector1, vector2):\n",
    "      #print(\"using distance.cosine\")\n",
    "      #vector unit length so that just use dot product\n",
    "      #print(np.array(vector1).shape)\n",
    "      vector1 = unit_vec(np.array(vector1))\n",
    "      vector2 = unit_vec(np.array(vector2))\n",
    "      #print(f'veclen after normalization: {normalize(vector1)}')\n",
    "      return 1-distance.cosine(vector1, vector2) #1-dot()/norm\n",
    "\n",
    "def get_similar_vectors_to_given_vector(topn, vocab, give_vector, all_vectors):\n",
    "      #print(vocab[2539])\n",
    "      dists = []\n",
    "      for vector2 in all_vectors:\n",
    "            dists.append(get_consine_similarity_2(give_vector, vector2))\n",
    "      #dists = np.array(dists)\n",
    "      #print(sorted(dists, reverse=True)[:10])\n",
    "      top_dists = sorted(dists, reverse=True)[1:topn+1]\n",
    "      #print(top_dists)\n",
    "      top_indices = [dists.index(d) for d in top_dists]\n",
    "      #[i for i in range(0, sorted(dists, reverse_))][1:topn+1]\n",
    "      #list(argsort(dists, reverse=False)[1:topn+1]) #do not use dist = 0 of same vectors\n",
    "      #print(top_indices)\n",
    "      top_words = {} #np.array(vocab)[indices]\n",
    "      for idx in top_indices:\n",
    "            top_words[vocab[idx]] = dists[idx]\n",
    "      return top_words\n",
    "\n",
    "def compare_word2vec_methods_and_bert_embeddings(word, word2vec_embeddings, bert_embeddings):\n",
    "      return {'word2vec': [], 'bert': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path(model_name, save_path):\n",
    "    if model_name != \"bert\":\n",
    "        save_path = \"\"\n",
    "        try:\n",
    "            file_name = f'{model_name}_vocab_embedding.txt'\n",
    "            save_path = save_path + \"/\" + file_name\n",
    "        except:\n",
    "            file_name = 'vocab_embedding.txt'\n",
    "            save_path = Path.joinpath(str(save_path),str(file_name))\n",
    "        return save_path\n",
    "    else:\n",
    "        return 'prepared_data/bert_vocab_embedding.txt'\n",
    "\n",
    "def read_prefitted_embedding(model_name, vocab, save_path):\n",
    "      save_path = get_path(model_name, save_path)\n",
    "      with open(save_path) as f:\n",
    "            lines = f.readlines()\n",
    "      embedding_data = {}\n",
    "      for t in lines:\n",
    "            w = t.split(\"\\t\")[0]\n",
    "            if w in vocab:\n",
    "                v = [float(e) for e in t.split(\"\\t\")[1].split(\" \")[:-1]] #not \\n\n",
    "                # check again only word in the etm-vocabulary\n",
    "                if w in vocab:\n",
    "                      embedding_data[w] = v\n",
    "      \n",
    "      # sort embedding_data again by the ordner of the vocabulary from bow\n",
    "      words_embeddings = np.array(list(embedding_data.values()))\n",
    "      words = np.array(list(embedding_data.keys()))\n",
    "      print(f'len after reading: {len(words)}')\n",
    "      print(f'len vocab: {len(vocab)}')\n",
    "      if len(words) == len(vocab):\n",
    "            indices = [vocab.index(words[i]) for i in range(0,len(words))]\n",
    "            words_in_vocab = [vocab[i] for i in range(0,len(words))]\n",
    "            words_embeddings = [e for _, e in sorted(zip(indices, words_embeddings))]\n",
    "            return words_in_vocab, words_embeddings #list(embedding_data.values())\n",
    "      else:\n",
    "            print(\"something wrong at the embedding.py/read_prefitted_embeddings\")\n",
    "            print(\"use for testing bert\")\n",
    "            return words, words_embeddings #list(embedding_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingCreator:\n",
    "      def __init__(self, model_name=\"cbow\", documents = None, save_path = \"\"):\n",
    "            \"\"\"\n",
    "            Input: documents in List of words, train-settings\n",
    "            Output: word-embedding for the vocabulary\n",
    "\n",
    "            Args:\n",
    "                model_name (str, optional): _description_. Defaults to \"cbow\".\n",
    "                documents (_type_, optional): _description_. Defaults to None.\n",
    "                vocab (_type_, optional): _description_. Defaults to None.\n",
    "                save_path (str, optional): _description_. Defaults to \"\".\n",
    "            \"\"\"\n",
    "            self.model_name = model_name\n",
    "            self.save_path = save_path\n",
    "            self.documents = documents\n",
    "            self.model = None\n",
    "            self.all_embeddings = []\n",
    "            \n",
    "      def train(self, min_count = 0, embedding_size = 300):\n",
    "            if self.model_name==\"cbow\":\n",
    "                  print(\"word-embedding train begins\")\n",
    "                  self.model = gensim.models.Word2Vec(self.documents, \n",
    "                                                      seed = 42,\n",
    "                                                      min_count=min_count, \n",
    "                                                      sg=0, \n",
    "                                                      window=5,\n",
    "                                                      size=embedding_size,\n",
    "                                                      iter=5)\n",
    "            elif self.model_name==\"skipgram\":\n",
    "                  print(\"train begin:word-embedding with skipgram\")\n",
    "                  self.model = gensim.models.Word2Vec(self.documents, \n",
    "                                                      seed = 42,\n",
    "                                                      min_count=min_count, \n",
    "                                                      sg=1, \n",
    "                                                      window=5,\n",
    "                                                      size=embedding_size,\n",
    "                                                      negative=1,\n",
    "                                                      iter=5)\n",
    "            else:\n",
    "                  print(\"word-embedding with BERT\")\n",
    "                  print(\"!!!! please run src/bert_main.py to get prepared_data/bert_vocab_embeddings.txt\")\n",
    "            print(\"word-embedding train finished\")\n",
    "            # todo: save the trained-model\n",
    "            #return self.model\n",
    "          \n",
    "      def create_and_save_vocab_embedding(self, train_vocab = None, embedding_path = None):\n",
    "            \"\"\"_summary_\n",
    "\n",
    "            Args:\n",
    "                train_vocab (_type_): vocabulary from the prepare_dataset.py\n",
    "                embedding_path (_type_): path to save the trained-embedding\n",
    "\n",
    "            Returns:\n",
    "                _type_: _description_\n",
    "            \"\"\"\n",
    "            model_vocab = []\n",
    "            if self.model_name==\"bert\":\n",
    "                  model_vocab = [] #bert in other processing, ignore here\n",
    "            else:\n",
    "                  model_vocab = list(self.model.wv.vocab)\n",
    "            print(f'length of vocabulary from word-embedding model {len(model_vocab)}')\n",
    "            print(f'length of the vocabulary of prepraring-dataset-vocabulary: {len(train_vocab)}')\n",
    "            del self.documents\n",
    "            \n",
    "            f = open(Path.joinpath(embedding_path, f'{self.model_name}_vocab_embedding.txt'), 'w') #add to prepared_data\n",
    "            # sort words in embedding matrix by the ordner from vocabulary\n",
    "            for v in tqdm(train_vocab): # sort the list embeddings by words in vocabulary\n",
    "                if v in model_vocab:\n",
    "                    vec = list(self.model.wv.__getitem__(v))\n",
    "                    self.all_embeddings.append(vec)\n",
    "                    f.write(v + '\\t')\n",
    "                    vec_str = ['%.9f' % val for val in vec]\n",
    "                    vec_str = \" \".join(vec_str)\n",
    "                    f.write(vec_str + '\\n')\n",
    "            f.close()\n",
    "            self.model.save(str(Path.joinpath(embedding_path, f'{self.model_name}_word2vec.model')))\n",
    "            return True\n",
    "      \n",
    "      def other_save_embeddings(self, train_vocab):\n",
    "            all_embeddings = []\n",
    "            model_vocab = list(self.model.wv.vocab)\n",
    "            for v in tqdm(train_vocab): # sort the list embeddings by words in vocabulary\n",
    "                 if v in model_vocab:\n",
    "                       vec = list(self.model.wv.__getitem__(v))\n",
    "                       all_embeddings.append(vec)\n",
    "            np.save(f'{self.model_name}_other_embedding.npy', all_embeddings)\n",
    "            return all_embeddings\n",
    "\n",
    "      def find_most_similar_words(self, n_neighbor=20, word = None):\n",
    "            if word!=None:\n",
    "                  return self.model.wv.most_similar(word, topn=n_neighbor)\n",
    "            else:\n",
    "                  print(f'give a word to get the {n_neighbor} neighbor words')\n",
    "\n",
    "      def find_similar_words_self_implemented(self, topn, train_vocab, word):\n",
    "            top_words = {}\n",
    "            model_vocab = list(self.model.wv.vocab)\n",
    "            #all_embeddings = self.other_save_embeddings(train_vocab)\n",
    "            if word in train_vocab:\n",
    "                  if word in model_vocab:\n",
    "                        considered_vector = list(self.model.wv.__getitem__(word))\n",
    "                        top_words = get_similar_vectors_to_given_vector(topn, train_vocab, considered_vector, self.all_embeddings)\n",
    "            return top_words\n",
    "\n",
    "      def cluster_words(self, embedding_save_path = None, fig_path = None, n_components=3, text = False):\n",
    "            import umap.umap_ as umap\n",
    "            import time\n",
    "            import plotly.express as px\n",
    "            from sklearn import cluster\n",
    "            from sklearn import metrics\n",
    "\n",
    "            # read embedding from file\n",
    "            with open(Path.joinpath(embedding_save_path, f'{self.model_name}_vocab_embedding.txt')) as f:\n",
    "              lines = f.readlines()\n",
    "            embedding_data = []\n",
    "            words_data = []\n",
    "            for t in lines:\n",
    "              w = t.split(\"\\t\")[0]\n",
    "              v = [float(e) for e in t.split(\"\\t\")[1].split(\" \")]\n",
    "              words_data.append(w)\n",
    "              embedding_data.append(v)\n",
    "            # using kmean to get clusters of words\n",
    "            kmeans = cluster.KMeans(n_clusters=10)\n",
    "            kmeans.fit(embedding_data)\n",
    "            labels = kmeans.labels_\n",
    "            centroids = kmeans.cluster_centers_\n",
    "            #print(\"Cluster id labels for inputted data\")\n",
    "            #print(labels)\n",
    "            #print(\"Centroids data\")\n",
    "            #print(centroids)\n",
    "            # dimension reduction with umap\n",
    "            reducer = umap.UMAP(random_state=42,n_components=n_components)\n",
    "            embedding = reducer.fit_transform(embedding_data)\n",
    "            # show samples after dim-reduction in dataframe\n",
    "            if n_components == 3:\n",
    "                  wb = pd.DataFrame(embedding, columns=['x', 'y', 'z'])\n",
    "            else:\n",
    "                  wb = pd.DataFrame(embedding, columns=['x', 'y'])\n",
    "            wb['word'] = words_data\n",
    "            wb['cluster'] = ['cluster ' + str(c) for c in labels]\n",
    "            # visualization with plotply\n",
    "            if n_components==3:\n",
    "                  fig = px.scatter_3d(wb, \n",
    "                                    text = wb['word'],\n",
    "                                    x='x', y='y', z='z',\n",
    "                                    color = wb['cluster'],\n",
    "                                    title =\"word-embedding-samples\")\n",
    "            else:\n",
    "                  # n_components = 2\n",
    "                  if text:\n",
    "                    fig = px.scatter(wb, text = wb['word'], x='x', y='y', color=wb['cluster'], title='word embedding samples')\n",
    "                  else:\n",
    "                    fig = px.scatter(wb, x='x', y='y', color=wb['cluster'], title='word embedding samples')\n",
    "                    \n",
    "            fig.write_image(Path.joinpath(fig_path, f'embedding_space_dim_{n_components}.png'))\n",
    "            fig.write_html(Path.joinpath(fig_path, f'embedding_space_dim_{n_components}.html'))\n",
    "            fig.show()\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train begin:word-embedding with skipgram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 343/8496 [00:00<00:02, 3416.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word-embedding train finished\n",
      "length of vocabulary from word-embedding model 8496\n",
      "length of the vocabulary of prepraring-dataset-vocabulary: 8496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8496/8496 [00:02<00:00, 3230.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbor words of some sample selected words\n",
      "neighbor of word defense\n",
      "['defence', 'canadians', 'opportunity', 'officials', 'fighting']\n",
      "[0.7752761840820312, 0.7698767185211182, 0.7674999237060547, 0.7539409399032593, 0.7515166997909546]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "neighbor of word reputation\n",
      "['reminded', 'odds', 'publicity', 'oldest', 'statistic']\n",
      "[0.9265125393867493, 0.9224061369895935, 0.9202713370323181, 0.9078179001808167, 0.9077446460723877]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "neighbor of word discusses\n",
      "['journals', 'cited', 'relating', 'commentary', 'literature']\n",
      "[0.9210744500160217, 0.8959219455718994, 0.868064284324646, 0.8452044725418091, 0.8443825244903564]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "neighbor of word warnings\n",
      "['stops', 'tag', 'screws', 'guts', 'salesman']\n",
      "[0.8925166726112366, 0.88664710521698, 0.8786121606826782, 0.8773837089538574, 0.8691571950912476]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "neighbor of word believes\n",
      "['gays', 'motives', 'promises', 'immoral', 'founded']\n",
      "[0.9275579452514648, 0.9239786863327026, 0.9239568710327148, 0.9234380722045898, 0.9206569790840149]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Path('prepared_data').mkdir(parents=True, exist_ok=True)\n",
    "Path(f'prepared_data/min_df_{min_df}').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_path = Path.joinpath(Path.cwd(), f'prepared_data/min_df_{min_df}')\n",
    "figures_path = Path.joinpath(Path.cwd(), f'figures/min_df_{min_df}')\n",
    "Path(figures_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "wb_creator = WordEmbeddingCreator(model_name=\"skipgram\", documents = docs_tr, save_path= save_path)\n",
    "wb_creator.train(min_count=0, embedding_size= 300)\n",
    "vocab = list(word2id.keys())\n",
    "wb_creator.create_and_save_vocab_embedding(vocab, save_path)\n",
    "\n",
    "print(\"neighbor words of some sample selected words\")\n",
    "for i in range(0,5):\n",
    "      print(f'neighbor of word {vocab[i]}')\n",
    "      print([r[0] for r in wb_creator.find_most_similar_words(n_neighbor=5, word=vocab[i])])\n",
    "      print([r[1] for r in wb_creator.find_most_similar_words(n_neighbor=5, word=vocab[i])])\n",
    "      print(100*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleichen ähnliche Wörter von Word2Vec (gensim und eigene Cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word-embedding of the word-- discusses: \n",
      "dim of vector: 300\n",
      "[('journals', 0.9210744500160217), ('cited', 0.8959219455718994), ('relating', 0.868064284324646), ('commentary', 0.8452044725418091), ('literature', 0.8443825244903564), ('representing', 0.837938666343689), ('practices', 0.8358759880065918), ('physicians', 0.8356345295906067), ('consensus', 0.8310937881469727), ('broad', 0.8309059143066406)]\n"
     ]
    }
   ],
   "source": [
    "v = vocab[2]\n",
    "vec = list(wb_creator.model.wv.__getitem__(v))\n",
    "print(f'word-embedding of the word-- {v}: ')\n",
    "print(f'dim of vector: {len(vec)}')\n",
    "print(wb_creator.find_most_similar_words(n_neighbor=10, word=v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "durham\n",
      "[1.0, 0.92107445, 0.8959219, 0.8680642, 0.8452044, 0.8443825, 0.8379387, 0.83587605, 0.8356345, 0.83109385]\n",
      "[0.92107445, 0.8959219, 0.8680642, 0.8452044, 0.8443825, 0.8379387, 0.83587605, 0.8356345, 0.83109385, 0.83090585]\n",
      "[7506, 7347, 5866, 2864, 8036, 7944, 88, 1685, 4136, 4697]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'journals': 0.92107445,\n",
       " 'cited': 0.8959219,\n",
       " 'relating': 0.8680642,\n",
       " 'commentary': 0.8452044,\n",
       " 'literature': 0.8443825,\n",
       " 'representing': 0.8379387,\n",
       " 'practices': 0.83587605,\n",
       " 'physicians': 0.8356345,\n",
       " 'consensus': 0.83109385,\n",
       " 'broad': 0.83090585}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb_creator.find_similar_words_self_implemented(10, vocab, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleich ähnliche Wörter zwischen Word2Vec und Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbedding:\n",
    "    def __init__(self, saved_embeddings_text_file):\n",
    "          self.file_path = saved_embeddings_text_file\n",
    "          self.bert_vocab = None\n",
    "          self.bert_embeddings = None\n",
    "          self.bert_norms = None\n",
    "    def get_bert_embeddings(self, etm_vocab):\n",
    "          # filtering words by etm_vocab\n",
    "          words_in_vocab, vocab_embeddings = read_prefitted_embedding(\"bert\", etm_vocab, self.file_path)\n",
    "          self.bert_embeddings = np.array(vocab_embeddings)\n",
    "          self.bert_vocab = list(words_in_vocab)\n",
    "          self.bert_norms = np.array([])\n",
    "          print(\"bert-embedding ready!\")\n",
    "          return True\n",
    "\n",
    "    def find_similar_words(self, word, top_neighbors, etm_vocab):\n",
    "          word_idx_in_vocab = self.bert_vocab.index(word)\n",
    "          considered_vector = self.bert_embeddings[word_idx_in_vocab]\n",
    "          top_words = get_similar_vectors_to_given_vector(top_neighbors,etm_vocab, considered_vector, self.bert_embeddings)\n",
    "          return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prepared_data/bert_vocab.txt') as f:\n",
    "    lines = f.readlines()\n",
    "readed_bert_vocab = [e.split(\"\\n\")[0] for e in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len after reading: 8473\n",
      "len vocab: 8496\n",
      "something wrong at the embedding.py/read_prefitted_embeddings\n",
      "use for testing bert\n",
      "bert-embedding ready!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(word2id.keys())\n",
    "bert_eb = BertEmbedding('prepared_data')\n",
    "bert_eb.get_bert_embeddings(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8473, 768)\n"
     ]
    }
   ],
   "source": [
    "print(bert_eb.bert_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xx\n",
      "iii\n",
      "mm\n",
      "ii\n",
      "ccc\n",
      "rr\n",
      "tt\n",
      "gotta\n",
      "pp\n",
      "oo\n",
      "cc\n",
      "gonna\n",
      "ll\n",
      "aa\n",
      "xxx\n",
      "ee\n",
      "ss\n",
      "aaa\n",
      "uu\n",
      "bb\n",
      "mmm\n",
      "wanna\n",
      "dd\n"
     ]
    }
   ],
   "source": [
    "for w in vocab:\n",
    "    if w not in bert_eb.bert_vocab:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "find_similar_words() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-487044925806>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_eb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_similar_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: find_similar_words() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "bert_eb.find_similar_words(v, 10, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Splitting again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_long_sentence(splitted_sent, given_len):\n",
    "    subsents = []\n",
    "    #for i in range(0,len(splitted_sent), given_len):\n",
    "    i=0\n",
    "    while i <= len(splitted_sent): \n",
    "        if i == 0:\n",
    "            print(\"test\")\n",
    "            sub = \" \".join(splitted_sent[i:i+given_len])\n",
    "            subsents.append(sub)\n",
    "            i = i + given_len\n",
    "        else:\n",
    "            print(f'actual: {i}')\n",
    "            j = i - 10 #windown 5\n",
    "            print(j)\n",
    "            if j + given_len <= len(splitted_sent):\n",
    "                sub = \" \".join(splitted_sent[j:j + given_len])\n",
    "                subsents.append(sub)\n",
    "            else:\n",
    "                sub = \" \".join(splitted_sent[j:])\n",
    "                #print(f'sub2: {sub}')\n",
    "                if len(sub)>1:\n",
    "                    subsents.append(sub)\n",
    "            i = j + given_len\n",
    "        print(f'next: {i}')\n",
    "        print('-'*100)\n",
    "    return subsents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"tel me a bout ths man ha eit anfls aneptm am ampb abur amo vakoiv balpn aka qjan af\" \n",
    "a = a.split(\" \")\n",
    "for j in range(1,200):\n",
    "    a.append('a'*j)\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "next: 128\n",
      "----------------------------------------------------------------------------------------------------\n",
      "actual: 128\n",
      "118\n",
      "next: 246\n",
      "----------------------------------------------------------------------------------------------------\n",
      "128\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "a = split_long_sentence(a, 128)\n",
    "for e in a:\n",
    "    print(len(e.split(\" \")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
