{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading texts: ...\n",
      "train-size after loading: 11314\n",
      "test-size after loading: 7532\n",
      "finished load!\n",
      "start: preprocessing: ...\n",
      "finised: preprocessing!\n",
      "vocab-size in df: 8496\n",
      "validation-size ist: 0.01\n",
      "start creating vocabulary ...\n",
      "length of the vocabulary: 8496\n",
      "sample ten words of the vocabulary: ['determines', 'cv', 'chung', 'dealing', 'tall', 'succeeded', 'sufficiently', 'oracle', 'distribute', 'compatible']\n",
      "length word2id list: 8496\n",
      "length id2word list: 8496\n",
      "finished: creating vocabulary\n",
      "train-size-after-all: 11214\n",
      "test-size-after-all: 7532\n",
      "validation-size-after-all: 100\n",
      "test-size-after-all: 11214\n",
      "test-indices-length: 11214\n",
      "test-size-after-all: 100\n",
      "test-indices-length: 100\n",
      "test-size-after-all: 7532\n",
      "test-indices-length: 7532\n",
      "length train-documents-indices : 1150368\n",
      "length of the vocabulary: 8496\n",
      "\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>determines</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chung</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dealing</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tall</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>existance</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>viewpoint</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>documented</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>issue</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>path</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          word  id\n",
       "0   determines   0\n",
       "1           cv   1\n",
       "2        chung   2\n",
       "3      dealing   3\n",
       "4         tall   4\n",
       "..         ...  ..\n",
       "95   existance  95\n",
       "96   viewpoint  96\n",
       "97  documented  97\n",
       "98       issue  98\n",
       "99        path  99\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# einige Paketten wurden für Visualisierung gebraucht\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#import umap.umap_ as umap\n",
    "import time\n",
    "#import plotly.express as px\n",
    "#from sklearn import cluster\n",
    "#from sklearn import metrics\n",
    "\n",
    "from src.prepare_dataset import TextDataLoader\n",
    "# init TextDataLoader für die Datenquelle 20 News Groups\n",
    "# Daten abrufen vom Sklearn, tokenisieren und besondere Charaktern entfernen\n",
    "textsloader = TextDataLoader(source=\"20newsgroups\", train_size=None, test_size=None)\n",
    "textsloader.load_tokenize_texts(\"20newsgroups\")\n",
    "# Vorverarbeitung von Daten mit folgenden Schritten:\n",
    "textsloader.preprocess_texts(length_one_remove=True, punctuation_lower = True, stopwords_filter = True)\n",
    "# Daten zerlegen für Train, Test und Validation. Erstellen Vocabular aus dem Trainset\n",
    "min_df= 30\n",
    "textsloader.split_and_create_voca_from_trainset(max_df=0.7, min_df=min_df, stopwords_remove_from_voca=True)\n",
    "\n",
    "# Erstellen BOW-Repräsentation für ETM Modell\n",
    "for_lda_model = False\n",
    "word2id, id2word, train_set, test_set, val_set = textsloader.create_bow_and_savebow_for_each_set(for_lda_model=for_lda_model)\n",
    "# show for samples: 100 word2id and id2 word\n",
    "word2id_df_100 = pd.DataFrame()\n",
    "word2id_df_100['word'] = list(word2id.keys())[:100]\n",
    "word2id_df_100['id'] = list(word2id.values())[:100]\n",
    "word2id_df_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary after prprocessing ist: 8496\n",
      "Size of train set: 11214\n",
      "Size of val set: 100\n",
      "Size of test set: 7532\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text-after-preprocessing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>biochem nwu jackson swimming pool defense nntp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apollo hp red herring police state usa nntp po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hades coos dartmouth brian hughes installing r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jaeger buphy bu gregg jaeger rushdie boston un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>king eng umd doug boom computer design lab mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>physics ca campbell pc windows os unix reply p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ncr jim sharp parts information distribution w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>sera zuma serdar argic nazi germany armenians ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>chips astro temple charlie mathew bible resear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>loss ece cmu doug loss crazy electrical comput...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text-after-preprocessing\n",
       "0   biochem nwu jackson swimming pool defense nntp...\n",
       "1   apollo hp red herring police state usa nntp po...\n",
       "2   hades coos dartmouth brian hughes installing r...\n",
       "3   jaeger buphy bu gregg jaeger rushdie boston un...\n",
       "4   king eng umd doug boom computer design lab mar...\n",
       "..                                                ...\n",
       "95  physics ca campbell pc windows os unix reply p...\n",
       "96  ncr jim sharp parts information distribution w...\n",
       "97  sera zuma serdar argic nazi germany armenians ...\n",
       "98  chips astro temple charlie mathew bible resear...\n",
       "99  loss ece cmu doug loss crazy electrical comput...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kontrollieren die Größen von verschiedenen Datensätzen\n",
    "print(f'Size of the vocabulary after prprocessing ist: {len(textsloader.vocabulary)}')\n",
    "print(f'Size of train set: {len(train_set[\"tokens\"])}')\n",
    "print(f'Size of val set: {len(val_set[\"tokens\"])}')\n",
    "print(f'Size of test set: {len(test_set[\"test\"][\"tokens\"])}')\n",
    "\n",
    "# re-erstellen von Dokumenten nach der Vorverarbeitungen. Die Dokumenten sind in Wörtern und werden für Word-Embedding Training benutzt\n",
    "docs_tr, docs_t, docs_v = textsloader.get_docs_in_words_for_each_set()\n",
    "train_docs_df = pd.DataFrame()\n",
    "train_docs_df['text-after-preprocessing'] = [' '.join(doc) for doc in docs_tr[:100]]\n",
    "train_docs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argsort(x, topn=None, reverse=False):\n",
    "    x = np.asarray(x)  # unify code path for when `x` is not a np array (list, tuple...)\n",
    "    if topn is None:\n",
    "        topn = x.size\n",
    "    if topn <= 0:\n",
    "        return []\n",
    "    if reverse:\n",
    "        x = -x\n",
    "    if topn >= x.size or not hasattr(np, 'argpartition'):\n",
    "        return np.argsort(x)[:topn]\n",
    "    # np >= 1.8 has a fast partial argsort, use that!\n",
    "    most_extreme = np.argpartition(x, topn)[:topn]\n",
    "    return most_extreme.take(np.argsort(x.take(most_extreme)))  # resort topn into order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training file for embedding\n",
    "# options: cbow, skipgram and bert-embedding\n",
    "# returns: word-embedding for each word in the vocabulary\n",
    "# inputs: train-documents in words and the vocabulary (?)\n",
    "\n",
    "import gensim\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from numpy import dot, argmax, indices\n",
    "\n",
    "def normalize(v):\n",
    "    norm=np.linalg.norm(v)\n",
    "    if norm==0:\n",
    "        norm=np.finfo(v.dtype).eps\n",
    "    return norm\n",
    "\n",
    "def unit_vec(vector):\n",
    "      veclen = np.sqrt(np.sum(vector ** 2))\n",
    "      #other_veclen = normalize(vector)\n",
    "      #print(f'veclen: {veclen}')\n",
    "      #print(f'other-veclen: {other_veclen}')\n",
    "      return vector/veclen\n",
    "def get_consine_similarity_2(vector1, vector2):\n",
    "      #vector unit length so that just use dot product\n",
    "      #print(np.array(vector1).shape)\n",
    "      vector1 = unit_vec(np.array(vector1))\n",
    "      vector2 = unit_vec(np.array(vector2))\n",
    "      #print(f'veclen after normalization: {normalize(vector1)}')\n",
    "      return dot(vector1, vector2)\n",
    "\n",
    "def get_consine_similarity(vector1, vector2):\n",
    "      #print(\"using distance.cosine\")\n",
    "      #vector unit length so that just use dot product\n",
    "      #print(np.array(vector1).shape)\n",
    "      vector1 = unit_vec(np.array(vector1))\n",
    "      vector2 = unit_vec(np.array(vector2))\n",
    "      #print(f'veclen after normalization: {normalize(vector1)}')\n",
    "      return 1-distance.cosine(vector1, vector2) #1-dot()/norm\n",
    "\n",
    "def get_similar_vectors_to_given_vector(topn, vocab, give_vector, all_vectors):\n",
    "      print(vocab[2539])\n",
    "      dists = []\n",
    "      for vector2 in all_vectors:\n",
    "            dists.append(get_consine_similarity_2(give_vector, vector2))\n",
    "      #dists = np.array(dists)\n",
    "      print(sorted(dists, reverse=True)[:10])\n",
    "      top_dists = sorted(dists, reverse=True)[1:topn+1]\n",
    "      print(top_dists)\n",
    "      top_indices = [dists.index(d) for d in top_dists]\n",
    "      #[i for i in range(0, sorted(dists, reverse_))][1:topn+1]\n",
    "      #list(argsort(dists, reverse=False)[1:topn+1]) #do not use dist = 0 of same vectors\n",
    "      print(top_indices)\n",
    "      top_words = {} #np.array(vocab)[indices]\n",
    "      for idx in top_indices:\n",
    "            top_words[vocab[idx]] = dists[idx]\n",
    "      return top_words\n",
    "\n",
    "def compare_word2vec_methods_and_bert_embeddings(word, word2vec_embeddings, bert_embeddings):\n",
    "      return {'word2vec': [], 'bert': []}\n",
    "\n",
    "def read_prefitted_embedding(model_name, vocab, save_path):\n",
    "      try:\n",
    "            save_path = Path.joinpath(save_path, f'{model_name}_vocab_embedding.txt')\n",
    "      except:\n",
    "            save_path = Path.joinpath(save_path, f'vocab_embedding.txt')\n",
    "\n",
    "      with open(save_path) as f:\n",
    "            lines = f.readlines()\n",
    "      embedding_data = {}\n",
    "      for t in lines:\n",
    "            w = t.split(\"\\t\")[0]\n",
    "            v = [float(e) for e in t.split(\"\\t\")[1].split(\" \")]\n",
    "            # check again only word in the etm-vocabulary\n",
    "            if w in vocab:\n",
    "                  embedding_data[w] = v\n",
    "      \n",
    "      # sort embedding_data again by the ordner of the vocabulary from bow\n",
    "      words_embeddings = np.array(list(embedding_data.values()))\n",
    "      words = np.array(list(embedding_data.keys()))\n",
    "      if words == np.array(vocab):\n",
    "            indices = [vocab.index(words[i]) for i in range(0,len(words))]\n",
    "            words_in_vocab = [vocab[i] for i in range(0,len(words))]\n",
    "            words_embeddings = [e for _, e in sorted(zip(indices, words_embeddings))]\n",
    "            return words_in_vocab, words_embeddings #list(embedding_data.values())\n",
    "      else:\n",
    "            print(\"something wrong at the embedding.py/read_prefitted_embeddings\")\n",
    "            print(\"use for testing bert\")\n",
    "            return words, words_embeddings #list(embedding_data.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingCreator:\n",
    "      def __init__(self, model_name=\"cbow\", documents = None, save_path = \"\"):\n",
    "            \"\"\"\n",
    "            Input: documents in List of words, train-settings\n",
    "            Output: word-embedding for the vocabulary\n",
    "\n",
    "            Args:\n",
    "                model_name (str, optional): _description_. Defaults to \"cbow\".\n",
    "                documents (_type_, optional): _description_. Defaults to None.\n",
    "                vocab (_type_, optional): _description_. Defaults to None.\n",
    "                save_path (str, optional): _description_. Defaults to \"\".\n",
    "            \"\"\"\n",
    "            self.model_name = model_name\n",
    "            self.save_path = save_path\n",
    "            self.documents = documents\n",
    "            self.model = None\n",
    "            self.all_embeddings = []\n",
    "            \n",
    "      def train(self, min_count = 0, embedding_size = 300):\n",
    "            if self.model_name==\"cbow\":\n",
    "                  print(\"word-embedding train begins\")\n",
    "                  self.model = gensim.models.Word2Vec(self.documents, \n",
    "                                                      seed = 42,\n",
    "                                                      min_count=min_count, \n",
    "                                                      sg=0, \n",
    "                                                      window=5,\n",
    "                                                      size=embedding_size,\n",
    "                                                      iter=5)\n",
    "            elif self.model_name==\"skipgram\":\n",
    "                  print(\"train begin:word-embedding with skipgram\")\n",
    "                  self.model = gensim.models.Word2Vec(self.documents, \n",
    "                                                      seed = 42,\n",
    "                                                      min_count=min_count, \n",
    "                                                      sg=1, \n",
    "                                                      window=5,\n",
    "                                                      size=embedding_size,\n",
    "                                                      negative=1,\n",
    "                                                      iter=5)\n",
    "            else:\n",
    "                  print(\"word-embedding with BERT\")\n",
    "                  print(\"!!!! please run src/bert_main.py to get prepared_data/bert_vocab_embeddings.txt\")\n",
    "            print(\"word-embedding train finished\")\n",
    "            # todo: save the trained-model\n",
    "            #return self.model\n",
    "          \n",
    "      def create_and_save_vocab_embedding(self, train_vocab = None, embedding_path = None):\n",
    "            \"\"\"_summary_\n",
    "\n",
    "            Args:\n",
    "                train_vocab (_type_): vocabulary from the prepare_dataset.py\n",
    "                embedding_path (_type_): path to save the trained-embedding\n",
    "\n",
    "            Returns:\n",
    "                _type_: _description_\n",
    "            \"\"\"\n",
    "            model_vocab = []\n",
    "            if self.model_name==\"bert\":\n",
    "                  model_vocab = [] #bert in other processing, ignore here\n",
    "            else:\n",
    "                  model_vocab = list(self.model.wv.vocab)\n",
    "            print(f'length of vocabulary from word-embedding model {len(model_vocab)}')\n",
    "            print(f'length of the vocabulary of prepraring-dataset-vocabulary: {len(train_vocab)}')\n",
    "            del self.documents\n",
    "            \n",
    "            f = open(Path.joinpath(embedding_path, f'{self.model_name}_vocab_embedding.txt'), 'w') #add to prepared_data\n",
    "            # sort words in embedding matrix by the ordner from vocabulary\n",
    "            for v in tqdm(train_vocab): # sort the list embeddings by words in vocabulary\n",
    "                if v in model_vocab:\n",
    "                    vec = list(self.model.wv.__getitem__(v))\n",
    "                    self.all_embeddings.append(vec)\n",
    "                    f.write(v + '\\t')\n",
    "                    vec_str = ['%.9f' % val for val in vec]\n",
    "                    vec_str = \" \".join(vec_str)\n",
    "                    f.write(vec_str + '\\n')\n",
    "            f.close()\n",
    "            self.model.save(str(Path.joinpath(embedding_path, f'{self.model_name}_word2vec.model')))\n",
    "            return True\n",
    "      \n",
    "      def other_save_embeddings(self, train_vocab):\n",
    "            all_embeddings = []\n",
    "            model_vocab = list(self.model.wv.vocab)\n",
    "            for v in tqdm(train_vocab): # sort the list embeddings by words in vocabulary\n",
    "                 if v in model_vocab:\n",
    "                       vec = list(self.model.wv.__getitem__(v))\n",
    "                       all_embeddings.append(vec)\n",
    "            np.save(f'{self.model_name}_other_embedding.npy', all_embeddings)\n",
    "            return all_embeddings\n",
    "\n",
    "      def find_most_similar_words(self, n_neighbor=20, word = None):\n",
    "            if word!=None:\n",
    "                  return self.model.wv.most_similar(word, topn=n_neighbor)\n",
    "            else:\n",
    "                  print(f'give a word to get the {n_neighbor} neighbor words')\n",
    "\n",
    "      def find_similar_words_self_implemented(self, topn, train_vocab, word):\n",
    "            top_words = {}\n",
    "            model_vocab = list(self.model.wv.vocab)\n",
    "            #all_embeddings = self.other_save_embeddings(train_vocab)\n",
    "            if word in train_vocab:\n",
    "                  if word in model_vocab:\n",
    "                        considered_vector = list(self.model.wv.__getitem__(word))\n",
    "                        top_words = get_similar_vectors_to_given_vector(topn, train_vocab, considered_vector, self.all_embeddings)\n",
    "            return top_words\n",
    "\n",
    "      def cluster_words(self, embedding_save_path = None, fig_path = None, n_components=3, text = False):\n",
    "            import umap.umap_ as umap\n",
    "            import time\n",
    "            import plotly.express as px\n",
    "            from sklearn import cluster\n",
    "            from sklearn import metrics\n",
    "\n",
    "            # read embedding from file\n",
    "            with open(Path.joinpath(embedding_save_path, f'{self.model_name}_vocab_embedding.txt')) as f:\n",
    "              lines = f.readlines()\n",
    "            embedding_data = []\n",
    "            words_data = []\n",
    "            for t in lines:\n",
    "              w = t.split(\"\\t\")[0]\n",
    "              v = [float(e) for e in t.split(\"\\t\")[1].split(\" \")]\n",
    "              words_data.append(w)\n",
    "              embedding_data.append(v)\n",
    "            # using kmean to get clusters of words\n",
    "            kmeans = cluster.KMeans(n_clusters=10)\n",
    "            kmeans.fit(embedding_data)\n",
    "            labels = kmeans.labels_\n",
    "            centroids = kmeans.cluster_centers_\n",
    "            #print(\"Cluster id labels for inputted data\")\n",
    "            #print(labels)\n",
    "            #print(\"Centroids data\")\n",
    "            #print(centroids)\n",
    "            # dimension reduction with umap\n",
    "            reducer = umap.UMAP(random_state=42,n_components=n_components)\n",
    "            embedding = reducer.fit_transform(embedding_data)\n",
    "            # show samples after dim-reduction in dataframe\n",
    "            if n_components == 3:\n",
    "                  wb = pd.DataFrame(embedding, columns=['x', 'y', 'z'])\n",
    "            else:\n",
    "                  wb = pd.DataFrame(embedding, columns=['x', 'y'])\n",
    "            wb['word'] = words_data\n",
    "            wb['cluster'] = ['cluster ' + str(c) for c in labels]\n",
    "            # visualization with plotply\n",
    "            if n_components==3:\n",
    "                  fig = px.scatter_3d(wb, \n",
    "                                    text = wb['word'],\n",
    "                                    x='x', y='y', z='z',\n",
    "                                    color = wb['cluster'],\n",
    "                                    title =\"word-embedding-samples\")\n",
    "            else:\n",
    "                  # n_components = 2\n",
    "                  if text:\n",
    "                    fig = px.scatter(wb, text = wb['word'], x='x', y='y', color=wb['cluster'], title='word embedding samples')\n",
    "                  else:\n",
    "                    fig = px.scatter(wb, x='x', y='y', color=wb['cluster'], title='word embedding samples')\n",
    "                    \n",
    "            fig.write_image(Path.joinpath(fig_path, f'embedding_space_dim_{n_components}.png'))\n",
    "            fig.write_html(Path.joinpath(fig_path, f'embedding_space_dim_{n_components}.html'))\n",
    "            fig.show()\n",
    "            return True\n",
    "\n",
    "  \n",
    "class BertEmbedding:\n",
    "    def __init__(self, saved_embeddings_text_file):\n",
    "          self.file_path = saved_embeddings_text_file\n",
    "          self.bert_vocab = None\n",
    "          self.bert_embeddings = None\n",
    "          self.bert_norms = None\n",
    "    def get_bert_embeddings(self, etm_vocab):\n",
    "          # filtering words by etm_vocab\n",
    "          words_in_vocab, vocab_embeddings = read_prefitted_embedding(\"bert\", etm_vocab, self.file_path)\n",
    "          self.bert_embeddings = np.array(vocab_embeddings)\n",
    "          self.bert_vocab = words_in_vocab\n",
    "          self.bert_norms = np.array()\n",
    "          print(\"bert-embedding ready!\")\n",
    "          return True\n",
    "\n",
    "    def find_similar_words(self, word, top_neighbors):\n",
    "          word_idx_in_vocab = self.bert_vocab.index(word)\n",
    "          considered_vector = self.bert_embeddings[word_idx_in_vocab]\n",
    "          top_words = get_similar_vectors_to_given_vector(top_neighbors, self.bert_vocab, considered_vector, self.bert_embeddings)\n",
    "          return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('prepared_data').mkdir(parents=True, exist_ok=True)\n",
    "Path(f'prepared_data/min_df_{min_df}').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_path = Path.joinpath(Path.cwd(), f'prepared_data/min_df_{min_df}')\n",
    "figures_path = Path.joinpath(Path.cwd(), f'figures/min_df_{min_df}')\n",
    "Path(figures_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "wb_creator = WordEmbeddingCreator(model_name=\"skipgram\", documents = docs_tr, save_path= save_path)\n",
    "wb_creator.train(min_count=0, embedding_size= 300)\n",
    "vocab = list(word2id.keys())\n",
    "wb_creator.create_and_save_vocab_embedding(vocab, save_path)\n",
    "\n",
    "print(\"neighbor words of some sample selected words\")\n",
    "for i in range(0,5):\n",
    "      print(f'neighbor of word {vocab[i]}')\n",
    "      print([r[0] for r in wb_creator.find_most_similar_words(n_neighbor=5, word=vocab[i])])\n",
    "      print([r[1] for r in wb_creator.find_most_similar_words(n_neighbor=5, word=vocab[i])])\n",
    "      print(100*\"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleichen ähnliche Wörter von Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word-embedding of the word-- chung: \n",
      "dim of vector: 300\n",
      "[('yang', 0.9316569566726685), ('coos', 0.9261748790740967), ('eecs', 0.9193140268325806), ('elroy', 0.9053330421447754), ('msd', 0.8993813991546631), ('orst', 0.8984891176223755), ('cae', 0.8945136070251465), ('speedy', 0.8906283378601074), ('starbase', 0.8893750905990601), ('sdsu', 0.8859439492225647)]\n"
     ]
    }
   ],
   "source": [
    "v = vocab[2]\n",
    "vec = list(wb_creator.model.wv.__getitem__(v))\n",
    "print(f'word-embedding of the word-- {v}: ')\n",
    "print(f'dim of vector: {len(vec)}')\n",
    "print(wb_creator.find_most_similar_words(n_neighbor=10, word=v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paperwork\n",
      "[0.99999976, 0.9316569, 0.9261748, 0.91931397, 0.905333, 0.89938134, 0.89848906, 0.8945134, 0.8906283, 0.8893751]\n",
      "[0.9316569, 0.9261748, 0.91931397, 0.905333, 0.89938134, 0.89848906, 0.8945134, 0.8906283, 0.8893751, 0.8859439]\n",
      "[6049, 2457, 2609, 2709, 5935, 8494, 7875, 4724, 8147, 7949]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'yang': 0.9316569,\n",
       " 'coos': 0.9261748,\n",
       " 'eecs': 0.91931397,\n",
       " 'elroy': 0.905333,\n",
       " 'msd': 0.89938134,\n",
       " 'orst': 0.89848906,\n",
       " 'cae': 0.8945134,\n",
       " 'speedy': 0.8906283,\n",
       " 'starbase': 0.8893751,\n",
       " 'sdsu': 0.8859439}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb_creator.find_similar_words_self_implemented(10, vocab, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleich ähnliche Wörter zwischen Word2Vec und Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
