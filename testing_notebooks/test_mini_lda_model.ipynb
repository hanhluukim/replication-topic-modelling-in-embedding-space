{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qh_7rTgfMSmH",
    "outputId": "f28a0b76-a99c-4528-cf8c-7f22101f841c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# example from https://github.com/susanli2016/NLP-with-Python/blob/master/LDA_news_headlines.ipynb\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "p_stemmer = PorterStemmer()\n",
    "en_stop = []\n",
    "\n",
    "# create sample documents: doc_set = dataset\n",
    "doc_a = \"Brocolli is good to eat. My brother likes to eat good brocolli, but not my mother.\"\n",
    "doc_b = \"My mother spends a lot of time driving my brother around to baseball practice.\"\n",
    "doc_c = \"Some health experts suggest that driving may cause increased tension and blood pressure.\"\n",
    "doc_d = \"I often feel pressure to perform well at school, but my mother never seems to drive my brother to do better.\"\n",
    "doc_e = \"Health professionals say that brocolli is good for your health.\" \n",
    "doc_set = [doc_a, doc_b, doc_c, doc_d, doc_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQ1wt06h-wvJ",
    "outputId": "5e7a1574-5fcf-47b8-c72a-d1cb6293cd58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'is', 'good', 'to', 'eat', 'my', 'brother', 'like', 'to', 'eat', 'good', 'brocolli', 'but', 'not', 'my', 'mother']\n",
      "['my', 'mother', 'spend', 'a', 'lot', 'of', 'time', 'drive', 'my', 'brother', 'around', 'to', 'basebal', 'practic']\n",
      "['some', 'health', 'expert', 'suggest', 'that', 'drive', 'may', 'caus', 'increas', 'tension', 'and', 'blood', 'pressur']\n",
      "['i', 'often', 'feel', 'pressur', 'to', 'perform', 'well', 'at', 'school', 'but', 'my', 'mother', 'never', 'seem', 'to', 'drive', 'my', 'brother', 'to', 'do', 'better']\n",
      "['health', 'profession', 'say', 'that', 'brocolli', 'is', 'good', 'for', 'your', 'health']\n"
     ]
    }
   ],
   "source": [
    "# tokenization and preprocessing\n",
    "texts = []\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "for i in range(0,len(texts)):\n",
    "  print(texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y4KCPj5nFoh3",
    "outputId": "da941f8b-392e-445b-f71f-1db210b22d0a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-b02f7d69-6c6d-4aaf-aa36-74de15409cf5\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>id</th>\n",
       "      <th>doc-freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brocolli</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brother</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>but</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eat</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>is</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>like</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mother</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>my</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>not</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>to</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>a</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>around</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>basebal</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>drive</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>lot</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>of</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>practic</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>spend</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>time</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>and</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>blood</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>caus</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>expert</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>health</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>increas</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>may</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>pressur</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>some</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>suggest</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>tension</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>that</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>at</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>better</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>do</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>feel</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>i</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>never</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>often</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>perform</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>school</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>seem</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>well</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>for</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>profession</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>say</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>your</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b02f7d69-6c6d-4aaf-aa36-74de15409cf5')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-b02f7d69-6c6d-4aaf-aa36-74de15409cf5 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-b02f7d69-6c6d-4aaf-aa36-74de15409cf5');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "          word  id  doc-freq\n",
       "0     brocolli   0         2\n",
       "1      brother   1         3\n",
       "2          but   2         2\n",
       "3          eat   3         1\n",
       "4         good   4         2\n",
       "5           is   5         2\n",
       "6         like   6         1\n",
       "7       mother   7         3\n",
       "8           my   8         3\n",
       "9          not   9         1\n",
       "10          to  10         3\n",
       "11           a  11         1\n",
       "12      around  12         1\n",
       "13     basebal  13         1\n",
       "14       drive  14         3\n",
       "15         lot  15         1\n",
       "16          of  16         1\n",
       "17     practic  17         1\n",
       "18       spend  18         1\n",
       "19        time  19         1\n",
       "20         and  20         1\n",
       "21       blood  21         1\n",
       "22        caus  22         1\n",
       "23      expert  23         1\n",
       "24      health  24         2\n",
       "25     increas  25         1\n",
       "26         may  26         1\n",
       "27     pressur  27         2\n",
       "28        some  28         1\n",
       "29     suggest  29         1\n",
       "30     tension  30         1\n",
       "31        that  31         2\n",
       "32          at  32         1\n",
       "33      better  33         1\n",
       "34          do  34         1\n",
       "35        feel  35         1\n",
       "36           i  36         1\n",
       "37       never  37         1\n",
       "38       often  38         1\n",
       "39     perform  39         1\n",
       "40      school  40         1\n",
       "41        seem  41         1\n",
       "42        well  42         1\n",
       "43         for  43         1\n",
       "44  profession  44         1\n",
       "45         say  45         1\n",
       "46        your  46         1"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.dictionary.Dictionary(texts)\n",
    "dictionary.id2token = { v:k for k, v in dictionary.token2id.items()}\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=[\"word\", \"id\", \"doc-freq\"])\n",
    "df[\"word\"] = dictionary.token2id.keys()\n",
    "df[\"id\"] = dictionary.token2id.values()\n",
    "df[\"doc-freq\"] = [dictionary.dfs[id] for id in dictionary.token2id.values()]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4nyTGTaENszX",
    "outputId": "6709c744-2731-4546-8e15-bb41ee954b94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brocolli', 'is', 'good', 'to', 'eat', 'my', 'brother', 'like', 'to', 'eat', 'good', 'brocolli', 'but', 'not', 'my', 'mother']\n",
      "[(0, 2), (1, 1), (2, 1), (3, 2), (4, 2), (5, 1), (6, 1), (7, 1), (8, 2), (9, 1), (10, 2)]\n"
     ]
    }
   ],
   "source": [
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(texts[0])\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vsq2QhmGMZ47",
    "outputId": "afd206bb-29be-47d7-e310-227bf5be4d35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.075*\"to\" + 0.075*\"my\" + 0.040*\"brother\" + 0.040*\"mother\"'), (1, '0.082*\"health\" + 0.048*\"that\" + 0.045*\"is\" + 0.044*\"good\"')]\n",
      "\n",
      "\n",
      "Topic: 0 Word: 0.075*\"to\" + 0.075*\"my\" + 0.040*\"brother\" + 0.040*\"mother\" + 0.040*\"drive\" + 0.029*\"but\" + 0.029*\"eat\" + 0.029*\"pressur\" + 0.029*\"brocolli\" + 0.029*\"good\"\n",
      "Topic: 1 Word: 0.082*\"health\" + 0.048*\"that\" + 0.045*\"is\" + 0.044*\"good\" + 0.044*\"brocolli\" + 0.044*\"profession\" + 0.044*\"for\" + 0.044*\"your\" + 0.044*\"say\" + 0.015*\"and\"\n",
      "\n",
      "\n",
      "[(0, '0.075*\"to\" + 0.075*\"my\" + 0.040*\"brother\"'), (1, '0.082*\"health\" + 0.048*\"that\" + 0.045*\"is\"')]\n"
     ]
    }
   ],
   "source": [
    "# generate LDA model\n",
    "# corpur can be doc2bow, doc2tfidf?\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20)\n",
    "\n",
    "# when choose n_topics = 2\n",
    "print(ldamodel.print_topics(num_topics=2, num_words=4))\n",
    "print(\"\\n\")\n",
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))\n",
    "\n",
    "# when choose n_topics = 3\n",
    "print(\"\\n\")\n",
    "print(ldamodel.print_topics(num_topics=3, num_words=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UwI1NDa694bK",
    "outputId": "5451727b-d33c-4862-c1b5-2c78916c623a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6420323252677917\t Topic: 0.082*\"health\" + 0.048*\"that\" + 0.045*\"is\" + 0.044*\"good\" + 0.044*\"brocolli\"\n",
      "Score: 0.35796764492988586\t Topic: 0.075*\"to\" + 0.075*\"my\" + 0.040*\"brother\" + 0.040*\"mother\" + 0.040*\"drive\"\n"
     ]
    }
   ],
   "source": [
    "# new document\n",
    "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(unseen_document.split(\" \"))\n",
    "\n",
    "for index, score in sorted(ldamodel[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, ldamodel.print_topic(index, 5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QxpS83wg_evu",
    "outputId": "63d7876c-0025-47aa-cf88-054943831ed0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.46510267502360986\n"
     ]
    }
   ],
   "source": [
    "# coherence score\n",
    "from gensim.models import CoherenceModel\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcLvISK6Pf6E"
   },
   "source": [
    "# **Mini-LDA-to-20NGs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhA1WvDEOMA5",
    "outputId": "91edf540-2c6c-4afc-98f6-e518ee842149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents 18846\n"
     ]
    }
   ],
   "source": [
    "# other test\n",
    "# download data and covert to the from that etm can understand and process\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train_data = fetch_20newsgroups(subset='train').data\n",
    "test_data = fetch_20newsgroups(subset='test').data\n",
    "documents = train_data\n",
    "documents.extend(test_data)\n",
    "print(f'Number of documents {len(documents)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZ6VvUKROhZF",
    "outputId": "6de541c8-4bae-4586-e13f-e1320fdded05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\",\n",
       " \"From: guykuo@carson.u.washington.edu (Guy Kuo)\\nSubject: SI Clock Poll - Final Call\\nSummary: Final call for SI clock reports\\nKeywords: SI,acceleration,clock,upgrade\\nArticle-I.D.: shelley.1qvfo9INNc3s\\nOrganization: University of Washington\\nLines: 11\\nNNTP-Posting-Host: carson.u.washington.edu\\n\\nA fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.\\n\\nGuy Kuo <guykuo@u.washington.edu>\\n\",\n",
       " 'From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\\nSubject: PB questions...\\nOrganization: Purdue University Engineering Computer Network\\nDistribution: usa\\nLines: 36\\n\\nwell folks, my mac plus finally gave up the ghost this weekend after\\nstarting life as a 512k way back in 1985.  sooo, i\\'m in the market for a\\nnew machine a bit sooner than i intended to be...\\n\\ni\\'m looking into picking up a powerbook 160 or maybe 180 and have a bunch\\nof questions that (hopefully) somebody can answer:\\n\\n* does anybody know any dirt on when the next round of powerbook\\nintroductions are expected?  i\\'d heard the 185c was supposed to make an\\nappearence \"this summer\" but haven\\'t heard anymore on it - and since i\\ndon\\'t have access to macleak, i was wondering if anybody out there had\\nmore info...\\n\\n* has anybody heard rumors about price drops to the powerbook line like the\\nones the duo\\'s just went through recently?\\n\\n* what\\'s the impression of the display on the 180?  i could probably swing\\na 180 if i got the 80Mb disk rather than the 120, but i don\\'t really have\\na feel for how much \"better\" the display is (yea, it looks great in the\\nstore, but is that all \"wow\" or is it really that good?).  could i solicit\\nsome opinions of people who use the 160 and 180 day-to-day on if its worth\\ntaking the disk size and money hit to get the active display?  (i realize\\nthis is a real subjective question, but i\\'ve only played around with the\\nmachines in a computer store breifly and figured the opinions of somebody\\nwho actually uses the machine daily might prove helpful).\\n\\n* how well does hellcats perform?  ;)\\n\\nthanks a bunch in advance for any info - if you could email, i\\'ll post a\\nsummary (news reading time is at a premium with finals just around the\\ncorner... :( )\\n--\\nTom Willis  \\\\  twillis@ecn.purdue.edu    \\\\    Purdue Electrical Engineering\\n---------------------------------------------------------------------------\\n\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\\nNietzsche\\n',\n",
       " 'From: jgreen@amber (Joe Green)\\nSubject: Re: Weitek P9000 ?\\nOrganization: Harris Computer Systems Division\\nLines: 14\\nDistribution: world\\nNNTP-Posting-Host: amber.ssd.csd.harris.com\\nX-Newsreader: TIN [version 1.1 PL9]\\n\\nRobert J.C. Kyanko (rob@rjck.UUCP) wrote:\\n> abraxis@iastate.edu writes in article <abraxis.734340159@class1.iastate.edu>:\\n> > Anyone know about the Weitek P9000 graphics chip?\\n> As far as the low-level stuff goes, it looks pretty nice.  It\\'s got this\\n> quadrilateral fill command that requires just the four points.\\n\\nDo you have Weitek\\'s address/phone number?  I\\'d like to get some information\\nabout this chip.\\n\\n--\\nJoe Green\\t\\t\\t\\tHarris Corporation\\njgreen@csd.harris.com\\t\\t\\tComputer Systems Division\\n\"The only thing that really scares me is a person with no sense of humor.\"\\n\\t\\t\\t\\t\\t\\t-- Jonathan Winters\\n',\n",
       " 'From: jcm@head-cfa.harvard.edu (Jonathan McDowell)\\nSubject: Re: Shuttle Launch Question\\nOrganization: Smithsonian Astrophysical Observatory, Cambridge, MA,  USA\\nDistribution: sci\\nLines: 23\\n\\nFrom article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\\n>>In article <C5JLwx.4H9.1@cs.cmu.edu>, ETRAT@ttacs1.ttu.edu (Pack Rat) writes...\\n>>>\"Clear caution & warning memory.  Verify no unexpected\\n>>>errors. ...\".  I am wondering what an \"expected error\" might\\n>>>be.  Sorry if this is a really dumb question, but\\n> \\n> Parity errors in memory or previously known conditions that were waivered.\\n>    \"Yes that is an error, but we already knew about it\"\\n> I\\'d be curious as to what the real meaning of the quote is.\\n> \\n> tom\\n\\n\\nMy understanding is that the \\'expected errors\\' are basically\\nknown bugs in the warning system software - things are checked\\nthat don\\'t have the right values in yet because they aren\\'t\\nset till after launch, and suchlike. Rather than fix the code\\nand possibly introduce new bugs, they just tell the crew\\n\\'ok, if you see a warning no. 213 before liftoff, ignore it\\'.\\n\\n - Jonathan\\n\\n\\n',\n",
       " 'From: dfo@vttoulu.tko.vtt.fi (Foxvog Douglas)\\nSubject: Re: Rewording the Second Amendment (ideas)\\nOrganization: VTT\\nLines: 58\\n\\nIn article <1r1eu1$4t@transfer.stratus.com> cdt@sw.stratus.com (C. D. Tavares) writes:\\n>In article <1993Apr20.083057.16899@ousrvr.oulu.fi>, dfo@vttoulu.tko.vtt.fi (Foxvog Douglas) writes:\\n>> In article <1qv87v$4j3@transfer.stratus.com> cdt@sw.stratus.com (C. D. Tavares) writes:\\n>> >In article <C5n3GI.F8F@ulowell.ulowell.edu>, jrutledg@cs.ulowell.edu (John Lawrence Rutledge) writes:\\n>\\n>> >> The massive destructive power of many modern weapons, makes the\\n>> >> cost of an accidental or crimial usage of these weapons to great.\\n>> >> The weapons of mass destruction need to be in the control of\\n>> >> the government only.  Individual access would result in the\\n>> >> needless deaths of millions.  This makes the right of the people\\n>> >> to keep and bear many modern weapons non-existant.\\n\\n>> >Thanks for stating where you\\'re coming from.  Needless to say, I\\n>> >disagree on every count.\\n\\n>> You believe that individuals should have the right to own weapons of\\n>> mass destruction?  I find it hard to believe that you would support a \\n>> neighbor\\'s right to keep nuclear weapons, biological weapons, and nerve\\n>> gas on his/her property.  \\n\\n>> If we cannot even agree on keeping weapons of mass destruction out of\\n>> the hands of individuals, can there be any hope for us?\\n\\n>I don\\'t sign any blank checks.\\n\\nOf course.  The term must be rigidly defined in any bill.\\n\\n>When Doug Foxvog says \"weapons of mass destruction,\" he means CBW and\\n>nukes.  When Sarah Brady says \"weapons of mass destruction\" she means\\n>Street Sweeper shotguns and semi-automatic SKS rifles.  \\n\\nI doubt she uses this term for that.  You are using a quote allegedly\\nfrom her, can you back it up?\\n\\n>When John\\n>Lawrence Rutledge says \"weapons of mass destruction,\" and then immediately\\n>follows it with:\\n\\n>>> The US has thousands of people killed each year by handguns,\\n>>> this number can easily be reduced by putting reasonable restrictions\\n>>> on them.\\n\\n>...what does Rutledge mean by the term?\\n\\nI read the article as presenting first an argument about weapons of mass\\ndestruction (as commonly understood) and then switching to other topics.\\nThe first point evidently was to show that not all weapons should be\\nallowed, and then the later analysis was, given this understanding, to\\nconsider another class.\\n\\n>cdt@rocket.sw.stratus.com   --If you believe that I speak for my company,\\n>OR cdt@vos.stratus.com        write today for my special Investors\\' Packet...\\n\\n\\n\\n-- \\ndoug foxvog\\ndouglas.foxvog@vtt.fi\\n',\n",
       " 'From: bmdelane@quads.uchicago.edu (brian manning delaney)\\nSubject: Brain Tumor Treatment (thanks)\\nReply-To: bmdelane@midway.uchicago.edu\\nOrganization: University of Chicago\\nLines: 12\\n\\nThere were a few people who responded to my request for info on\\ntreatment for astrocytomas through email, whom I couldn\\'t thank\\ndirectly because of mail-bouncing probs (Sean, Debra, and Sharon).  So\\nI thought I\\'d publicly thank everyone.\\n\\nThanks! \\n\\n(I\\'m sure glad I accidentally hit \"rn\" instead of \"rm\" when I was\\ntrying to delete a file last September. \"Hmmm... \\'News?\\' What\\'s\\nthis?\"....)\\n\\n-Brian\\n',\n",
       " 'From: bgrubb@dante.nmsu.edu (GRUBB)\\nSubject: Re: IDE vs SCSI\\nOrganization: New Mexico State University, Las Cruces, NM\\nLines: 44\\nDistribution: world\\nNNTP-Posting-Host: dante.nmsu.edu\\n\\nDXB132@psuvm.psu.edu writes:\\n>In article <1qlbrlINN7rk@dns1.NMSU.Edu>, bgrubb@dante.nmsu.edu (GRUBB) says:\\n>>In PC Magazine April 27, 1993:29 \"Although SCSI is twice as fasst as ESDI,\\n>>20% faster than IDE, and support up to 7 devices its acceptance ...has   \\n>>long been stalled by incompatability problems and installation headaches.\"\\n                                                                      \\n>I love it when magazine writers make stupid statements like that re:      \\n>performance. Where do they get those numbers? I\\'ll list the actual\\n>performance ranges, which should convince anyone that such a               \\n>statement is absurd:                                                     \\n>SCSI-I ranges from 0-5MB/s.                                                \\n>SCSI-II ranges from 0-40MB/s.            \\n>IDE ranges from 0-8.3MB/s.                          \\n>ESDI is always 1.25MB/s (although there are some non-standard versions)\\nALL this shows is that YOU don\\'t know much about SCSI.\\n\\nSCSI-1 {with a SCSI-1 controler chip} range is indeed 0-5MB/s\\nand that is ALL you have right about SCSI\\nSCSI-1 {With a SCSI-2 controller chip}: 4-6MB/s with 10MB/s burst {8-bit}\\n Note the INCREASE in SPEED, the Mac Quadra uses this version of SCSI-1\\n so it DOES exist. Some PC use this set up too.\\nSCSI-2 {8-bit/SCSI-1 mode}:          4-6MB/s with 10MB/s burst\\nSCSI-2 {16-bit/wide or fast mode}:  8-12MB/s with 20MB/s burst\\nSCSI-2 {32-bit/wide AND fast}:     15-20MB/s with 40MB/s burst\\n \\nBy your OWN data the \"Although SCSI is twice as fast as ESDI\" is correct\\nWith a SCSI-2 controller chip SCSI-1 can reach 10MB/s which is indeed\\n\"20% faster than IDE\" {120% of 8.3 is 9.96}. ALL these SCSI facts have been\\nposted to this newsgroup in my Mac & IBM info sheet {available by FTP on \\nsumex-aim.stanford.edu (36.44.0.6) in the info-mac/report as \\nmac-ibm-compare[version #].txt (It should be 173 but 161 may still be there)}\\n\\nPart of this problem is both Mac and IBM PC are inconsiant about what SCSI\\nis which.  Though it is WELL documented that the Quadra has a SCSI-2 chip\\nan Apple salesperson said \"it uses a fast SCSI-1 chip\" {Not at a 6MB/s,\\n10MB/s burst it does not. SCSI-1 is 5MB/s maximum synchronous and Quadra\\nuses ANsynchronous SCSI which is SLOWER}  It seems that Mac and IBM see\\nSCSI-1 interface and think \\'SCSI-1\\' when it maybe a SCSI-1 interface driven\\nin the machine by a SCSi-2 controller chip in 8-bit mode {Which is MUCH\\nFASTER then true SCSI-1 can go}.\\n\\nDon\\'t slam an article because you don\\'t understand what is going on.\\nOne reference for the Quadra\\'s SCSI-2 controller chip is \\n(Digital Review, Oct 21, 1991 v8 n33 p8(1)).\\n',\n",
       " 'From: holmes7000@iscsvax.uni.edu\\nSubject: WIn 3.0 ICON HELP PLEASE!\\nOrganization: University of Northern Iowa\\nLines: 10\\n\\nI have win 3.0 and downloaded several icons and BMP\\'s but I can\\'t figure out\\nhow to change the \"wallpaper\" or use the icons.  Any help would be appreciated.\\n\\n\\nThanx,\\n\\n-Brando\\n\\nPS Please E-mail me\\n\\n',\n",
       " \"From: kerr@ux1.cso.uiuc.edu (Stan Kerr)\\nSubject: Re: Sigma Designs Double up??\\nArticle-I.D.: ux1.C52u8x.B62\\nOrganization: University of Illinois at Urbana\\nLines: 29\\n\\njap10@po.CWRU.Edu (Joseph A. Pellettiere) writes:\\n\\n\\n>\\tI am looking for any information about the Sigma Designs\\n>\\tdouble up board.  All I can figure out is that it is a\\n>\\thardware compression board that works with AutoDoubler, but\\n>\\tI am not sure about this.  Also how much would one cost?\\n\\nI've had the board for over a year, and it does work with Diskdoubler,\\nbut not with Autodoubler, due to a licensing problem with Stac Technologies,\\nthe owners of the board's compression technology. (I'm writing this\\nfrom memory; I've lost the reference. Please correct me if I'm wrong.)\\n\\nUsing the board, I've had problems with file icons being lost, but it's\\nhard to say whether it's the board's fault or something else; however,\\nif I decompress the troubled file and recompress it without the board,\\nthe icon usually reappears. Because of the above mentioned licensing\\nproblem, the freeware expansion utility DD Expand will not decompress\\na board-compressed file unless you have the board installed.\\n\\nSince Stac has its own product now, it seems unlikely that the holes\\nin Autodoubler/Diskdoubler related to the board will be fixed.\\nWhich is sad, and makes me very reluctant to buy Stac's product since\\nthey're being so stinky. (But hey, that's competition.)\\n-- \\n\\nStan Kerr    \\nComputing & Communications Services Office, U of Illinois/Urbana\\nPhone: 217-333-5217  Email: stankerr@uiuc.edu   \\n\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mVmgOurbOl5u"
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# tokenization and preprocessing\n",
    "texts = []\n",
    "# loop through document list\n",
    "for i in documents:\n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in set(stopwords.words('english'))]\n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yYv-iSpAO_wQ",
    "outputId": "b3878081-0939-4e96-f41c-eaea0e438c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lerxst', 'wam', 'umd', 'edu', 'thing', 'subject', 'car', 'nntp', 'post', 'host', 'rac3', 'wam', 'umd', 'edu', 'organ', 'univers', 'maryland', 'colleg', 'park', 'line', '15', 'wonder', 'anyon', 'could', 'enlighten', 'car', 'saw', 'day', '2', 'door', 'sport', 'car', 'look', 'late', '60', 'earli', '70', 'call', 'bricklin', 'door', 'realli', 'small', 'addit', 'front', 'bumper', 'separ', 'rest', 'bodi', 'know', 'anyon', 'tellm', 'model', 'name', 'engin', 'spec', 'year', 'product', 'car', 'made', 'histori', 'whatev', 'info', 'funki', 'look', 'car', 'pleas', 'e', 'mail', 'thank', 'il', 'brought', 'neighborhood', 'lerxst']\n",
      "['guykuo', 'carson', 'u', 'washington', 'edu', 'guy', 'kuo', 'subject', 'si', 'clock', 'poll', 'final', 'call', 'summari', 'final', 'call', 'si', 'clock', 'report', 'keyword', 'si', 'acceler', 'clock', 'upgrad', 'articl', 'shelley', '1qvfo9innc3', 'organ', 'univers', 'washington', 'line', '11', 'nntp', 'post', 'host', 'carson', 'u', 'washington', 'edu', 'fair', 'number', 'brave', 'soul', 'upgrad', 'si', 'clock', 'oscil', 'share', 'experi', 'poll', 'pleas', 'send', 'brief', 'messag', 'detail', 'experi', 'procedur', 'top', 'speed', 'attain', 'cpu', 'rate', 'speed', 'add', 'card', 'adapt', 'heat', 'sink', 'hour', 'usag', 'per', 'day', 'floppi', 'disk', 'function', '800', '1', '4', 'floppi', 'especi', 'request', 'summar', 'next', 'two', 'day', 'pleas', 'add', 'network', 'knowledg', 'base', 'done', 'clock', 'upgrad', 'answer', 'poll', 'thank', 'guy', 'kuo', 'guykuo', 'u', 'washington', 'edu']\n",
      "['twilli', 'ec', 'ecn', 'purdu', 'edu', 'thoma', 'e', 'willi', 'subject', 'pb', 'question', 'organ', 'purdu', 'univers', 'engin', 'comput', 'network', 'distribut', 'usa', 'line', '36', 'well', 'folk', 'mac', 'plu', 'final', 'gave', 'ghost', 'weekend', 'start', 'life', '512k', 'way', 'back', '1985', 'sooo', 'market', 'new', 'machin', 'bit', 'sooner', 'intend', 'look', 'pick', 'powerbook', '160', 'mayb', '180', 'bunch', 'question', 'hope', 'somebodi', 'answer', 'anybodi', 'know', 'dirt', 'next', 'round', 'powerbook', 'introduct', 'expect', 'heard', '185c', 'suppos', 'make', 'appear', 'summer', 'heard', 'anymor', 'sinc', 'access', 'macleak', 'wonder', 'anybodi', 'info', 'anybodi', 'heard', 'rumor', 'price', 'drop', 'powerbook', 'line', 'like', 'one', 'duo', 'went', 'recent', 'impress', 'display', '180', 'could', 'probabl', 'swing', '180', 'got', '80mb', 'disk', 'rather', '120', 'realli', 'feel', 'much', 'better', 'display', 'yea', 'look', 'great', 'store', 'wow', 'realli', 'good', 'could', 'solicit', 'opinion', 'peopl', 'use', '160', '180', 'day', 'day', 'worth', 'take', 'disk', 'size', 'money', 'hit', 'get', 'activ', 'display', 'realiz', 'real', 'subject', 'question', 'play', 'around', 'machin', 'comput', 'store', 'breifli', 'figur', 'opinion', 'somebodi', 'actual', 'use', 'machin', 'daili', 'might', 'prove', 'help', 'well', 'hellcat', 'perform', 'thank', 'bunch', 'advanc', 'info', 'could', 'email', 'post', 'summari', 'news', 'read', 'time', 'premium', 'final', 'around', 'corner', 'tom', 'willi', 'twilli', 'ecn', 'purdu', 'edu', 'purdu', 'electr', 'engin', 'convict', 'danger', 'enemi', 'truth', 'lie', 'f', 'w', 'nietzsch']\n",
      "['jgreen', 'amber', 'joe', 'green', 'subject', 'weitek', 'p9000', 'organ', 'harri', 'comput', 'system', 'divis', 'line', '14', 'distribut', 'world', 'nntp', 'post', 'host', 'amber', 'ssd', 'csd', 'harri', 'com', 'x', 'newsread', 'tin', 'version', '1', '1', 'pl9', 'robert', 'j', 'c', 'kyanko', 'rob', 'rjck', 'uucp', 'wrote', 'abraxi', 'iastat', 'edu', 'write', 'articl', 'abraxi', '734340159', 'class1', 'iastat', 'edu', 'anyon', 'know', 'weitek', 'p9000', 'graphic', 'chip', 'far', 'low', 'level', 'stuff', 'goe', 'look', 'pretti', 'nice', 'got', 'quadrilater', 'fill', 'command', 'requir', 'four', 'point', 'weitek', 'address', 'phone', 'number', 'like', 'get', 'inform', 'chip', 'joe', 'green', 'harri', 'corpor', 'jgreen', 'csd', 'harri', 'com', 'comput', 'system', 'divis', 'thing', 'realli', 'scare', 'person', 'sens', 'humor', 'jonathan', 'winter']\n",
      "['jcm', 'head', 'cfa', 'harvard', 'edu', 'jonathan', 'mcdowel', 'subject', 'shuttl', 'launch', 'question', 'organ', 'smithsonian', 'astrophys', 'observatori', 'cambridg', 'usa', 'distribut', 'sci', 'line', '23', 'articl', 'c5owcb', 'n3p', 'world', 'std', 'com', 'tombak', 'world', 'std', 'com', 'tom', 'baker', 'articl', 'c5jlwx', '4h9', '1', 'cs', 'cmu', 'edu', 'etrat', 'ttacs1', 'ttu', 'edu', 'pack', 'rat', 'write', 'clear', 'caution', 'warn', 'memori', 'verifi', 'unexpect', 'error', 'wonder', 'expect', 'error', 'might', 'sorri', 'realli', 'dumb', 'question', 'pariti', 'error', 'memori', 'previous', 'known', 'condit', 'waiver', 'ye', 'error', 'alreadi', 'knew', 'curiou', 'real', 'mean', 'quot', 'tom', 'understand', 'expect', 'error', 'basic', 'known', 'bug', 'warn', 'system', 'softwar', 'thing', 'check', 'right', 'valu', 'yet', 'set', 'till', 'launch', 'suchlik', 'rather', 'fix', 'code', 'possibl', 'introduc', 'new', 'bug', 'tell', 'crew', 'ok', 'see', 'warn', '213', 'liftoff', 'ignor', 'jonathan']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(texts[:5])):\n",
    "  print(texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vE3iqmKwTBlf",
    "outputId": "4391981c-b605-4b83-92f1-0e1870fe0038"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-ab28eff7-9a01-400d-bf67-2a9d0ed48a32\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>id</th>\n",
       "      <th>doc-freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>addit</td>\n",
       "      <td>4</td>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>anyon</td>\n",
       "      <td>5</td>\n",
       "      <td>3560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bodi</td>\n",
       "      <td>6</td>\n",
       "      <td>682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bricklin</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>brought</td>\n",
       "      <td>8</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bumper</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>call</td>\n",
       "      <td>10</td>\n",
       "      <td>2818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>car</td>\n",
       "      <td>11</td>\n",
       "      <td>1101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>colleg</td>\n",
       "      <td>12</td>\n",
       "      <td>966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>could</td>\n",
       "      <td>13</td>\n",
       "      <td>3900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>day</td>\n",
       "      <td>14</td>\n",
       "      <td>2303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>door</td>\n",
       "      <td>15</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>e</td>\n",
       "      <td>16</td>\n",
       "      <td>2936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>earli</td>\n",
       "      <td>17</td>\n",
       "      <td>586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>edu</td>\n",
       "      <td>18</td>\n",
       "      <td>12416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>engin</td>\n",
       "      <td>19</td>\n",
       "      <td>1555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>enlighten</td>\n",
       "      <td>20</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>front</td>\n",
       "      <td>21</td>\n",
       "      <td>589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>funki</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>histori</td>\n",
       "      <td>23</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>host</td>\n",
       "      <td>24</td>\n",
       "      <td>8160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>il</td>\n",
       "      <td>25</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>info</td>\n",
       "      <td>26</td>\n",
       "      <td>1097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>know</td>\n",
       "      <td>27</td>\n",
       "      <td>5911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>late</td>\n",
       "      <td>28</td>\n",
       "      <td>515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lerxst</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>line</td>\n",
       "      <td>30</td>\n",
       "      <td>18796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>look</td>\n",
       "      <td>31</td>\n",
       "      <td>3626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>made</td>\n",
       "      <td>32</td>\n",
       "      <td>1877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>mail</td>\n",
       "      <td>33</td>\n",
       "      <td>2425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>maryland</td>\n",
       "      <td>34</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>model</td>\n",
       "      <td>35</td>\n",
       "      <td>688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>name</td>\n",
       "      <td>36</td>\n",
       "      <td>1649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>neighborhood</td>\n",
       "      <td>37</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>nntp</td>\n",
       "      <td>38</td>\n",
       "      <td>8054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>organ</td>\n",
       "      <td>39</td>\n",
       "      <td>18155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>park</td>\n",
       "      <td>40</td>\n",
       "      <td>435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>pleas</td>\n",
       "      <td>41</td>\n",
       "      <td>2888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>post</td>\n",
       "      <td>42</td>\n",
       "      <td>9778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>product</td>\n",
       "      <td>43</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>rac3</td>\n",
       "      <td>44</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>realli</td>\n",
       "      <td>45</td>\n",
       "      <td>2701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>rest</td>\n",
       "      <td>46</td>\n",
       "      <td>757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>saw</td>\n",
       "      <td>47</td>\n",
       "      <td>649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>separ</td>\n",
       "      <td>48</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>small</td>\n",
       "      <td>49</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab28eff7-9a01-400d-bf67-2a9d0ed48a32')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-ab28eff7-9a01-400d-bf67-2a9d0ed48a32 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-ab28eff7-9a01-400d-bf67-2a9d0ed48a32');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "            word  id  doc-freq\n",
       "0             15   0      1774\n",
       "1              2   1      4544\n",
       "2             60   2       539\n",
       "3             70   3       420\n",
       "4          addit   4       656\n",
       "5          anyon   5      3560\n",
       "6           bodi   6       682\n",
       "7       bricklin   7        10\n",
       "8        brought   8       394\n",
       "9         bumper   9        50\n",
       "10          call  10      2818\n",
       "11           car  11      1101\n",
       "12        colleg  12       966\n",
       "13         could  13      3900\n",
       "14           day  14      2303\n",
       "15          door  15       373\n",
       "16             e  16      2936\n",
       "17         earli  17       586\n",
       "18           edu  18     12416\n",
       "19         engin  19      1555\n",
       "20     enlighten  20        88\n",
       "21         front  21       589\n",
       "22         funki  22         8\n",
       "23       histori  23       706\n",
       "24          host  24      8160\n",
       "25            il  25       328\n",
       "26          info  26      1097\n",
       "27          know  27      5911\n",
       "28          late  28       515\n",
       "29        lerxst  29         5\n",
       "30          line  30     18796\n",
       "31          look  31      3626\n",
       "32          made  32      1877\n",
       "33          mail  33      2425\n",
       "34      maryland  34       166\n",
       "35         model  35       688\n",
       "36          name  36      1649\n",
       "37  neighborhood  37        64\n",
       "38          nntp  38      8054\n",
       "39         organ  39     18155\n",
       "40          park  40       435\n",
       "41         pleas  41      2888\n",
       "42          post  42      9778\n",
       "43       product  43       896\n",
       "44          rac3  44        13\n",
       "45        realli  45      2701\n",
       "46          rest  46       757\n",
       "47           saw  47       649\n",
       "48         separ  48       447\n",
       "49         small  49       985"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.dictionary.Dictionary(texts)\n",
    "dictionary.id2token = { v:k for k, v in dictionary.token2id.items()}\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=[\"word\", \"id\", \"doc-freq\"])\n",
    "df[\"word\"] = dictionary.token2id.keys()\n",
    "df[\"id\"] = dictionary.token2id.values()\n",
    "df[\"doc-freq\"] = [dictionary.dfs[id] for id in dictionary.token2id.values()]\n",
    "df.iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EIBeBoxWS05n",
    "outputId": "87c26fe0-0ea8-492d-f5ce-268bf2b42e7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lerxst', 'wam', 'umd', 'edu', 'thing', 'subject', 'car', 'nntp', 'post', 'host', 'rac3', 'wam', 'umd', 'edu', 'organ', 'univers', 'maryland', 'colleg', 'park', 'line', '15', 'wonder', 'anyon', 'could', 'enlighten', 'car', 'saw', 'day', '2', 'door', 'sport', 'car', 'look', 'late', '60', 'earli', '70', 'call', 'bricklin', 'door', 'realli', 'small', 'addit', 'front', 'bumper', 'separ', 'rest', 'bodi', 'know', 'anyon', 'tellm', 'model', 'name', 'engin', 'spec', 'year', 'product', 'car', 'made', 'histori', 'whatev', 'info', 'funki', 'look', 'car', 'pleas', 'e', 'mail', 'thank', 'il', 'brought', 'neighborhood', 'lerxst']\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 5), (12, 1), (13, 1), (14, 1), (15, 2), (16, 1), (17, 1), (18, 2), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 2), (30, 1), (31, 2), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 2), (57, 1), (58, 2), (59, 1), (60, 1), (61, 1)]\n"
     ]
    }
   ],
   "source": [
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(texts[0])\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-XtHto9oTSy5",
    "outputId": "250f6400-a414-4d60-dc99-8649073c786f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.018*\"com\" + 0.010*\"govern\" + 0.007*\"articl\" + 0.007*\"organ\" + 0.007*\"law\" + 0.007*\"state\" + 0.007*\"write\" + 0.006*\"public\" + 0.006*\"subject\" + 0.005*\"line\"\n",
      "Topic: 1 Word: 0.012*\"use\" + 0.009*\"file\" + 0.008*\"system\" + 0.007*\"1\" + 0.007*\"imag\" + 0.006*\"2\" + 0.006*\"drive\" + 0.006*\"window\" + 0.006*\"softwar\" + 0.006*\"do\"\n",
      "Topic: 2 Word: 0.103*\"_\" + 0.023*\"netcom\" + 0.020*\"__\" + 0.019*\"___\" + 0.018*\"com\" + 0.016*\"ohio\" + 0.014*\"ac\" + 0.012*\"state\" + 0.011*\"sandvik\" + 0.011*\"kent\"\n",
      "Topic: 3 Word: 0.018*\"god\" + 0.011*\"christian\" + 0.008*\"one\" + 0.006*\"jesu\" + 0.006*\"say\" + 0.005*\"believ\" + 0.005*\"would\" + 0.005*\"homosexu\" + 0.005*\"bibl\" + 0.005*\"peopl\"\n",
      "Topic: 4 Word: 0.069*\"x\" + 0.018*\"1\" + 0.013*\"uk\" + 0.010*\"newsread\" + 0.008*\"version\" + 0.008*\"tin\" + 0.008*\"ac\" + 0.008*\"use\" + 0.007*\"file\" + 0.007*\"line\"\n",
      "Topic: 5 Word: 0.032*\"com\" + 0.012*\"car\" + 0.011*\"line\" + 0.010*\"subject\" + 0.010*\"organ\" + 0.010*\"write\" + 0.009*\"articl\" + 0.006*\"edu\" + 0.005*\"post\" + 0.005*\"bike\"\n",
      "Topic: 6 Word: 0.009*\"book\" + 0.008*\"univers\" + 0.008*\"cc\" + 0.007*\"columbia\" + 0.006*\"bill\" + 0.006*\"theori\" + 0.006*\"larson\" + 0.006*\"au\" + 0.005*\"gari\" + 0.004*\"osrh\"\n",
      "Topic: 7 Word: 0.031*\"w\" + 0.019*\"stratu\" + 0.016*\"1\" + 0.011*\"sw\" + 0.008*\"cdt\" + 0.008*\"c\" + 0.006*\"_\" + 0.006*\"v\" + 0.005*\"u\" + 0.005*\"z\"\n",
      "Topic: 8 Word: 0.076*\"0\" + 0.069*\"1\" + 0.053*\"2\" + 0.038*\"3\" + 0.028*\"4\" + 0.022*\"5\" + 0.018*\"6\" + 0.015*\"7\" + 0.011*\"10\" + 0.011*\"00\"\n",
      "Topic: 9 Word: 0.031*\"window\" + 0.015*\"com\" + 0.012*\"de\" + 0.011*\"use\" + 0.011*\"line\" + 0.010*\"monitor\" + 0.010*\"subject\" + 0.009*\"organ\" + 0.009*\"microsoft\" + 0.009*\"mous\"\n",
      "Topic: 10 Word: 0.087*\"edu\" + 0.021*\"write\" + 0.020*\"univers\" + 0.020*\"line\" + 0.019*\"organ\" + 0.019*\"subject\" + 0.018*\"articl\" + 0.013*\"post\" + 0.011*\"cs\" + 0.010*\"host\"\n",
      "Topic: 11 Word: 0.018*\"key\" + 0.016*\"armenian\" + 0.010*\"chip\" + 0.010*\"clipper\" + 0.008*\"encrypt\" + 0.007*\"turkish\" + 0.005*\"turk\" + 0.005*\"secur\" + 0.005*\"algorithm\" + 0.004*\"escrow\"\n",
      "Topic: 12 Word: 0.018*\"game\" + 0.010*\"team\" + 0.009*\"year\" + 0.008*\"edu\" + 0.008*\"player\" + 0.007*\"play\" + 0.007*\"line\" + 0.006*\"ca\" + 0.006*\"organ\" + 0.006*\"subject\"\n",
      "Topic: 13 Word: 0.012*\"space\" + 0.008*\"nasa\" + 0.005*\"would\" + 0.005*\"price\" + 0.004*\"new\" + 0.004*\"use\" + 0.004*\"gov\" + 0.004*\"cost\" + 0.004*\"one\" + 0.004*\"launch\"\n",
      "Topic: 14 Word: 0.435*\"ax\" + 0.035*\"max\" + 0.025*\"q\" + 0.021*\"3\" + 0.012*\"p\" + 0.011*\"r\" + 0.011*\"7\" + 0.010*\"g9v\" + 0.009*\"g\" + 0.007*\"n\"\n",
      "Topic: 15 Word: 0.052*\"edu\" + 0.023*\"line\" + 0.023*\"subject\" + 0.023*\"organ\" + 0.018*\"post\" + 0.014*\"nntp\" + 0.014*\"host\" + 0.014*\"univers\" + 0.009*\"write\" + 0.008*\"articl\"\n",
      "Topic: 16 Word: 0.006*\"write\" + 0.006*\"peopl\" + 0.006*\"israel\" + 0.006*\"say\" + 0.006*\"edu\" + 0.006*\"one\" + 0.005*\"moral\" + 0.005*\"would\" + 0.005*\"jew\" + 0.005*\"articl\"\n",
      "Topic: 17 Word: 0.010*\"would\" + 0.009*\"one\" + 0.008*\"peopl\" + 0.008*\"get\" + 0.008*\"like\" + 0.007*\"go\" + 0.007*\"think\" + 0.006*\"know\" + 0.006*\"time\" + 0.005*\"say\"\n",
      "Topic: 18 Word: 0.008*\"use\" + 0.006*\"planet\" + 0.006*\"earth\" + 0.006*\"system\" + 0.005*\"medic\" + 0.004*\"henri\" + 0.004*\"studi\" + 0.004*\"research\" + 0.004*\"patient\" + 0.004*\"year\"\n",
      "Topic: 19 Word: 0.019*\"r\" + 0.017*\"p\" + 0.016*\"0\" + 0.016*\"g\" + 0.015*\"h\" + 0.015*\"b\" + 0.014*\"l\" + 0.014*\"k\" + 0.014*\"c\" + 0.014*\"1\"\n"
     ]
    }
   ],
   "source": [
    "# generate LDA model\n",
    "# corpur can be doc2bow, doc2tfidf?\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=20, id2word = dictionary, passes=20)\n",
    "for idx, topic in ldamodel.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "test_mini_lda_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
