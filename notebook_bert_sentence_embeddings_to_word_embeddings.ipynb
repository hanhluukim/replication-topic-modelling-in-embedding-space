{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanhluukim/replication-topic-modelling-in-embedding-space/blob/main/notebook_bert_sentence_embeddings_to_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltx1Xj-4KM94"
      },
      "source": [
        "#**Bert-Sentence/Subwords-Embeddings zum Wortembeddings**\n",
        "\n",
        "1. Das Notebook wurde hier nur genutzt, um die Richtigkeit der Implementierung zu kontrollieren\n",
        "2. Die Implementierung mit Bert, die für ETM dann benutzt wurde, ist in den folgenden Dateien:\n",
        "\n",
        "    - `src\\bert_embedding.py`\n",
        "    - `src\\bert_preparing.py`\n",
        "    - `src\\covert_embeddings.py`\n",
        "3. Verwendete Modelle: base-BERT und BertTokenizeFast"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformation von Subwords-Embeddings zum Wort-Embeddings**\n",
        "\n",
        "1. Originale Dokumenten werden mittels \".\" zu Sätzen gesplitted\n",
        "2. Es gibt zwei Menge von kurzen und langen Sätzen\n",
        "3. Die Sätze kurzer als 128 Tokens werden unverändert bleiben\n",
        "4. Die Sätze länger als 128 Tokens werden durch die Länge 128 und ein Fenster von 10 Tokens gesplittet"
      ],
      "metadata": {
        "id": "cz6gK71tLRg5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEt5DyniPBcW",
        "outputId": "d63d69ce-6c01-47b9-b475-95fcde264fb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/miss-luu/anaconda3/lib/python3.7/site-packages (4.3.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/miss-luu/anaconda3/lib/python3.7/site-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: requests in /home/miss-luu/anaconda3/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
            "Requirement already satisfied: sacremoses in /home/miss-luu/anaconda3/lib/python3.7/site-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/miss-luu/anaconda3/lib/python3.7/site-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /home/miss-luu/anaconda3/lib/python3.7/site-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: filelock in /home/miss-luu/anaconda3/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/miss-luu/.local/lib/python3.7/site-packages (from transformers) (2020.11.13)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/miss-luu/anaconda3/lib/python3.7/site-packages (from transformers) (4.60.0)\n",
            "Requirement already satisfied: packaging in /home/miss-luu/anaconda3/lib/python3.7/site-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /home/miss-luu/anaconda3/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/miss-luu/anaconda3/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /home/miss-luu/.local/lib/python3.7/site-packages (from packaging->transformers) (2.4.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /home/miss-luu/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/miss-luu/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/miss-luu/anaconda3/lib/python3.7/site-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/miss-luu/anaconda3/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /home/miss-luu/.local/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /home/miss-luu/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /home/miss-luu/.local/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n",
            "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.1 is available.\n",
            "You should consider upgrading via the '/home/miss-luu/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-I7OQILPvGv",
        "outputId": "2aeb04e2-03a4-4342-9e82-0c459a52ff4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: stop-words in /usr/local/lib/python3.7/dist-packages (2018.7.23)\n"
          ]
        }
      ],
      "source": [
        "!pip install stop-words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EJnBCguQeWE",
        "outputId": "ef07acfe-1cd1-42cb-dbb8-b0e20b678bc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "from stop_words import get_stop_words\n",
        "stop_words = get_stop_words('en')\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hdos3ORw_Ec0"
      },
      "source": [
        "# **Datenvorbearbeitung für BERT-Modell**\n",
        "\n",
        "Note: Dieses Notebook ist nur für die Kontrolle der Richtigkeit von Implementierung, damit die richtige Code in dem `src` eingebaut wird\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX21MYTEObtF"
      },
      "outputs": [],
      "source": [
        "# read train_data from 20newsgroups\n",
        "def read_raw_documents():\n",
        "    raw_documents = []\n",
        "    raw_labels = []\n",
        "    for i in range(0,len(newsgroups_train.data)):\n",
        "        raw_documents.append(newsgroups_train.data[i])\n",
        "        raw_labels.append(newsgroups_train.target[i])\n",
        "    return raw_documents, raw_labels\n",
        "\n",
        "def simple_preprocess(raw_documents):\n",
        "    def only_letters(tested_string):\n",
        "        for letter in tested_string:\n",
        "            if letter not in \"abcdefghijklmnopqrstuvwxyz\":\n",
        "                return False\n",
        "        return True\n",
        "    def clean_doc_for_bert(doc): \n",
        "        doc = doc.replace(\">\",\"\").lower()\n",
        "        word_list = word_tokenize(doc) #only using empty space and punctation for tokenization\n",
        "        cleaned = []\n",
        "        for w in word_list:\n",
        "            if w not in stop_words:\n",
        "                if w in string.punctuation or only_letters(w): #using only character from punctation and alpha characters\n",
        "                    if w in string.punctuation or len( set(w) ) > 1: #punctation with len 1 allowed but alpha word must be longer then 1\n",
        "                        cleaned.append( w)\n",
        "        return \" \".join(cleaned), cleaned  #save doc in string and in token-list         \n",
        "       \n",
        "    cleaned_documents = []\n",
        "    for doc in raw_documents:\n",
        "        doc_in_string, doc_in_token_list = clean_doc_for_bert(doc)\n",
        "        cleaned_documents.append(doc_in_string)\n",
        "    return cleaned_documents\n",
        "\n",
        "def transform_to_sentences_with_labels():\n",
        "    # we will not use labels\n",
        "    sentences_with_labels = []\n",
        "    return sentences_with_labels\n",
        "\n",
        "def fine_tune_bert():\n",
        "    # should to be trained?\n",
        "    # no, because for topic modelling, that is usupervised problem. We just find topics for the documents\n",
        "    # topic modelling no targets\n",
        "    return True\n",
        "\n",
        "def transform_to_sentences(docs): #no labels\n",
        "    data_as_sentences = []\n",
        "    for doc in docs:\n",
        "      for sent in doc.split(\". \"): #make sentences\n",
        "        updated_sent = \" \".join([t for t in sent.split(\" \") if len(t) > 1])\n",
        "        if len(updated_sent.split(\" \")) > 1:\n",
        "            data_as_sentences.append(updated_sent)\n",
        "        else:\n",
        "            if updated_sent not in data_as_sentences:\n",
        "                data_as_sentences.append(updated_sent)\n",
        "    return data_as_sentences\n",
        "\n",
        "def split_long_sentence(splitted_sent, given_len):\n",
        "    subsents = []\n",
        "   #for i in range(0,len(splitted_sent), given_len):\n",
        "    i=0\n",
        "    while i < len(splitted_sent): \n",
        "        if i == 0:\n",
        "            sub = \" \".join(splitted_sent[i:i+given_len])\n",
        "            subsents.append(sub)\n",
        "            i = i + given_len\n",
        "        if i!=0:\n",
        "            j = i + given_len - 5 #windown 5\n",
        "            if j + given_len <= len(splitted_sent):\n",
        "                sub = \" \".join(splitted_sent[j:j + given_len])\n",
        "                subsents.append(sub)\n",
        "            else:\n",
        "                sub = \" \".join(splitted_sent[j:])\n",
        "                if len(sub)>1:\n",
        "                    subsents.append(sub)\n",
        "            i = j + given_len\n",
        "    return subsents\n",
        "\n",
        "def handle_long_sentences(sentences, given_len):\n",
        "    # overlapped splitting sentence windown 5\n",
        "    subsents = []\n",
        "    deleted_long_sents = []\n",
        "    for sent in sentences:\n",
        "        splitted_sent = sent.split(\" \")\n",
        "        if len(splitted_sent) > given_len:\n",
        "          long_sent_subsents = split_long_sentence(splitted_sent, given_len)\n",
        "          subsents.extend(long_sent_subsents)\n",
        "          deleted_long_sents.append(sent)\n",
        "    # update sentences: remove and add subsents\n",
        "    for del_sent in deleted_long_sents:\n",
        "        sentences.remove(del_sent)\n",
        "    for add_sent in subsents:\n",
        "        sentences.append(add_sent)\n",
        "    return sentences\n",
        "\n",
        "def create_marked_senteces(sentences):\n",
        "    return ['[CLS] ' + sent.strip() + ' [SEP]' for sent in sentences]\n",
        "def save_sents_to_txt(shorted_sentences):\n",
        "    with open(r'./bert_sentences.txt', 'w') as fp:\n",
        "      for sent in shorted_sentences:\n",
        "          # write each item on a new line\n",
        "          fp.write(f'{sent} \\n')\n",
        "      print('saving sentences from bert-processing')\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBqLI7sVQPQ-",
        "outputId": "87d6c7f7-c5ab-4364-f631-8694a453cc4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading data:...\n",
            "11314\n",
            "preprocess data:...\n",
            "11314\n",
            "transform to sentences:...\n",
            "split sentences to 128 tokens:...\n",
            "saving sentences from bert-processing\n",
            "finished: ...\n"
          ]
        }
      ],
      "source": [
        "print(\"reading data:...\")\n",
        "raw_documents, _ = read_raw_documents()\n",
        "print(len(raw_documents))\n",
        "print(\"preprocess data:...\")\n",
        "preprocessed_docs = simple_preprocess(raw_documents)\n",
        "print(len(preprocessed_docs))\n",
        "print(\"transform to sentences:...\")\n",
        "sentences = transform_to_sentences(preprocessed_docs)\n",
        "print(\"split sentences to 128 tokens:...\")\n",
        "shorted_sentences =  handle_long_sentences(sentences, 128)\n",
        "marked_shorted_sentences = create_marked_senteces(shorted_sentences)\n",
        "# write sentences to txt files\n",
        "with open(r'bert_sentences.txt', 'w') as fp:\n",
        "    for sent in shorted_sentences:\n",
        "        # write each item on a new line\n",
        "        fp.write(f'{sent} \\n')\n",
        "    print('saving sentences from bert-processing')\n",
        "print(\"finished: ...\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktZVaf0YVDIF"
      },
      "source": [
        "# **Bert-Modell and Bert-TokenizerFast**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scUleQQQXbJr",
        "outputId": "675e4fca-1a24-4966-a864-b22dea70d508"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertModel(\n",
              "  (embeddings): BertEmbeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (token_type_embeddings): Embedding(2, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import BertTokenizerFast, BertTokenizer\n",
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizerfast = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28n_S3YdYngR"
      },
      "source": [
        "# **Funktionen für Bert-Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blyG2KaZWn2C"
      },
      "outputs": [],
      "source": [
        "def tokenizerfast_for_a_sent(sent, tokenizer):\n",
        "  this_sent_tokenizer = tokenizer(sent)\n",
        "  # index of token in the vocabulary\n",
        "  indexed_tokens = this_sent_tokenizer.input_ids\n",
        "  segments_ids = [1] * len(indexed_tokens)\n",
        "  # Convert inputs to PyTorch tensors\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "  tokens_ids_with_belonging_information = this_sent_tokenizer.word_ids()\n",
        "  return tokens_tensor, segments_tensors, tokens_ids_with_belonging_information\n",
        "  \n",
        "def reform_token_embeddings_of_sentence(full_outputs):\n",
        "  hidden_states = outputs[2]\n",
        "  token_embeddings = torch.stack(hidden_states, dim=0)\n",
        "  #print(token_embeddings.shape)\n",
        "  token_embeddings = torch.squeeze(token_embeddings, dim=1) # size= (n_hidden_layers, n_tokens, 768)\n",
        "  #print(token_embeddings.shape)\n",
        "  token_embeddings = token_embeddings.permute(1,0,2) # size= (n_tokens, n_hidden_layers, 768)\n",
        "  #print(token_embeddings.shape)\n",
        "  return token_embeddings \n",
        "\n",
        "def get_token_embeddings(reformed_token_embeddings):\n",
        "  # using sum four last layers\n",
        "  token_vecs_sum = []\n",
        "  #print(f'get-token-embedding-function: {reformed_token_embeddings.shape}')\n",
        "  for i, token in enumerate(reformed_token_embeddings): \n",
        "    sum_vec = torch.sum(token[-4:], dim=0)\n",
        "    token_vecs_sum.append(sum_vec)\n",
        "    #print(f'original {token.shape} and token-emb after sum {sum_vec.shape}')\n",
        "  return token_vecs_sum # size: n_tokens: 768\n",
        "\n",
        "def get_subwords_embeddings_of_word(bert_unique_token_id, tokenized_indices, tokens_embeddings):\n",
        "    belongging_embeddings_of_word = []\n",
        "    for idx, tokenizer_idx in enumerate(tokenized_indices):\n",
        "        if tokenizer_idx == bert_unique_token_id:\n",
        "            belongging_embeddings_of_word.append(tokens_embeddings[idx])\n",
        "    return torch.stack(belongging_embeddings_of_word, dim=0)\n",
        "\n",
        "def get_unique_embedding(embeddings=None, methode=\"mean\"):\n",
        "    #print(embeddings[0].shape)\n",
        "    if methode == \"mean\":\n",
        "        if embeddings.shape[0] == 1:\n",
        "          return torch.squeeze(embeddings, dim=0)\n",
        "        else:\n",
        "          mean_embedding = torch.mean(embeddings, dim=0) #torch.tensor([embeddings])#.mean()\n",
        "          return mean_embedding\n",
        "\n",
        "def need_to_update(sent_tokens_ids):\n",
        "    special_ids = [101, 102] #of CLS and SEP\n",
        "    for e in sent_tokens_ids:\n",
        "        if e in special_ids:\n",
        "            sent_tokens_ids.remove(e)\n",
        "    return sent_tokens_ids\n",
        "  \n",
        "def get_multiple_embeddings_for_words_in_sent(sent_tokens_ids, sent_outputs_tokens_embeddings):\n",
        "    # a word can be one time oder multiple times in a sentence\n",
        "    #print(f'tokens-ids in get_multiple_embeddings_: {sent_tokens_ids}')\n",
        "    sent_tokens_ids = need_to_update(sent_tokens_ids)\n",
        "    multiple_words_embeddings = []\n",
        "    unique_words_ids = list(set(sent_tokens_ids))\n",
        "    for unique_id in unique_words_ids:\n",
        "        belong_embeddings = get_subwords_embeddings_of_word(unique_id, sent_tokens_ids, sent_outputs_tokens_embeddings)\n",
        "        print(f'word-id: {unique_id} - belong-embeddings shape: {belong_embeddings.shape}')\n",
        "        # mean of belonging_embeddings to get embedding of whole word\n",
        "        word_embedding = get_unique_embedding(belong_embeddings, \"mean\")\n",
        "        #print(f'mean-word-id {unique_id} word-embedding {word_embedding.shape}')\n",
        "        multiple_words_embeddings.append(word_embedding)\n",
        "        #print(\"----------------------------------------------------------\")\n",
        "    return torch.stack(multiple_words_embeddings, dim=0)\n",
        "\n",
        "def get_indices_of_word_in_original_sent(word, splitted_original_sent):\n",
        "    indices = []\n",
        "    for i, e in enumerate(splitted_original_sent):\n",
        "        if e == word:\n",
        "            indices.append(i)\n",
        "    return indices\n",
        "\n",
        "def get_final_words_embeddings_in_sent(original_sent, sent_tokens_ids, sent_outputs_tokens_embeddings):\n",
        "    #import numpy as np\n",
        "    print(f'sentence-tokenizerfast-word-ids: {sent_tokens_ids}')\n",
        "    not_unique_words_embeddings = get_multiple_embeddings_for_words_in_sent(sent_tokens_ids, sent_outputs_tokens_embeddings)\n",
        "    print(f'total found embeddings in sent: {not_unique_words_embeddings.shape}')\n",
        "    original_words_list = original_sent.split(\" \")\n",
        "    print(f'original-splitted: {original_words_list}')\n",
        "    set_original_words_list = []\n",
        "    for e in original_words_list:\n",
        "        if e not in set_original_words_list:\n",
        "            set_original_words_list.append(e) #[e for e in original_sent if e not in ]\n",
        "    words_embeddings_in_sent_dict = {}\n",
        "    for word in set_original_words_list:\n",
        "        if word not in ['[CLS]', '[SEP]']:\n",
        "          word_indices = get_indices_of_word_in_original_sent(word, original_words_list)\n",
        "          print(f'word---- {word} ---- indices in original sent: {word_indices}')\n",
        "          # a word can have different-word-embeddings in the sentence, because a word can occur multple times\n",
        "          # each occurance has a different embedding for this word\n",
        "          different_occurrences_embeddings_of_word = not_unique_words_embeddings[word_indices]\n",
        "          print(f'test: {different_occurrences_embeddings_of_word.shape}')\n",
        "          mean_unique_word_embedding = get_unique_embedding(torch.tensor(different_occurrences_embeddings_of_word), \"mean\")\n",
        "          words_embeddings_in_sent_dict[word] = mean_unique_word_embedding\n",
        "    return words_embeddings_in_sent_dict\n",
        "\n",
        "def save_embeddings_in_sent_to_text(sent_id, words_embeddings_in_sent_dict):\n",
        "    with open(f'./sent_{str(sent_id)}_words_embeddings.txt', 'w') as fp:\n",
        "        for word, vector in words_embeddings_in_sent_dict.items():\n",
        "            # write each item on a new line\n",
        "            fp.write(f'{word}\\t')\n",
        "            for e in vector.tolist():\n",
        "                fp.write(f'{e} ')\n",
        "            fp.write(\"\\n\")\n",
        "        print('saving embeddings')\n",
        "    return True\n",
        "\n",
        "def save_embeddings_to_text(words_embeddings_in_sent_dict):\n",
        "    with open(r'./bert_words_embeddings.txt', 'a') as fp:\n",
        "      for word, vector in words_embeddings_in_sent_dict.items():\n",
        "          fp.write(f'{word}\\t')\n",
        "          for e in vector.tolist():\n",
        "            fp.write(f'{e} ')\n",
        "          fp.write(\"\\n\")\n",
        "      print('saving embeddings')\n",
        "    return True\n",
        "\n",
        "def vocabulary_embeddings_to_text(vocab_embeddings):\n",
        "    with open(r'./bert_vocab_embeddings.txt', 'w') as fp:\n",
        "      for word, vector in vocab_embeddings.items():\n",
        "          fp.write(f'{word}\\t')\n",
        "          for e in vector.tolist():\n",
        "            fp.write(f'{e} ')\n",
        "          fp.write(\"\\n\")\n",
        "      print('saving embeddings')\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzlHez2rm3yE"
      },
      "source": [
        "# **BertTokenizerFast**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz7IQKoQgQny",
        "outputId": "51eecba4-6146-4cb8-9059-90923fe64555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of found embeddings: 25\n",
            "sentence-tokenizerfast-word-ids: [None, 0, 1, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 13, 13, 14, 15, 16, 17, None]\n",
            "word-id: 0 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 1 - beling-embeddings shape: torch.Size([4, 768])\n",
            "word-id: 2 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 3 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 4 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 5 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 6 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 7 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 8 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 9 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 10 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 11 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 12 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 13 - beling-embeddings shape: torch.Size([3, 768])\n",
            "word-id: 14 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 15 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 16 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 17 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: None - beling-embeddings shape: torch.Size([2, 768])\n",
            "total found embeddings in sent: torch.Size([19, 768])\n",
            "original-splitted: ['[CLS]', 'lerxst', 'thing', 'subject', 'car', 'organization', 'university', 'maryland', 'college', 'park', 'lines', 'wondering', 'anyone', 'enlighten', 'car', 'saw', 'day', '[SEP]']\n",
            "word---- lerxst ---- indices in original sent: [1]\n",
            "test: torch.Size([1, 768])\n",
            "word---- thing ---- indices in original sent: [2]\n",
            "test: torch.Size([1, 768])\n",
            "word---- subject ---- indices in original sent: [3]\n",
            "test: torch.Size([1, 768])\n",
            "word---- car ---- indices in original sent: [4, 14]\n",
            "test: torch.Size([2, 768])\n",
            "word---- organization ---- indices in original sent: [5]\n",
            "test: torch.Size([1, 768])\n",
            "word---- university ---- indices in original sent: [6]\n",
            "test: torch.Size([1, 768])\n",
            "word---- maryland ---- indices in original sent: [7]\n",
            "test: torch.Size([1, 768])\n",
            "word---- college ---- indices in original sent: [8]\n",
            "test: torch.Size([1, 768])\n",
            "word---- park ---- indices in original sent: [9]\n",
            "test: torch.Size([1, 768])\n",
            "word---- lines ---- indices in original sent: [10]\n",
            "test: torch.Size([1, 768])\n",
            "word---- wondering ---- indices in original sent: [11]\n",
            "test: torch.Size([1, 768])\n",
            "word---- anyone ---- indices in original sent: [12]\n",
            "test: torch.Size([1, 768])\n",
            "word---- enlighten ---- indices in original sent: [13]\n",
            "test: torch.Size([1, 768])\n",
            "word---- saw ---- indices in original sent: [15]\n",
            "test: torch.Size([1, 768])\n",
            "word---- day ---- indices in original sent: [16]\n",
            "test: torch.Size([1, 768])\n",
            "saving embeddings\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ],
      "source": [
        "ex_sent = marked_shorted_sentences[0]\n",
        "tokens_tensor, segments_tensors, tokens_ids_with_belonging_information = tokenizerfast_for_a_sent(ex_sent, tokenizerfast)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(tokens_tensor, segments_tensors)\n",
        "    reformed = reform_token_embeddings_of_sentence(outputs)\n",
        "    sent_tokens_embeddings = get_token_embeddings(reformed)\n",
        "    print(f'number of found embeddings: {len(sent_tokens_embeddings)}')\n",
        "    words_embeddings_in_sent_dict = get_final_words_embeddings_in_sent(ex_sent, tokens_ids_with_belonging_information, sent_tokens_embeddings)\n",
        "    save_embeddings_in_sent_to_text(0, words_embeddings_in_sent_dict)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCyXezVQDc2y",
        "outputId": "175df2e5-99c2-49ff-ba0d-2411303be6eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLS] lerxst thing subject car organization university maryland college park lines wondering anyone enlighten car saw day [SEP]\n",
            "number of found embeddings: 25\n",
            "sentence-tokenizerfast-word-ids: [None, 0, 1, 1, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 13, 13, 14, 15, 16, 17, None]\n",
            "word-id: 0 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 1 - beling-embeddings shape: torch.Size([4, 768])\n",
            "word-id: 2 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 3 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 4 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 5 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 6 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 7 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 8 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 9 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 10 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 11 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 12 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 13 - beling-embeddings shape: torch.Size([3, 768])\n",
            "word-id: 14 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 15 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 16 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 17 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: None - beling-embeddings shape: torch.Size([2, 768])\n",
            "total found embeddings in sent: torch.Size([19, 768])\n",
            "original-splitted: ['[CLS]', 'lerxst', 'thing', 'subject', 'car', 'organization', 'university', 'maryland', 'college', 'park', 'lines', 'wondering', 'anyone', 'enlighten', 'car', 'saw', 'day', '[SEP]']\n",
            "word---- lerxst ---- indices in original sent: [1]\n",
            "test: torch.Size([1, 768])\n",
            "word---- thing ---- indices in original sent: [2]\n",
            "test: torch.Size([1, 768])\n",
            "word---- subject ---- indices in original sent: [3]\n",
            "test: torch.Size([1, 768])\n",
            "word---- car ---- indices in original sent: [4, 14]\n",
            "test: torch.Size([2, 768])\n",
            "word---- organization ---- indices in original sent: [5]\n",
            "test: torch.Size([1, 768])\n",
            "word---- university ---- indices in original sent: [6]\n",
            "test: torch.Size([1, 768])\n",
            "word---- maryland ---- indices in original sent: [7]\n",
            "test: torch.Size([1, 768])\n",
            "word---- college ---- indices in original sent: [8]\n",
            "test: torch.Size([1, 768])\n",
            "word---- park ---- indices in original sent: [9]\n",
            "test: torch.Size([1, 768])\n",
            "word---- lines ---- indices in original sent: [10]\n",
            "test: torch.Size([1, 768])\n",
            "word---- wondering ---- indices in original sent: [11]\n",
            "test: torch.Size([1, 768])\n",
            "word---- anyone ---- indices in original sent: [12]\n",
            "test: torch.Size([1, 768])\n",
            "word---- enlighten ---- indices in original sent: [13]\n",
            "test: torch.Size([1, 768])\n",
            "word---- saw ---- indices in original sent: [15]\n",
            "test: torch.Size([1, 768])\n",
            "word---- day ---- indices in original sent: [16]\n",
            "test: torch.Size([1, 768])\n",
            "saving embeddings\n",
            "---------------------------------------------------------------------------------------\n",
            "[CLS] sports car looked late early [SEP]\n",
            "number of found embeddings: 9\n",
            "sentence-tokenizerfast-word-ids: [None, 0, 1, 2, 3, 4, 5, 6, None]\n",
            "word-id: 0 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 1 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 2 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 3 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 4 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: 5 - beling-embeddings shape: torch.Size([1, 768])\n",
            "word-id: None - beling-embeddings shape: torch.Size([2, 768])\n",
            "word-id: 6 - beling-embeddings shape: torch.Size([1, 768])\n",
            "total found embeddings in sent: torch.Size([8, 768])\n",
            "original-splitted: ['[CLS]', 'sports', 'car', 'looked', 'late', 'early', '[SEP]']\n",
            "word---- sports ---- indices in original sent: [1]\n",
            "test: torch.Size([1, 768])\n",
            "word---- car ---- indices in original sent: [2]\n",
            "test: torch.Size([1, 768])\n",
            "word---- looked ---- indices in original sent: [3]\n",
            "test: torch.Size([1, 768])\n",
            "word---- late ---- indices in original sent: [4]\n",
            "test: torch.Size([1, 768])\n",
            "word---- early ---- indices in original sent: [5]\n",
            "test: torch.Size([1, 768])\n",
            "saving embeddings\n",
            "---------------------------------------------------------------------------------------\n",
            "saving embeddings\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab = {}\n",
        "\n",
        "for marked_sent in marked_shorted_sentences[:2]:\n",
        "  print(marked_sent)\n",
        "  tokens_tensor, segments_tensors, tokens_ids_with_belonging_information = tokenizerFast_for_a_sent(marked_sent, tokenizerfast)\n",
        "  with torch.no_grad():\n",
        "      outputs = model(tokens_tensor, segments_tensors)\n",
        "      reformed = reform_token_embeddings_of_sentence(outputs)\n",
        "      sent_tokens_embeddings = get_token_embeddings(reformed)\n",
        "      print(f'number of found embeddings: {len(sent_tokens_embeddings)}')\n",
        "      words_embeddings_in_sent_dict = get_final_words_embeddings_in_sent(marked_sent, tokens_ids_with_belonging_information, sent_tokens_embeddings)\n",
        "      save_embeddings_to_text(words_embeddings_in_sent_dict)\n",
        "      for word, vector in words_embeddings_in_sent_dict.items():\n",
        "        #print(word)\n",
        "        if word in vocab.keys():\n",
        "          #print(vector[:2])\n",
        "          sum_vector = vocab[word][1] + vector\n",
        "          #print(sum_vector)\n",
        "          count = vocab[word][0] + 1\n",
        "          vocab[word] = (count, sum_vector)\n",
        "        else:\n",
        "          #print(vector[:2])\n",
        "          vocab[word] = (1, vector)\n",
        "\n",
        "      del tokens_tensor\n",
        "      del segments_tensors\n",
        "      del outputs\n",
        "      del reformed\n",
        "      del sent_tokens_embeddings\n",
        "      del words_embeddings_in_sent_dict\n",
        "  print(\"---------------------------------------------------------------------------------------\")\n",
        "\n",
        "#update vocab over all sentences\n",
        "updated_vocab = {}\n",
        "for word, (count, sum_vector) in vocab.items():\n",
        "  updated_vocab[word] = (sum_vector/count)\n",
        "vocabulary_embeddings_to_text(updated_vocab)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BERT_Embedding_Notebook.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}