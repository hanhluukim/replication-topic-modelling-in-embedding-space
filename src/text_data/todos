1. Datensatz: 20NewGroups und New York Times
    + woher: 20NewGroups bereits aus dem sklearn
    + New York Times: https://github.com/moorissa/nmf_nyt/blob/master/nyt_data.txt
    + Stopwords Datensatz: download from Author-Ordner
2. Pre-Processing:
    + Stopwords filtern (für die Behauptung 1)
    + Tokenisieren
    + BOW-Repräsentation für Vokabular
3. Funktionen:
    + data_loader(url)
    + data_preprocess()
    + create_bow()

    
4. New York Times benutzen wir direkt die Datensatz in dem Github verlinkt, damit die Daten gleich sind


#
1. data prepraring for LDA and word-embedding
2. topic modelling with LDA (using multicore for efficient runtime)
3. script for evaluation
2. etm 
