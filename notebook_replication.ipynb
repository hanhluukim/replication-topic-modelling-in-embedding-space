{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanhluukim/replication-topic-modelling-in-embedding-space/blob/main/notebook_replication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ7P852F7yzU"
      },
      "source": [
        "# **Das Projekt aus dem Github klonen und in den Projektsordner**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riOxinNHJcIB",
        "outputId": "cf233a4c-4cf5-4fca-fda6-f035d9e771b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'replication-topic-modelling-in-embedding-space'...\n",
            "remote: Enumerating objects: 360, done.\u001b[K\n",
            "remote: Counting objects: 100% (135/135), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 360 (delta 81), reused 82 (delta 34), pack-reused 225\u001b[K\n",
            "Receiving objects: 100% (360/360), 4.71 MiB | 5.07 MiB/s, done.\n",
            "Resolving deltas: 100% (191/191), done.\n"
          ]
        }
      ],
      "source": [
        "#wenn die Ordner noch nicht geklont ist, soll dieser Fehler zuerst durchgeführt werden.\n",
        "!git clone https://github.com/hanhluukim/replication-topic-modelling-in-embedding-space.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_6em-5qJg5e",
        "outputId": "66c2f6b5-05bf-40c6-c2aa-f510250909aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/replication-topic-modelling-in-embedding-space\n"
          ]
        }
      ],
      "source": [
        "cd /content/replication-topic-modelling-in-embedding-space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AAG98vV1JCg"
      },
      "source": [
        "#**Die benötige Paketen für das Projekt mittels requirements.txt installieren**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcBay625sD5D",
        "outputId": "c33609cb-bccf-4340-9f01-770aa50ca0fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 2)) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 3)) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 4)) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 6)) (1.11.0+cu113)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 12.2 MB/s \n",
            "\u001b[?25hCollecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting plotly==5.7.0\n",
            "  Downloading plotly-5.7.0-py2.py3-none-any.whl (28.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.8 MB 46.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pathlib in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 10)) (1.0.1)\n",
            "Collecting pyyaml==5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 21.5 MB/s \n",
            "\u001b[?25hCollecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 79.9 MB 101 kB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly==5.7.0->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 9)) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly==5.7.0->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 9)) (8.0.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 1)) (6.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 6)) (4.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (21.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 48.9 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 48.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (4.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (4.64.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (3.0.8)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 8)) (0.51.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.6.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 63.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 8)) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 8)) (0.34.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (7.1.2)\n",
            "Building wheels for collected packages: umap-learn, pynndescent, sacremoses\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=0525575b5bdbf276a5b367ba592f87364251b539d14ce87245684ce392def1a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=85c86943d0668eddb5c512772d21e9d806fb3ece2da9d91b6289aa94dd0331ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f1/56/f80d72741e400345b5a5b50ec3d929aca581bf45e0225d5c50\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=fde4ecd3f2877a529b33c43c292da45bbb3c7abda186b18fead53daa93ee57a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built umap-learn pynndescent sacremoses\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, pynndescent, huggingface-hub, umap-learn, transformers, plotly, kaleido\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.5.0\n",
            "    Uninstalling plotly-5.5.0:\n",
            "      Successfully uninstalled plotly-5.5.0\n",
            "Successfully installed huggingface-hub-0.5.1 kaleido-0.2.1 plotly-5.7.0 pynndescent-0.5.6 pyyaml-5.4.1 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0 umap-learn-0.5.3\n"
          ]
        }
      ],
      "source": [
        "# Falls die Packages noch nicht installiert wurden, \n",
        "!pip install -r \"/content/replication-topic-modelling-in-embedding-space/requirements.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7xiPgja8eZe"
      },
      "source": [
        "# **Gebrauchte Paketen importieren**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV7KZhGq1P7g",
        "outputId": "b1f998c4-cf0f-422c-c4c7-2987f441e801"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  defaults = yaml.load(f)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import umap.umap_ as umap\n",
        "import time\n",
        "import plotly.express as px\n",
        "from sklearn import cluster\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzWqQhPQdJWV"
      },
      "source": [
        "# **Vorverarbeitung und BOW-Repräsentationen für Textdaten durchführen**\n",
        "1. Vocabular erstellen\n",
        "2. BOW-Repräsentationen für allen Teildatensätzen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1OCULr82pfgk"
      },
      "outputs": [],
      "source": [
        "from src.preprare_dataset import TextDataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy0PpjxEpbrR",
        "outputId": "a9b6e529-91e7-4552-b555-74be15d9310b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading texts: ...\n",
            "finished load!\n",
            "check some sample texts of the dataset\n",
            "['From', ':', 'lerxst', '@', 'wam', '.', 'umd', '.', 'edu', '(', \"where's\", 'my', 'thing', ')', 'Subject', ':', 'WHAT', 'car', 'is', 'this', '!', '?', 'Nntp', 'Posting', 'Host', ':', 'rac3', '.', 'wam', '.', 'umd', '.', 'edu', 'Organization', ':', 'University', 'of', 'Maryland', ',', 'College', 'Park', 'Lines', ':', '15', 'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.', 'It', 'was', 'a', '2', 'door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s', '/', 'early', '70s', '.', 'It', 'was', 'called', 'a', 'Bricklin', '.', 'The', 'doors', 'were', 'really', 'small', '.', 'In', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'This', 'is', 'all', 'I', 'know', '.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e', 'mail', '.', 'Thanks', ',', 'IL', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst']\n",
            "====================================================================================================\n",
            "['From', ':', 'guykuo', '@', 'carson', '.', 'u', '.', 'washington', '.', 'edu', '(', 'Guy', 'Kuo', ')', 'Subject', ':', 'SI', 'Clock', 'Poll', 'Final', 'Call', 'Summary', ':', 'Final', 'call', 'for', 'SI', 'clock', 'reports', 'Keywords', ':', 'SI', ',', 'acceleration', ',', 'clock', ',', 'upgrade', 'Article', 'I', '.', 'D', '.', ':', 'shelley', '.', '1qvfo9INNc3s', 'Organization', ':', 'University', 'of', 'Washington', 'Lines', ':', '11', 'NNTP', 'Posting', 'Host', ':', 'carson', '.', 'u', '.', 'washington', '.', 'edu', 'A', 'fair', 'number', 'of', 'brave', 'souls', 'who', 'upgraded', 'their', 'SI', 'clock', 'oscillator', 'have', 'shared', 'their', 'experiences', 'for', 'this', 'poll', '.', 'Please', 'send', 'a', 'brief', 'message', 'detailing', 'your', 'experiences', 'with', 'the', 'procedure', '.', 'Top', 'speed', 'attained', ',', 'CPU', 'rated', 'speed', ',', 'add', 'on', 'cards', 'and', 'adapters', ',', 'heat', 'sinks', ',', 'hour', 'of', 'usage', 'per', 'day', ',', 'floppy', 'disk', 'functionality', 'with', '800', 'and', '1', '.', '4', 'm', 'floppies', 'are', 'especially', 'requested', '.', 'I', 'will', 'be', 'summarizing', 'in', 'the', 'next', 'two', 'days', ',', 'so', 'please', 'add', 'to', 'the', 'network', 'knowledge', 'base', 'if', 'you', 'have', 'done', 'the', 'clock', 'upgrade', 'and', \"haven't\", 'answered', 'this', 'poll', '.', 'Thanks', '.', 'Guy', 'Kuo', '<', 'guykuo', '@', 'u', '.', 'washington', '.', 'edu', '>']\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# init TextDataLoader für die Datenquelle 20 News Groups\n",
        "# Daten abrufen vom Sklearn, tokenisieren und besondere Charaktern entfernen\n",
        "textsloader = TextDataLoader(source=\"20newsgroups\", train_size=None, test_size=None)\n",
        "textsloader.load_tokenize_texts(\"20newsgroups\")\n",
        "# Beispiel von Textdaten\n",
        "textsloader.show_example_raw_texts(n_docs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5odpQDJ7qPTt",
        "outputId": "ed3ab047-394c-483f-8cdf-16c25e3c655f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start: preprocessing: ...\n",
            "finised: preprocessing!\n"
          ]
        }
      ],
      "source": [
        "# Vorverarbeitung von Daten mit folgenden Schritten:\n",
        "textsloader.preprocess_texts(length_one_remove=True, punctuation_lower = True, stopwords_filter = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRRCNPa9qXfq",
        "outputId": "97089f2a-8865-40af-b5e1-85340d063449"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test-document-frequency: \n",
            "[[ 15  17  12  18  11  16  14  20  12  17  19 135  16  10  15  36  15  19\n",
            "   11  21  11  10  35  10  13  17  21  54  10  30  24  10  10  15  13  12\n",
            "   31  29  17  14  10  14  10  10  11  12  12  14  10  17  13  11  13  57\n",
            "   10  13  12  10  27  12  11  22  19  45  18  20  13  19  21  15  14  13\n",
            "   19  17  12  16  11  11  62  12  10  10  17  13  11  10  14  11  28  23\n",
            "   14  12  10  11  11  30  10  19  16  12  14  10  35  12  13  11  21  17\n",
            "   12  13  13  10  10  15  22  19  46  14  13  28  26  16  10  17  16  25\n",
            "   11  10  23  10  17  10  12  10  10  13 136  13  11  17  13  17  11  22\n",
            "   23  12  10  14  11  11  19  11  17  11  10  12  22  12  29  18  11  15\n",
            "   11  14  18  21  11  21  10  14  29  10  21  13  14  12  12  26  31  17\n",
            "   48  14  10  13  16  14  21  16  12  21  12  10  17  16  10  18  18  11\n",
            "   14  21  18  14  32  19  14 135  14  30  13  14  12  23  12  14  11  10\n",
            "   25  12  10  13  67  11  21  23  37  10  10  25  16 141  21  13  16  39\n",
            "   26  22  12  16  24  43  10  14  10  10  27  11  33  17  10  10  10  15\n",
            "   10  17  71  11  19  14  11  11  12  10  15  26  11  10  14  24  10  15\n",
            "   13  13  16  12  12  16  11  16  10  15  12  16  26  13  10  14  16  18\n",
            "   14  36  16  10  14  18  19  18  14  35  19  10  10  14  10  24  12  26\n",
            "   28  11  22  67  21  15  21  12  11  10  13  12  27  11  17  10  20  18\n",
            "   11 125  43  27  17  11  11  13  12  12  21  12  15  32  52  10  10  14\n",
            "  149  11  15  25  23  29]]\n",
            "vocab-size in df: 348\n",
            "start creating vocabulary ...\n",
            "length of the vocabulary: 348\n",
            "sample ten words of the vocabulary: ['strong', 'mine', 'miles', 'short', 'case', 'hell', 'program', 'order', 'work', 'message']\n",
            "length word2id list: 348\n",
            "length id2word list: 348\n",
            "finished: creating vocabulary\n"
          ]
        }
      ],
      "source": [
        "# Daten zerlegen für Train, Test und Validation. Erstellen Vocabular aus dem Trainset\n",
        "textsloader.split_and_create_voca_from_trainset(max_df=0.7, min_df=10, stopwords_remove_from_voca=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etzyjh_nqi19",
        "outputId": "4b41bd0c-09bf-447a-8b01-757996124575"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length train-documents-indices : 4561\n",
            "length of the vocabulary: 348\n",
            "\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 149\n",
            "min doc-id: 0\n",
            "all docs: 4561\n",
            "all words: 4561\n",
            "docidx unique 150\n",
            "words unique: 348\n",
            "ndocs: 150\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "need normalized bows\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 49\n",
            "min doc-id: 0\n",
            "all docs: 1555\n",
            "all words: 1555\n",
            "docidx unique 50\n",
            "words unique: 326\n",
            "ndocs: 50\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "need normalized bows\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 4, 5, 7, 8, 9, 10, 12]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 24\n",
            "min doc-id: 0\n",
            "all docs: 797\n",
            "all words: 797\n",
            "docidx unique 25\n",
            "words unique: 263\n",
            "ndocs: 25\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "need normalized bows\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 3, 4, 5, 6, 8, 12, 13]\n",
            "max word-id: 346\n",
            "min word-id: 0\n",
            "max doc-id: 24\n",
            "min doc-id: 0\n",
            "all docs: 758\n",
            "all words: 758\n",
            "docidx unique 25\n",
            "words unique: 263\n",
            "ndocs: 25\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "need normalized bows\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "max word-id: 347\n",
            "min word-id: 1\n",
            "max doc-id: 99\n",
            "min doc-id: 0\n",
            "all docs: 2998\n",
            "all words: 2998\n",
            "docidx unique 100\n",
            "words unique: 346\n",
            "ndocs: 100\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "need normalized bows\n"
          ]
        }
      ],
      "source": [
        "# Erstellen BOW-Repräsentation für ETM Modell\n",
        "for_lda_model = False \n",
        "word2id, id2word, train_set, test_set, val_set = textsloader.create_bow_and_savebow_for_each_set(for_lda_model=for_lda_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-DXUMguC8zM"
      },
      "source": [
        "# **Vocabular und IDs anzeigen als Beispiel**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "6RBJYhLHCfwy",
        "outputId": "24b3ff6c-dd1f-4a4b-e789-f2c4874bd9f5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-15a2704c-a299-4c2e-a3a9-c17d7f8581ed\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>strong</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mine</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>miles</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>short</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>case</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>sense</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>time</td>\n",
              "      <td>96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>money</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>nice</td>\n",
              "      <td>98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>error</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15a2704c-a299-4c2e-a3a9-c17d7f8581ed')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-15a2704c-a299-4c2e-a3a9-c17d7f8581ed button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-15a2704c-a299-4c2e-a3a9-c17d7f8581ed');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      word  id\n",
              "0   strong   0\n",
              "1     mine   1\n",
              "2    miles   2\n",
              "3    short   3\n",
              "4     case   4\n",
              "..     ...  ..\n",
              "95   sense  95\n",
              "96    time  96\n",
              "97   money  97\n",
              "98    nice  98\n",
              "99   error  99\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# show for samples: 100 word2id and id2 word\n",
        "word2id_df_100 = pd.DataFrame()\n",
        "word2id_df_100['word'] = list(word2id.keys())[:100]\n",
        "word2id_df_100['id'] = list(word2id.values())[:100]\n",
        "word2id_df_100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tupeI6Pw85_L"
      },
      "source": [
        "# **Die Größe von Datensätzen kontrollieren**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1d-5ji3qwE8",
        "outputId": "4b0aaffc-6d03-48d8-c318-7338d9ac869d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of the vocabulary after prprocessing ist: 348\n",
            "Size of train set: 150\n",
            "Size of val set: 100\n",
            "Size of test set: 50\n"
          ]
        }
      ],
      "source": [
        "# Kontrollieren die Größen von verschiedenen Datensätzen\n",
        "print(f'Size of the vocabulary after prprocessing ist: {len(textsloader.vocabulary)}')\n",
        "print(f'Size of train set: {len(train_set[\"tokens\"])}')\n",
        "print(f'Size of val set: {len(val_set[\"tokens\"])}')\n",
        "print(f'Size of test set: {len(test_set[\"test\"][\"tokens\"])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxQL5jQtDb1c"
      },
      "source": [
        "# **Dokumenten wiederstellen für Word2Vec Embedding**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "PDXEEBHfq3Cy",
        "outputId": "42cbc74d-0c86-4809-f265-9b0f28da346a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a825e57c-3e5d-4386-81e1-88dbcac5b44b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text-after-preprocessing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sale corporation distribution nntp posting hos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>windows line reply windows line pc line window...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cs reply world message apr originator cs nntp ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>computer systems division distribution world n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>required systems laboratory distribution usa a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>university newsreader tin version mike cc wrot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>washington guy memory university washington nn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>day article reply distribution world article c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>state software nntp posting host state state u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>washington question university washington nntp...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a825e57c-3e5d-4386-81e1-88dbcac5b44b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a825e57c-3e5d-4386-81e1-88dbcac5b44b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a825e57c-3e5d-4386-81e1-88dbcac5b44b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                             text-after-preprocessing\n",
              "0   sale corporation distribution nntp posting hos...\n",
              "1   windows line reply windows line pc line window...\n",
              "2   cs reply world message apr originator cs nntp ...\n",
              "3   computer systems division distribution world n...\n",
              "4   required systems laboratory distribution usa a...\n",
              "..                                                ...\n",
              "95  university newsreader tin version mike cc wrot...\n",
              "96  washington guy memory university washington nn...\n",
              "97  day article reply distribution world article c...\n",
              "98  state software nntp posting host state state u...\n",
              "99  washington question university washington nntp...\n",
              "\n",
              "[100 rows x 1 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# re-erstellen von Dokumenten nach der Vorverarbeitungen. Die Dokumenten sind in Wörtern und werden für Word-Embedding Training benutzt\n",
        "docs_tr, docs_t, docs_v = textsloader.get_docs_in_words_for_each_set()\n",
        "train_docs_df = pd.DataFrame()\n",
        "train_docs_df['text-after-preprocessing'] = [' '.join(doc) for doc in docs_tr[:100]]\n",
        "train_docs_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds_KuUTQrK5P"
      },
      "source": [
        "# **Word-Embedding trainieren mit dem Traindatensatz**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBexKIVf8Qs5",
        "outputId": "5bdf9a55-152f-4b92-ab66-f03d2fbfd016"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word-embedding train begins\n",
            "word-embedding train finished\n",
            "length of vocabulary from word-embedding model 348\n",
            "length of the vocabulary of prepraring-dataset-vocabulary: 348\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 348/348 [00:00<00:00, 22785.17it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from src.embedding import WordEmbeddingCreator\n",
        "save_path = Path.joinpath(Path.cwd(), \"vocab_embedding.txt\")\n",
        "wb_creator = WordEmbeddingCreator(model_name=\"cbow\", documents = docs_tr, save_path= save_path)\n",
        "wb_creator.train(min_count=0, embedding_size= 10)\n",
        "vocab = list(word2id.keys())\n",
        "wb_creator.create_and_save_vocab_embedding(vocab, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f23xipx7MSV4",
        "outputId": "de5891be-e488-4148-e441-2b80e7f7068f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word-embedding of the word-- sale: \n",
            "vector: [0.009344382, 0.012920203, -0.0023479077, 0.033772264, 0.063363284, 0.020490194, -0.029590111, -0.11440278, 0.03663475, -0.030568153]\n",
            "dim of vector: 10\n"
          ]
        }
      ],
      "source": [
        "v = list(wb_creator.model.wv.vocab)[0]\n",
        "vec = list(wb_creator.model.wv.__getitem__(v))\n",
        "print(f'word-embedding of the word-- {v}: ')\n",
        "print(f'vector: {vec}')\n",
        "print(f'dim of vector: {len(vec)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l53_jkUS-hl-"
      },
      "source": [
        "# **Word-Embeddings visualieren als Beispiel**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "o96LsIWkNrZS"
      },
      "outputs": [],
      "source": [
        "# read word-embedding files\n",
        "with open(save_path) as f:\n",
        "  lines = f.readlines()\n",
        "embedding_data = []\n",
        "words_data = []\n",
        "for t in lines:\n",
        "  w = t.split(\"\\t\")[0]\n",
        "  v = [float(e) for e in t.split(\"\\t\")[1].split(\" \")]\n",
        "  words_data.append(w)\n",
        "  embedding_data.append(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99hYOKPwB5aw",
        "outputId": "42996172-a687-45ce-c9c0-0efe1c1d3722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster id labels for inputted data\n",
            "[1 1 2 7 7 7 4 9 5 6 6 8 4 8 4 6 4 2 4 1 1 5 2 4 0 0 0 2 9 8 2 2 9 5 1 6 2\n",
            " 8 4 4 4 8 2 7 8 4 1 6 2 7 7 2 8 3 4 4 5 8 2 6 9 3 7 6 7 7 6 9 4 7 4 3 5 7\n",
            " 4 6 6 4 6 7 5 3 6 5 0 8 6 8 8 6 5 5 6 6 6 8 2 7 3 0 8 4 1 7 4 9 9 7 7 0 9\n",
            " 8 2 2 2 8 4 9 0 4 1 2 7 9 1 1 3 2 5 9 4 4 4 7 9 5 5 8 3 9 6 6 9 7 4 1 7 9\n",
            " 7 7 8 4 7 4 0 7 4 6 3 6 5 4 9 7 6 9 4 4 8 1 3 7 2 0 7 1 3 7 0 4 8 4 6 8 9\n",
            " 4 1 4 2 2 8 2 8 1 4 2 2 7 7 9 1 6 9 4 5 5 9 6 8 0 7 8 3 6 1 7 1 8 1 3 7 6\n",
            " 6 6 0 8 5 2 0 8 9 4 5 0 1 3 1 5 7 9 4 7 6 5 9 6 8 1 8 4 6 6 6 9 6 7 6 6 0\n",
            " 1 1 1 4 2 4 7 4 4 3 5 1 6 4 1 6 9 2 3 4 7 2 6 5 4 1 5 9 8 1 5 6 1 3 2 4 2\n",
            " 3 3 2 1 2 7 1 9 7 4 4 2 7 4 2 2 6 4 1 9 7 4 6 3 3 3 8 7 5 6 2 3 2 1 6 4 9\n",
            " 8 4 8 7 9 1 2 9 2 5 3 9 3 6 5]\n",
            "Centroids data\n",
            "[[ 8.25416467e-02  1.00402704e-01 -6.16999657e-02  7.06811393e-03\n",
            "   7.20044294e-02  3.30265375e-02 -1.02687385e-01 -3.91052393e-01\n",
            "  -2.08210220e-02  6.31299027e-02]\n",
            " [ 3.44045872e-02  2.67566570e-02 -2.58173186e-02  1.56572979e-02\n",
            "   3.58078472e-02 -5.41884300e-03 -6.51039090e-02 -1.44770008e-01\n",
            "  -1.25012751e-03 -1.29668451e-03]\n",
            " [ 6.04465699e-02  6.41332945e-02 -2.28503957e-02  1.19839944e-02\n",
            "   3.94253233e-02  1.73603118e-02 -6.48704880e-02 -2.90748748e-01\n",
            "  -2.52345519e-02  5.42857835e-02]\n",
            " [ 5.10539079e-03  4.36672673e-02 -2.47966500e-03  1.97327603e-02\n",
            "   4.69075446e-03  1.17809980e-02 -3.74115768e-02 -7.02961628e-02\n",
            "  -2.62737097e-02  1.06971860e-02]\n",
            " [ 4.86366619e-02  5.81795500e-02 -2.88301412e-02  2.17010789e-02\n",
            "   4.13021831e-02 -2.74763404e-03 -6.84910932e-02 -2.09806519e-01\n",
            "   1.63461102e-03  4.99252657e-02]\n",
            " [ 8.09009488e-03 -1.81613808e-03 -7.71342369e-03 -1.58663838e-02\n",
            "   7.94200562e-03 -1.93587427e-02 -5.19974542e-03 -8.24979816e-02\n",
            "  -1.06110908e-02  2.45363615e-04]\n",
            " [ 2.87970186e-02  4.43345663e-02 -1.06280663e-02  2.28418170e-03\n",
            "   1.41113549e-02  2.15581109e-02 -3.04751286e-02 -1.65848870e-01\n",
            "  -9.82217913e-03  2.95005572e-02]\n",
            " [ 6.94229219e-02  8.06978695e-02 -5.60012326e-02  4.04095741e-02\n",
            "   6.71807650e-02  1.70237903e-02 -7.51310211e-02 -2.68736465e-01\n",
            "  -2.34035092e-02  2.61211293e-02]\n",
            " [ 6.13530105e-02  4.34672494e-02 -4.88896931e-02  2.63186788e-02\n",
            "   4.88167531e-02  8.93091782e-03 -2.86925915e-02 -2.05538389e-01\n",
            "  -4.33688105e-02  3.13917584e-02]\n",
            " [ 3.87805355e-02  2.42323155e-02 -2.02810837e-02 -5.96502061e-04\n",
            "   3.61574380e-02  2.27377320e-02 -4.40845770e-03 -9.31705202e-02\n",
            "  -2.21984045e-03  2.54670407e-02]]\n"
          ]
        }
      ],
      "source": [
        "# clustering words with KMeans and Words-Vectors\n",
        "kmeans = cluster.KMeans(n_clusters=10)\n",
        "kmeans.fit(embedding_data)\n",
        " \n",
        "labels = kmeans.labels_\n",
        "centroids = kmeans.cluster_centers_\n",
        " \n",
        "print (\"Cluster id labels for inputted data\")\n",
        "print (labels)\n",
        "print (\"Centroids data\")\n",
        "print (centroids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAMgZ9aIE9A6",
        "outputId": "6b6c7bc6-165f-4da7-c28b-4b9d75aa03ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  warnings.warn(problem)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Duration: 17.860264778137207 seconds\n"
          ]
        }
      ],
      "source": [
        "# dimension reduction with umap\n",
        "start = time.time()\n",
        "reducer = umap.UMAP(random_state=42,n_components=3)\n",
        "embedding = reducer.fit_transform(embedding_data)\n",
        "print('Duration: {} seconds'.format(time.time() - start))\n",
        "\n",
        "# show samples after dim-reduction in dataframe\n",
        "wb = pd.DataFrame(embedding, columns=['x', 'y', 'z'])\n",
        "wb['word'] = words_data\n",
        "wb['cluster'] = ['cluster ' + str(c) for c in labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "spomMOt_yy0W",
        "outputId": "28117805-94e9-4669-a96f-3eab8ebae36f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.11.1.min.js\"></script>                <div id=\"55bcada3-8e00-45c9-ac5c-1f0908fdc79d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"55bcada3-8e00-45c9-ac5c-1f0908fdc79d\")) {                    Plotly.newPlot(                        \"55bcada3-8e00-45c9-ac5c-1f0908fdc79d\",                        [{\"hovertemplate\":\"cluster=cluster 1<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 1\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 1\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"sale\",\"corporation\",\"apr\",\"originator\",\"phone\",\"nice\",\"stop\",\"disk\",\"keywords\",\"de\",\"international\",\"religion\",\"history\",\"thinking\",\"reading\",\"sci\",\"answer\",\"steve\",\"considered\",\"error\",\"possibly\",\"situation\",\"taking\",\"numbers\",\"making\",\"org\",\"change\",\"high\",\"late\",\"summary\",\"worse\",\"assuming\",\"wrong\",\"hp\",\"parts\"],\"x\":[7.273033142089844,7.370299339294434,6.427645206451416,6.4694504737854,4.796447277069092,5.782594203948975,5.672845840454102,6.168217658996582,5.820353031158447,5.371102333068848,6.180684566497803,6.576564788818359,5.973232269287109,5.696743011474609,4.921462535858154,6.585013389587402,5.765857696533203,5.348556995391846,5.236345291137695,6.425394058227539,6.373933792114258,6.0526204109191895,5.33580207824707,4.859206676483154,5.719041347503662,4.657007694244385,6.126739025115967,5.307193756103516,5.2167744636535645,5.3379034996032715,5.458893775939941,6.5256147384643555,4.703683853149414,5.66353178024292,5.852900981903076],\"y\":[9.347469329833984,9.324723243713379,7.924280166625977,8.266753196716309,8.720129013061523,7.732641220092773,8.684489250183105,7.3436503410339355,7.058541297912598,8.179031372070312,7.40635347366333,8.160910606384277,7.161128044128418,6.973135471343994,7.156322479248047,7.937820911407471,7.46952486038208,7.221852779388428,7.373946666717529,7.973505020141602,8.411879539489746,7.194336414337158,7.1767120361328125,6.585672378540039,8.352045059204102,6.929333686828613,7.635289669036865,7.472371578216553,7.380148410797119,6.755626201629639,7.625776290893555,8.319615364074707,7.117684364318848,8.406371116638184,7.04007625579834],\"z\":[6.530817985534668,6.695122718811035,6.209969520568848,6.531128406524658,6.456295967102051,5.9519362449646,6.0447516441345215,6.764069557189941,6.209292411804199,6.031586647033691,6.318476676940918,6.481921672821045,6.7970685958862305,6.2635416984558105,6.921863079071045,6.3764472007751465,7.358607769012451,6.991843223571777,6.197427749633789,6.213619709014893,6.172485828399658,6.346560478210449,7.068771839141846,6.176653861999512,5.705045223236084,6.619593620300293,6.712413787841797,7.163906097412109,6.733941078186035,6.123225688934326,6.0851240158081055,6.438430309295654,6.417263507843018,5.778404235839844,6.694143772125244],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 2<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 2\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 2\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"distribution\",\"world\",\"computer\",\"great\",\"talking\",\"real\",\"usa\",\"wrote\",\"number\",\"person\",\"give\",\"ca\",\"found\",\"cc\",\"sun\",\"back\",\"au\",\"book\",\"case\",\"thought\",\"mark\",\"government\",\"hold\",\"left\",\"group\",\"access\",\"data\",\"based\",\"gov\",\"population\",\"find\",\"ibm\",\"control\",\"public\",\"gun\",\"children\",\"game\",\"source\"],\"x\":[3.0730724334716797,2.215080976486206,2.503338575363159,1.7091370820999146,3.240811586380005,2.1677446365356445,2.0684194564819336,1.7060081958770752,2.7239043712615967,3.213454008102417,2.9668068885803223,2.767655372619629,3.386505603790283,2.6159043312072754,2.9212026596069336,1.507367730140686,3.2480618953704834,2.7205941677093506,2.8498854637145996,3.2229599952697754,2.644974708557129,1.9444972276687622,2.4837541580200195,2.2459120750427246,2.97328519821167,3.5295753479003906,1.649693250656128,1.9623056650161743,2.892246961593628,1.9716554880142212,3.2355785369873047,2.5497829914093018,1.6382489204406738,2.6249608993530273,3.056922435760498,2.281737804412842,2.539616107940674,1.8224176168441772],\"y\":[6.593036651611328,6.368471622467041,6.185797691345215,7.367760181427002,6.61018180847168,7.081378936767578,7.488129615783691,7.202749729156494,6.4222731590271,7.2287445068359375,6.679948329925537,6.310821056365967,6.790972709655762,6.922300815582275,6.435379505157471,6.883406639099121,6.826473712921143,6.45746374130249,6.4953107833862305,6.9083476066589355,7.555046558380127,6.433144569396973,7.556535720825195,7.116942882537842,6.433444499969482,6.612092018127441,6.556166648864746,6.9840989112854,6.605807304382324,6.507462501525879,6.826725006103516,7.468539714813232,7.133745193481445,7.477578163146973,7.047593593597412,7.282373905181885,6.713366985321045,7.329815864562988],\"z\":[3.755361557006836,3.710092306137085,3.8726775646209717,4.165040969848633,4.2879438400268555,3.592165231704712,4.436849594116211,4.237060546875,3.4858829975128174,3.9549031257629395,3.5932812690734863,3.697565793991089,4.419568061828613,4.138623237609863,3.653244972229004,4.138718128204346,3.8353497982025146,3.458472490310669,3.5364434719085693,3.9144327640533447,4.5807342529296875,4.537903785705566,4.5074663162231445,3.996558666229248,3.898224353790283,4.423986911773682,4.231871604919434,3.792243242263794,4.1794352531433105,4.1177849769592285,3.7940313816070557,4.465784549713135,4.480114936828613,4.4320597648620605,4.432284832000732,4.499205589294434,3.9651827812194824,4.406134605407715],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 7<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 7\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 7\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"nntp\",\"posting\",\"host\",\"chip\",\"information\",\"thing\",\"word\",\"state\",\"today\",\"true\",\"bike\",\"opinions\",\"john\",\"heard\",\"understanding\",\"major\",\"run\",\"technology\",\"email\",\"card\",\"bit\",\"ram\",\"problem\",\"strong\",\"god\",\"life\",\"question\",\"matter\",\"code\",\"things\",\"center\",\"place\",\"called\",\"issue\",\"news\",\"law\",\"car\",\"space\",\"times\",\"drive\",\"remember\",\"ac\",\"turn\",\"front\"],\"x\":[1.5119446516036987,1.6190205812454224,1.8636512756347656,2.2617199420928955,2.7495265007019043,1.7566404342651367,2.2696926593780518,2.2490639686584473,1.9597867727279663,2.1837525367736816,1.935636043548584,2.1194896697998047,2.5122039318084717,2.0673210620880127,1.4410476684570312,2.195801019668579,1.9057989120483398,1.675871729850769,2.622175455093384,2.0593180656433105,1.708052158355713,1.8719252347946167,1.5047805309295654,2.6115686893463135,1.5182998180389404,2.034022331237793,2.405673027038574,2.4477527141571045,2.0415897369384766,1.5346115827560425,2.4872493743896484,2.4470460414886475,1.5521339178085327,2.176713228225708,2.3299427032470703,2.4782214164733887,1.680374026298523,4.244670867919922,2.198688268661499,1.7839288711547852,2.402736186981201,2.0731570720672607,1.617229700088501,1.7354671955108643],\"y\":[6.565737724304199,6.955364227294922,6.249622344970703,6.840416431427002,6.3581085205078125,6.483118057250977,7.23807430267334,7.159277439117432,6.945725917816162,6.0251851081848145,6.406116008758545,6.586239814758301,7.3302693367004395,6.378247261047363,6.723032474517822,7.459883213043213,6.99599027633667,6.420912742614746,7.269701957702637,6.472762107849121,6.4575090408325195,6.88961124420166,6.379123687744141,6.158057689666748,6.438048839569092,7.197402477264404,7.0697760581970215,6.021862030029297,6.2354512214660645,6.59628438949585,6.49420690536499,6.043994426727295,6.434977054595947,7.263047695159912,6.738256931304932,6.810518741607666,6.309782028198242,6.343760013580322,7.190864086151123,6.765517234802246,7.209301471710205,6.915480136871338,6.824263572692871,6.396414279937744],\"z\":[5.134727954864502,4.0763654708862305,4.672896862030029,5.5132598876953125,5.333057880401611,5.11794900894165,3.9227170944213867,3.6134727001190186,5.3122663497924805,4.49736213684082,5.160219669342041,5.416585922241211,3.9732561111450195,5.512474060058594,5.069494724273682,4.737087726593018,4.334149360656738,5.407010555267334,5.323962688446045,5.469764709472656,5.171461582183838,5.1242570877075195,4.425754547119141,4.2708868980407715,5.208000183105469,4.031367301940918,3.942991256713867,4.445357799530029,4.780660629272461,5.159111022949219,5.221810817718506,4.2322869300842285,4.705268383026123,4.643469333648682,3.8823442459106445,5.520011901855469,4.548102855682373,5.769382953643799,5.331233024597168,3.5058374404907227,5.277387619018555,3.462836980819702,4.300027370452881,4.685699939727783],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 4<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 4\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 4\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"box\",\"line\",\"pc\",\"cs\",\"message\",\"science\",\"newsreader\",\"tin\",\"version\",\"pretty\",\"laboratory\",\"tom\",\"large\",\"call\",\"disclaimer\",\"network\",\"year\",\"current\",\"start\",\"hard\",\"national\",\"image\",\"work\",\"made\",\"faster\",\"read\",\"opinion\",\"nasa\",\"reason\",\"simply\",\"told\",\"means\",\"day\",\"feel\",\"project\",\"view\",\"asked\",\"part\",\"hand\",\"put\",\"info\",\"higher\",\"speed\",\"jews\",\"long\",\"key\",\"white\",\"idea\",\"rate\",\"uk\",\"list\",\"order\",\"east\",\"toronto\"],\"x\":[3.059309482574463,3.8748252391815186,4.059592247009277,4.078829288482666,4.623157978057861,3.66373348236084,4.196933269500732,3.8854873180389404,1.9793727397918701,3.9378304481506348,4.774960994720459,4.131475448608398,3.371556043624878,3.847205638885498,3.7326061725616455,4.167324542999268,4.114039421081543,3.563617467880249,5.134095668792725,4.132928371429443,4.684662818908691,4.627072334289551,3.3811898231506348,2.8029532432556152,4.7035956382751465,4.656331539154053,3.8789186477661133,3.8723866939544678,4.688185691833496,4.318411350250244,4.878360271453857,3.8292617797851562,4.414409637451172,5.013249397277832,3.75118088722229,4.137286186218262,4.394577503204346,3.6671574115753174,4.10911750793457,3.5365183353424072,2.859433174133301,5.082834243774414,4.701064586639404,4.386680603027344,4.599889278411865,2.939382314682007,4.267324447631836,3.517859697341919,4.56829309463501,2.6062207221984863,4.422910690307617,4.3124847412109375,3.6706886291503906,2.398125648498535],\"y\":[6.384858131408691,6.675772666931152,6.362993240356445,6.335835933685303,6.91074275970459,7.370723724365234,6.580743312835693,6.567651748657227,6.331479072570801,6.599419593811035,7.495335102081299,7.146181106567383,7.229801654815674,7.026510238647461,6.9562225341796875,7.336671829223633,6.510297775268555,6.666705131530762,7.109259128570557,6.51686429977417,7.696358680725098,6.896101951599121,7.507297515869141,6.612052917480469,7.741210460662842,6.387305736541748,7.193972587585449,6.518276214599609,6.5181498527526855,6.762612819671631,6.528552532196045,7.371455669403076,6.317023277282715,6.721547603607178,7.217377662658691,7.365821838378906,7.053581714630127,6.489303112030029,6.434472560882568,7.230018615722656,6.849973678588867,7.38893985748291,6.558432102203369,6.703636169433594,6.603343963623047,6.269717693328857,7.120942115783691,7.516172409057617,7.577388286590576,6.059725284576416,6.758551597595215,6.368369102478027,6.866162300109863,6.171365261077881],\"z\":[5.5683465003967285,4.511605739593506,5.623201847076416,5.647707462310791,6.568357944488525,5.818484306335449,5.768187046051025,5.436999320983887,5.592590808868408,5.400505065917969,5.072473526000977,4.726012229919434,4.9377360343933105,5.504937648773193,4.28409481048584,5.501596927642822,6.176424980163574,4.454362392425537,5.259199619293213,5.321301460266113,5.122939109802246,5.673302173614502,5.898190021514893,5.5030622482299805,5.456146240234375,5.783141136169434,5.592891693115234,4.824387073516846,5.439943790435791,4.759680271148682,5.758429050445557,5.125343322753906,5.37024450302124,5.479746341705322,6.004279613494873,5.073509216308594,5.137357711791992,5.357544422149658,5.298371315002441,4.9483962059021,5.703884124755859,6.271383762359619,6.173892021179199,6.411846160888672,5.237446308135986,4.770603179931641,5.4681854248046875,6.124433994293213,5.780560493469238,4.351139068603516,6.410315036773682,5.76131534576416,4.307766914367676,4.899357318878174],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 9<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 9\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 9\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"type\",\"live\",\"agree\",\"cost\",\"pay\",\"top\",\"interest\",\"ms\",\"display\",\"advance\",\"computing\",\"machine\",\"form\",\"college\",\"questions\",\"love\",\"bible\",\"coming\",\"claim\",\"hell\",\"mine\",\"gas\",\"receive\",\"mind\",\"open\",\"home\",\"including\",\"sell\",\"light\",\"western\",\"religious\",\"sound\",\"canada\"],\"x\":[6.584036350250244,6.271790981292725,6.4678144454956055,6.07860803604126,7.119082450866699,6.8172502517700195,6.466531753540039,6.365036964416504,6.609452247619629,7.277446746826172,7.117912292480469,6.735527038574219,5.234708309173584,6.888096332550049,5.935548305511475,6.781825542449951,7.219349384307861,5.4186906814575195,6.358218669891357,7.217414379119873,7.156267166137695,6.981166839599609,5.7483954429626465,7.107309341430664,6.0599846839904785,7.469518661499023,5.908186912536621,6.838071346282959,7.057983875274658,6.659925937652588,5.0209527015686035,7.288216590881348,6.991020202636719],\"y\":[9.764911651611328,9.272320747375488,9.473556518554688,8.958786010742188,9.808988571166992,8.935063362121582,9.449967384338379,9.294622421264648,9.413691520690918,9.692060470581055,9.530244827270508,8.73934268951416,9.209762573242188,9.17375659942627,9.38158893585205,9.449121475219727,9.58415699005127,8.164912223815918,9.069476127624512,9.814998626708984,9.56863021850586,9.822443008422852,9.078947067260742,9.193185806274414,9.340502738952637,9.544221878051758,8.375202178955078,9.368480682373047,8.974822998046875,8.584799766540527,8.98861312866211,9.51204776763916,9.715860366821289],\"z\":[7.2093329429626465,7.204039573669434,7.38990592956543,6.275772571563721,7.360024929046631,7.324403762817383,7.303252696990967,6.251707077026367,6.30448055267334,7.357589244842529,7.669910907745361,6.735165596008301,6.422709941864014,6.5187788009643555,6.771814823150635,6.431524276733398,6.911290168762207,6.570947647094727,7.063248157501221,7.488556861877441,6.803463459014893,7.244344234466553,6.787723541259766,6.640914440155029,6.796475887298584,7.565677642822266,6.686182498931885,6.401283264160156,7.097048282623291,6.600832939147949,6.488207817077637,6.749307632446289,7.4514265060424805],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 5<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 5\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 5\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"stuff\",\"school\",\"internet\",\"lot\",\"dod\",\"expressed\",\"write\",\"free\",\"support\",\"friend\",\"hardware\",\"technical\",\"christian\",\"easy\",\"jim\",\"hear\",\"clear\",\"institute\",\"interesting\",\"california\",\"result\",\"original\",\"radio\",\"buy\",\"exists\",\"minutes\"],\"x\":[7.39865779876709,7.096437931060791,6.7915496826171875,7.114871978759766,7.522890567779541,7.232110500335693,6.4476447105407715,6.770295143127441,7.518642902374268,7.2313232421875,7.306118011474609,6.849895477294922,7.581527233123779,7.3677077293396,7.3239264488220215,7.115388870239258,7.312840461730957,7.514609336853027,7.23591947555542,7.451903820037842,7.363090515136719,7.3433990478515625,7.436172008514404,7.478355407714844,6.9912309646606445,7.316809177398682],\"y\":[9.34859561920166,8.377814292907715,9.524023056030273,8.08548641204834,8.896514892578125,8.740620613098145,7.634753704071045,9.566975593566895,8.911956787109375,8.82181453704834,8.612588882446289,9.381343841552734,8.796239852905273,8.56730842590332,8.56409740447998,8.434175491333008,8.737757682800293,9.365973472595215,9.443211555480957,9.223238945007324,8.860231399536133,8.511795043945312,9.275114059448242,8.665861129760742,8.910908699035645,8.967509269714355],\"z\":[6.9311957359313965,8.010783195495605,7.042294025421143,7.698902130126953,7.321297645568848,6.814781188964844,7.380794048309326,7.439810752868652,7.569454669952393,6.8989176750183105,7.830320358276367,7.500714302062988,7.61987829208374,7.333214282989502,7.731966018676758,7.9074883460998535,7.050005912780762,7.584657192230225,7.208201885223389,7.308101177215576,6.949193954467773,7.680498123168945,7.695002555847168,7.736968994140625,7.1369452476501465,7.45281457901001],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 6<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 6\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 6\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"contact\",\"mail\",\"division\",\"fax\",\"address\",\"total\",\"takes\",\"money\",\"board\",\"copy\",\"posted\",\"date\",\"couple\",\"send\",\"small\",\"figure\",\"price\",\"important\",\"due\",\"early\",\"department\",\"simple\",\"big\",\"net\",\"folks\",\"week\",\"service\",\"calls\",\"man\",\"mentioned\",\"side\",\"rest\",\"written\",\"common\",\"sort\",\"care\",\"design\",\"continue\",\"worth\",\"close\",\"short\",\"add\",\"fine\",\"response\",\"full\",\"rights\"],\"x\":[4.865654468536377,5.092130661010742,3.578228235244751,5.270092964172363,5.391572952270508,5.183903694152832,4.83291482925415,5.353076457977295,5.880101203918457,5.630695343017578,5.964000701904297,5.68253755569458,4.786611080169678,4.7954206466674805,5.285404682159424,4.959959506988525,5.74044942855835,4.828857898712158,5.936924457550049,5.519993305206299,4.86536979675293,5.296241760253906,5.738258361816406,4.618223667144775,5.115872859954834,4.818821907043457,5.426179885864258,5.124363899230957,5.3906965255737305,5.2837233543396,5.463447093963623,5.071426868438721,5.13401460647583,5.417201995849609,5.556520938873291,5.05985164642334,5.3702216148376465,6.2748308181762695,5.063559055328369,5.196791648864746,5.601971626281738,5.004653453826904,5.850577354431152,5.2624592781066895,5.394632339477539,5.366284370422363],\"y\":[8.398530960083008,8.114253997802734,6.476717948913574,8.886421203613281,8.193414688110352,7.7011308670043945,7.57855749130249,6.879995822906494,7.596097469329834,8.417923927307129,7.262027740478516,7.882683753967285,7.827428817749023,8.785804748535156,8.089487075805664,7.888913154602051,6.908761024475098,8.32178783416748,7.471491813659668,6.997182846069336,8.853492736816406,8.31183910369873,9.391718864440918,7.147068977355957,8.123072624206543,8.746603965759277,9.308319091796875,8.097424507141113,6.933008193969727,7.767704486846924,8.610310554504395,7.884537220001221,8.151973724365234,7.671382904052734,9.25370979309082,7.710748672485352,9.313820838928223,7.484425067901611,8.397815704345703,6.850370407104492,9.13891887664795,7.139015197753906,7.728819847106934,7.196341514587402,7.906317710876465,7.370712757110596],\"z\":[6.744991779327393,7.107006549835205,4.649055004119873,6.715826034545898,6.117014408111572,7.072668075561523,6.337320327758789,5.48402214050293,6.221331596374512,6.398090362548828,6.841999053955078,7.189254283905029,5.861244201660156,5.629530906677246,6.6435933113098145,7.003536701202393,5.988739013671875,6.614300727844238,6.51015567779541,5.840047359466553,6.34894323348999,5.7214765548706055,6.413042068481445,5.638591766357422,6.512633800506592,5.720296859741211,6.209018707275391,7.0349955558776855,5.882786750793457,7.161459922790527,6.034933567047119,6.52836799621582,6.597048759460449,6.890864372253418,6.1099019050598145,6.748958587646484,6.229607105255127,7.215133190155029,7.03229284286499,5.779147148132324,6.093502521514893,6.406881332397461,6.494271755218506,6.067232131958008,6.218625545501709,6.6815667152404785],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 8<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 8\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 8\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"windows\",\"reply\",\"guy\",\"systems\",\"robert\",\"low\",\"sense\",\"guess\",\"software\",\"program\",\"similar\",\"bad\",\"effect\",\"dos\",\"problems\",\"results\",\"general\",\"truth\",\"ago\",\"washington\",\"understand\",\"dept\",\"months\",\"research\",\"david\",\"wondering\",\"end\",\"kind\",\"appreciated\",\"days\",\"deleted\",\"single\",\"james\"],\"x\":[3.275785207748413,3.8152923583984375,3.9010932445526123,3.7391717433929443,4.096240043640137,3.281900644302368,4.0067925453186035,4.147512435913086,4.131801605224609,3.0760624408721924,4.5306549072265625,2.784703254699707,3.8641369342803955,4.754770755767822,3.854490280151367,4.536334991455078,3.987046003341675,4.671327590942383,3.3742003440856934,4.0049967765808105,3.9673876762390137,5.616102695465088,4.541426658630371,3.623460054397583,3.489835023880005,4.454657077789307,3.3579227924346924,4.185500621795654,4.080170631408691,3.161602020263672,4.798636436462402,4.728578567504883,4.018978118896484],\"y\":[8.249404907226562,8.482848167419434,8.42249870300293,7.68936824798584,7.888420104980469,8.253718376159668,7.753519535064697,8.30537223815918,8.314330101013184,7.723708629608154,8.399646759033203,7.457014083862305,8.497648239135742,8.7311429977417,8.128273010253906,8.04227352142334,8.2734375,8.24377727508545,7.623411178588867,8.14787483215332,7.821319580078125,9.346824645996094,8.503426551818848,8.31201171875,8.225135803222656,8.569164276123047,8.137894630432129,8.31764030456543,8.276450157165527,7.747559070587158,8.152480125427246,8.342772483825684,7.474211692810059],\"z\":[4.89178466796875,5.113792896270752,5.0397796630859375,4.937509059906006,5.707744121551514,4.852701187133789,6.3316168785095215,5.091579437255859,4.996726036071777,5.092953681945801,5.2467169761657715,5.229584693908691,5.443818092346191,5.449487686157227,5.51284646987915,5.53186559677124,5.697729587554932,5.240681171417236,5.954739093780518,6.0852437019348145,5.430886268615723,6.539029121398926,6.029555797576904,5.115754127502441,5.000783920288086,5.934286117553711,4.7655487060546875,5.041598320007324,5.360463619232178,5.127456188201904,5.2866387367248535,5.476469993591309,5.430293560028076],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 0<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 0\",\"marker\":{\"color\":\"#FF97FF\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 0\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"university\",\"article\",\"writes\",\"good\",\"time\",\"years\",\"file\",\"system\",\"fact\",\"people\",\"make\",\"post\",\"power\",\"memory\",\"point\"],\"x\":[1.4098228216171265,1.4423584938049316,1.3536783456802368,1.3065762519836426,1.7180052995681763,1.3130966424942017,1.4934096336364746,1.4855892658233643,1.3819597959518433,1.401727557182312,2.0288143157958984,1.5010453462600708,1.4899163246154785,1.5467199087142944,1.8765933513641357],\"y\":[6.902474880218506,6.396080493927002,6.549528121948242,6.452355861663818,6.39505672454834,6.52383279800415,6.5745978355407715,6.469955921173096,6.420597553253174,6.490143299102783,6.526170253753662,6.622561454772949,6.510274887084961,6.53872537612915,6.588525772094727],\"z\":[3.8323299884796143,3.0953919887542725,3.2665252685546875,3.0865368843078613,3.2537384033203125,3.484611749649048,3.0808825492858887,3.1610934734344482,3.1302661895751953,3.2054502964019775,3.2595374584198,3.8333232402801514,3.752811908721924,3.3135287761688232,3.2228164672851562],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 3<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 3\",\"marker\":{\"color\":\"#FECB52\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 3\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"required\",\"company\",\"job\",\"bill\",\"city\",\"interested\",\"final\",\"mac\",\"show\",\"class\",\"makes\",\"texas\",\"set\",\"application\",\"hope\",\"note\",\"engineering\",\"miles\",\"references\",\"mike\",\"experience\",\"related\",\"wanted\",\"lost\"],\"x\":[6.448614120483398,6.576678276062012,6.105144023895264,6.582845687866211,5.993597030639648,6.2969441413879395,5.828474521636963,5.863863945007324,6.474443435668945,6.752425193786621,7.215498447418213,6.541752815246582,6.921909332275391,6.802846431732178,7.04067850112915,6.446582317352295,5.936138153076172,5.864892482757568,6.720188617706299,5.881692886352539,7.00513219833374,6.8470940589904785,6.696568012237549,7.133274078369141],\"y\":[8.273782730102539,9.172362327575684,8.205779075622559,8.235982894897461,8.155061721801758,7.9715704917907715,8.053461074829102,8.056325912475586,9.046088218688965,8.75317668914795,9.420721054077148,9.025654792785645,8.666898727416992,8.45612907409668,8.38941764831543,8.147494316101074,8.113588333129883,7.550858497619629,8.374282836914062,7.860899925231934,9.140151023864746,9.03686237335205,8.286970138549805,8.939284324645996],\"z\":[6.81367301940918,7.659848690032959,7.60036039352417,8.058019638061523,7.24276876449585,7.700978755950928,7.386314392089844,6.370791435241699,7.293168544769287,7.792135238647461,7.773465156555176,7.236067295074463,7.954050540924072,8.034055709838867,7.917751789093018,7.819118022918701,7.446199417114258,7.37579870223999,7.9882588386535645,6.996420383453369,7.639013767242432,7.732618808746338,8.005488395690918,7.74714469909668],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"x\"}},\"yaxis\":{\"title\":{\"text\":\"y\"}},\"zaxis\":{\"title\":{\"text\":\"z\"}}},\"legend\":{\"title\":{\"text\":\"cluster\"},\"tracegroupgap\":0},\"title\":{\"text\":\"word-embedding-samples\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('55bcada3-8e00-45c9-ac5c-1f0908fdc79d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# visualization\n",
        "fig = px.scatter_3d(wb, \n",
        "                    text = wb['word'],\n",
        "                    x='x', y='y', z='z',\n",
        "                    color = wb['cluster'],\n",
        "                    title =\"word-embedding-samples\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9jSI12r9zqu"
      },
      "source": [
        "# **ETM-Model trainieren**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwiU9zWCIUh_",
        "outputId": "ae67cf82-ba4f-4a87-d200-e6d008f7891e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "150\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0.])\n"
          ]
        }
      ],
      "source": [
        "# using DocSet to use easier the modul DataSet from torch\n",
        "from src.train_etm import DocSet, TrainETM\n",
        "from src.etm import ETM\n",
        "\n",
        "vocab_size = len(list(word2id.keys()))\n",
        "tr_set = DocSet(\"train\", vocab_size, train_set)\n",
        "print(len(tr_set))\n",
        "print(tr_set.__getitem__(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kbJIWLZpJRnG",
        "outputId": "c254c730-ea6a-405e-cbed-db3399deb121"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000\n",
            "adam\n",
            "number of batches: 25\n",
            "Epoch: 0/1000  -  Loss: 177.95843505859375\n",
            "Epoch: 1/1000  -  Loss: 177.9403839111328\n",
            "Epoch: 2/1000  -  Loss: 177.93252563476562\n",
            "Epoch: 3/1000  -  Loss: 177.92466735839844\n",
            "Epoch: 4/1000  -  Loss: 177.9407196044922\n",
            "Epoch: 5/1000  -  Loss: 177.91397094726562\n",
            "Epoch: 6/1000  -  Loss: 177.91070556640625\n",
            "Epoch: 7/1000  -  Loss: 177.90721130371094\n",
            "Epoch: 8/1000  -  Loss: 177.90585327148438\n",
            "Epoch: 9/1000  -  Loss: 177.90821838378906\n",
            "Epoch: 10/1000  -  Loss: 177.9027099609375\n",
            "Epoch: 11/1000  -  Loss: 177.90142822265625\n",
            "Epoch: 12/1000  -  Loss: 177.9008026123047\n",
            "Epoch: 13/1000  -  Loss: 177.90577697753906\n",
            "Epoch: 14/1000  -  Loss: 177.9033660888672\n",
            "Epoch: 15/1000  -  Loss: 177.8999481201172\n",
            "Epoch: 16/1000  -  Loss: 177.89614868164062\n",
            "Epoch: 17/1000  -  Loss: 177.90455627441406\n",
            "Epoch: 18/1000  -  Loss: 177.89430236816406\n",
            "Epoch: 19/1000  -  Loss: 177.8922119140625\n",
            "Epoch: 20/1000  -  Loss: 177.89447021484375\n",
            "Epoch: 21/1000  -  Loss: 177.88951110839844\n",
            "Epoch: 22/1000  -  Loss: 177.88868713378906\n",
            "Epoch: 23/1000  -  Loss: 177.89244079589844\n",
            "Epoch: 24/1000  -  Loss: 177.88397216796875\n",
            "Epoch: 25/1000  -  Loss: 177.88526916503906\n",
            "Epoch: 26/1000  -  Loss: 177.89297485351562\n",
            "Epoch: 27/1000  -  Loss: 177.88494873046875\n",
            "Epoch: 28/1000  -  Loss: 177.8897247314453\n",
            "Epoch: 29/1000  -  Loss: 177.88623046875\n",
            "Epoch: 30/1000  -  Loss: 177.8878173828125\n",
            "Epoch: 31/1000  -  Loss: 177.88726806640625\n",
            "Epoch: 32/1000  -  Loss: 177.8839111328125\n",
            "Epoch: 33/1000  -  Loss: 177.88882446289062\n",
            "Epoch: 34/1000  -  Loss: 177.8847198486328\n",
            "Epoch: 35/1000  -  Loss: 177.8833465576172\n",
            "Epoch: 36/1000  -  Loss: 177.88755798339844\n",
            "Epoch: 37/1000  -  Loss: 177.8863067626953\n",
            "Epoch: 38/1000  -  Loss: 177.88441467285156\n",
            "Epoch: 39/1000  -  Loss: 177.88575744628906\n",
            "Epoch: 40/1000  -  Loss: 177.88510131835938\n",
            "Epoch: 41/1000  -  Loss: 177.8840789794922\n",
            "Epoch: 42/1000  -  Loss: 177.88421630859375\n",
            "Epoch: 43/1000  -  Loss: 177.88510131835938\n",
            "Epoch: 44/1000  -  Loss: 177.8849639892578\n",
            "Epoch: 45/1000  -  Loss: 177.88156127929688\n",
            "Epoch: 46/1000  -  Loss: 177.88510131835938\n",
            "Epoch: 47/1000  -  Loss: 177.8837432861328\n",
            "Epoch: 48/1000  -  Loss: 177.88394165039062\n",
            "Epoch: 49/1000  -  Loss: 177.88307189941406\n",
            "Epoch: 50/1000  -  Loss: 177.88204956054688\n",
            "Epoch: 51/1000  -  Loss: 177.88282775878906\n",
            "Epoch: 52/1000  -  Loss: 177.88157653808594\n",
            "Epoch: 53/1000  -  Loss: 177.8831787109375\n",
            "Epoch: 54/1000  -  Loss: 177.88182067871094\n",
            "Epoch: 55/1000  -  Loss: 177.88232421875\n",
            "Epoch: 56/1000  -  Loss: 177.8837432861328\n",
            "Epoch: 57/1000  -  Loss: 177.88302612304688\n",
            "Epoch: 58/1000  -  Loss: 177.88197326660156\n",
            "Epoch: 59/1000  -  Loss: 177.88377380371094\n",
            "Epoch: 60/1000  -  Loss: 177.8822479248047\n",
            "Epoch: 61/1000  -  Loss: 177.88279724121094\n",
            "Epoch: 62/1000  -  Loss: 177.88250732421875\n",
            "Epoch: 63/1000  -  Loss: 177.88272094726562\n",
            "Epoch: 64/1000  -  Loss: 177.8824462890625\n",
            "Epoch: 65/1000  -  Loss: 177.88241577148438\n",
            "Epoch: 66/1000  -  Loss: 177.88246154785156\n",
            "Epoch: 67/1000  -  Loss: 177.88233947753906\n",
            "Epoch: 68/1000  -  Loss: 177.8830108642578\n",
            "Epoch: 69/1000  -  Loss: 177.8819122314453\n",
            "Epoch: 70/1000  -  Loss: 177.88211059570312\n",
            "Epoch: 71/1000  -  Loss: 177.8814697265625\n",
            "Epoch: 72/1000  -  Loss: 177.8826141357422\n",
            "Epoch: 73/1000  -  Loss: 177.88136291503906\n",
            "Epoch: 74/1000  -  Loss: 177.88185119628906\n",
            "Epoch: 75/1000  -  Loss: 177.8834228515625\n",
            "Epoch: 76/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 77/1000  -  Loss: 177.8831024169922\n",
            "Epoch: 78/1000  -  Loss: 177.88230895996094\n",
            "Epoch: 79/1000  -  Loss: 177.88265991210938\n",
            "Epoch: 80/1000  -  Loss: 177.88209533691406\n",
            "Epoch: 81/1000  -  Loss: 177.88153076171875\n",
            "Epoch: 82/1000  -  Loss: 177.88150024414062\n",
            "Epoch: 83/1000  -  Loss: 177.8821258544922\n",
            "Epoch: 84/1000  -  Loss: 177.88307189941406\n",
            "Epoch: 85/1000  -  Loss: 177.88265991210938\n",
            "Epoch: 86/1000  -  Loss: 177.88185119628906\n",
            "Epoch: 87/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 88/1000  -  Loss: 177.8824005126953\n",
            "Epoch: 89/1000  -  Loss: 177.88185119628906\n",
            "Epoch: 90/1000  -  Loss: 177.8823699951172\n",
            "Epoch: 91/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 92/1000  -  Loss: 177.88128662109375\n",
            "Epoch: 93/1000  -  Loss: 177.88197326660156\n",
            "Epoch: 94/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 95/1000  -  Loss: 177.8822021484375\n",
            "Epoch: 96/1000  -  Loss: 177.88134765625\n",
            "Epoch: 97/1000  -  Loss: 177.8830108642578\n",
            "Epoch: 98/1000  -  Loss: 177.88250732421875\n",
            "Epoch: 99/1000  -  Loss: 177.88255310058594\n",
            "Epoch: 100/1000  -  Loss: 177.8824462890625\n",
            "Epoch: 101/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 102/1000  -  Loss: 177.8814697265625\n",
            "Epoch: 103/1000  -  Loss: 177.88230895996094\n",
            "Epoch: 104/1000  -  Loss: 177.88153076171875\n",
            "Epoch: 105/1000  -  Loss: 177.88296508789062\n",
            "Epoch: 106/1000  -  Loss: 177.88233947753906\n",
            "Epoch: 107/1000  -  Loss: 177.88255310058594\n",
            "Epoch: 108/1000  -  Loss: 177.88204956054688\n",
            "Epoch: 109/1000  -  Loss: 177.88217163085938\n",
            "Epoch: 110/1000  -  Loss: 177.88226318359375\n",
            "Epoch: 111/1000  -  Loss: 177.8819122314453\n",
            "Epoch: 112/1000  -  Loss: 177.88218688964844\n",
            "Epoch: 113/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 114/1000  -  Loss: 177.88209533691406\n",
            "Epoch: 115/1000  -  Loss: 177.8822479248047\n",
            "Epoch: 116/1000  -  Loss: 177.8811492919922\n",
            "Epoch: 117/1000  -  Loss: 177.88226318359375\n",
            "Epoch: 118/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 119/1000  -  Loss: 177.8816375732422\n",
            "Epoch: 120/1000  -  Loss: 177.88160705566406\n",
            "Epoch: 121/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 122/1000  -  Loss: 177.88233947753906\n",
            "Epoch: 123/1000  -  Loss: 177.8813934326172\n",
            "Epoch: 124/1000  -  Loss: 177.88211059570312\n",
            "Epoch: 125/1000  -  Loss: 177.88233947753906\n",
            "Epoch: 126/1000  -  Loss: 177.8818817138672\n",
            "Epoch: 127/1000  -  Loss: 177.88185119628906\n",
            "Epoch: 128/1000  -  Loss: 177.88174438476562\n",
            "Epoch: 129/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 130/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 131/1000  -  Loss: 177.88108825683594\n",
            "Epoch: 132/1000  -  Loss: 177.8817901611328\n",
            "Epoch: 133/1000  -  Loss: 177.88198852539062\n",
            "Epoch: 134/1000  -  Loss: 177.88150024414062\n",
            "Epoch: 135/1000  -  Loss: 177.88189697265625\n",
            "Epoch: 136/1000  -  Loss: 177.8815460205078\n",
            "Epoch: 137/1000  -  Loss: 177.8818817138672\n",
            "Epoch: 138/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 139/1000  -  Loss: 177.88275146484375\n",
            "Epoch: 140/1000  -  Loss: 177.88214111328125\n",
            "Epoch: 141/1000  -  Loss: 177.8825225830078\n",
            "Epoch: 142/1000  -  Loss: 177.88253784179688\n",
            "Epoch: 143/1000  -  Loss: 177.88218688964844\n",
            "Epoch: 144/1000  -  Loss: 177.88165283203125\n",
            "Epoch: 145/1000  -  Loss: 177.88217163085938\n",
            "Epoch: 146/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 147/1000  -  Loss: 177.8812255859375\n",
            "Epoch: 148/1000  -  Loss: 177.8817901611328\n",
            "Epoch: 149/1000  -  Loss: 177.8824462890625\n",
            "Epoch: 150/1000  -  Loss: 177.8814239501953\n",
            "Epoch: 151/1000  -  Loss: 177.88150024414062\n",
            "Epoch: 152/1000  -  Loss: 177.88182067871094\n",
            "Epoch: 153/1000  -  Loss: 177.8821258544922\n",
            "Epoch: 154/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 155/1000  -  Loss: 177.8803253173828\n",
            "Epoch: 156/1000  -  Loss: 177.8824005126953\n",
            "Epoch: 157/1000  -  Loss: 177.8817138671875\n",
            "Epoch: 158/1000  -  Loss: 177.88258361816406\n",
            "Epoch: 159/1000  -  Loss: 177.88198852539062\n",
            "Epoch: 160/1000  -  Loss: 177.88125610351562\n",
            "Epoch: 161/1000  -  Loss: 177.88192749023438\n",
            "Epoch: 162/1000  -  Loss: 177.88253784179688\n",
            "Epoch: 163/1000  -  Loss: 177.880859375\n",
            "Epoch: 164/1000  -  Loss: 177.88136291503906\n",
            "Epoch: 165/1000  -  Loss: 177.8824462890625\n",
            "Epoch: 166/1000  -  Loss: 177.8824462890625\n",
            "Epoch: 167/1000  -  Loss: 177.88255310058594\n",
            "Epoch: 168/1000  -  Loss: 177.88198852539062\n",
            "Epoch: 169/1000  -  Loss: 177.88052368164062\n",
            "Epoch: 170/1000  -  Loss: 177.88162231445312\n",
            "Epoch: 171/1000  -  Loss: 177.8832550048828\n",
            "Epoch: 172/1000  -  Loss: 177.88160705566406\n",
            "Epoch: 173/1000  -  Loss: 177.8810577392578\n",
            "Epoch: 174/1000  -  Loss: 177.88104248046875\n",
            "Epoch: 175/1000  -  Loss: 177.88150024414062\n",
            "Epoch: 176/1000  -  Loss: 177.8822479248047\n",
            "Epoch: 177/1000  -  Loss: 177.88177490234375\n",
            "Epoch: 178/1000  -  Loss: 177.88157653808594\n",
            "Epoch: 179/1000  -  Loss: 177.88211059570312\n",
            "Epoch: 180/1000  -  Loss: 177.88258361816406\n",
            "Epoch: 181/1000  -  Loss: 177.8824005126953\n",
            "Epoch: 182/1000  -  Loss: 177.88160705566406\n",
            "Epoch: 183/1000  -  Loss: 177.88214111328125\n",
            "Epoch: 184/1000  -  Loss: 177.88214111328125\n",
            "Epoch: 185/1000  -  Loss: 177.88272094726562\n",
            "Epoch: 186/1000  -  Loss: 177.88259887695312\n",
            "Epoch: 187/1000  -  Loss: 177.8816375732422\n",
            "Epoch: 188/1000  -  Loss: 177.8823699951172\n",
            "Epoch: 189/1000  -  Loss: 177.88230895996094\n",
            "Epoch: 190/1000  -  Loss: 177.88189697265625\n",
            "Epoch: 191/1000  -  Loss: 177.88111877441406\n",
            "Epoch: 192/1000  -  Loss: 177.88189697265625\n",
            "Epoch: 193/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 194/1000  -  Loss: 177.88211059570312\n",
            "Epoch: 195/1000  -  Loss: 177.88182067871094\n",
            "Epoch: 196/1000  -  Loss: 177.88253784179688\n",
            "Epoch: 197/1000  -  Loss: 177.8821258544922\n",
            "Epoch: 198/1000  -  Loss: 177.8818817138672\n",
            "Epoch: 199/1000  -  Loss: 177.88128662109375\n",
            "Epoch: 200/1000  -  Loss: 177.8827362060547\n",
            "Epoch: 201/1000  -  Loss: 177.88267517089844\n",
            "Epoch: 202/1000  -  Loss: 177.8822784423828\n",
            "Epoch: 203/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 204/1000  -  Loss: 177.88253784179688\n",
            "Epoch: 205/1000  -  Loss: 177.8817901611328\n",
            "Epoch: 206/1000  -  Loss: 177.88169860839844\n",
            "Epoch: 207/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 208/1000  -  Loss: 177.88214111328125\n",
            "Epoch: 209/1000  -  Loss: 177.88104248046875\n",
            "Epoch: 210/1000  -  Loss: 177.8818817138672\n",
            "Epoch: 211/1000  -  Loss: 177.88157653808594\n",
            "Epoch: 212/1000  -  Loss: 177.88247680664062\n",
            "Epoch: 213/1000  -  Loss: 177.88238525390625\n",
            "Epoch: 214/1000  -  Loss: 177.8817138671875\n",
            "Epoch: 215/1000  -  Loss: 177.88140869140625\n",
            "Epoch: 216/1000  -  Loss: 177.88148498535156\n",
            "Epoch: 217/1000  -  Loss: 177.88128662109375\n",
            "Epoch: 218/1000  -  Loss: 177.88189697265625\n",
            "Epoch: 219/1000  -  Loss: 177.8817138671875\n",
            "Epoch: 220/1000  -  Loss: 177.88230895996094\n",
            "Epoch: 221/1000  -  Loss: 177.88165283203125\n",
            "Epoch: 222/1000  -  Loss: 177.8817901611328\n",
            "Epoch: 223/1000  -  Loss: 177.8821258544922\n",
            "Epoch: 224/1000  -  Loss: 177.8814697265625\n",
            "Epoch: 225/1000  -  Loss: 177.8828125\n",
            "Epoch: 226/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 227/1000  -  Loss: 177.88182067871094\n",
            "Epoch: 228/1000  -  Loss: 177.88185119628906\n",
            "Epoch: 229/1000  -  Loss: 177.88185119628906\n",
            "Epoch: 230/1000  -  Loss: 177.88226318359375\n",
            "Epoch: 231/1000  -  Loss: 177.88247680664062\n",
            "Epoch: 232/1000  -  Loss: 177.8818817138672\n",
            "Epoch: 233/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 234/1000  -  Loss: 177.88125610351562\n",
            "Epoch: 235/1000  -  Loss: 177.88197326660156\n",
            "Epoch: 236/1000  -  Loss: 177.8822479248047\n",
            "Epoch: 237/1000  -  Loss: 177.8816375732422\n",
            "Epoch: 238/1000  -  Loss: 177.8816375732422\n",
            "Epoch: 239/1000  -  Loss: 177.8809356689453\n",
            "Epoch: 240/1000  -  Loss: 177.8817901611328\n",
            "Epoch: 241/1000  -  Loss: 177.88099670410156\n",
            "Epoch: 242/1000  -  Loss: 177.88233947753906\n",
            "Epoch: 243/1000  -  Loss: 177.88157653808594\n",
            "Epoch: 244/1000  -  Loss: 177.8822479248047\n",
            "Epoch: 245/1000  -  Loss: 177.88258361816406\n",
            "Epoch: 246/1000  -  Loss: 177.88169860839844\n",
            "Epoch: 247/1000  -  Loss: 177.88116455078125\n",
            "Epoch: 248/1000  -  Loss: 177.88134765625\n",
            "Epoch: 249/1000  -  Loss: 177.88241577148438\n",
            "Epoch: 250/1000  -  Loss: 177.88198852539062\n",
            "Epoch: 251/1000  -  Loss: 177.88304138183594\n",
            "Epoch: 252/1000  -  Loss: 177.8814697265625\n",
            "Epoch: 253/1000  -  Loss: 177.8816375732422\n",
            "Epoch: 254/1000  -  Loss: 177.88165283203125\n",
            "Epoch: 255/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 256/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 257/1000  -  Loss: 177.88079833984375\n",
            "Epoch: 258/1000  -  Loss: 177.88230895996094\n",
            "Epoch: 259/1000  -  Loss: 177.88218688964844\n",
            "Epoch: 260/1000  -  Loss: 177.88169860839844\n",
            "Epoch: 261/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 262/1000  -  Loss: 177.8816375732422\n",
            "Epoch: 263/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 264/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 265/1000  -  Loss: 177.88165283203125\n",
            "Epoch: 266/1000  -  Loss: 177.8817596435547\n",
            "Epoch: 267/1000  -  Loss: 177.88259887695312\n",
            "Epoch: 268/1000  -  Loss: 177.88272094726562\n",
            "Epoch: 269/1000  -  Loss: 177.8826141357422\n",
            "Epoch: 270/1000  -  Loss: 177.8823699951172\n",
            "Epoch: 271/1000  -  Loss: 177.88279724121094\n",
            "Epoch: 272/1000  -  Loss: 177.88165283203125\n",
            "Epoch: 273/1000  -  Loss: 177.8817901611328\n",
            "Epoch: 274/1000  -  Loss: 177.8822784423828\n",
            "Epoch: 275/1000  -  Loss: 177.88279724121094\n",
            "Epoch: 276/1000  -  Loss: 177.88238525390625\n",
            "Epoch: 277/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 278/1000  -  Loss: 177.88238525390625\n",
            "Epoch: 279/1000  -  Loss: 177.88197326660156\n",
            "Epoch: 280/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 281/1000  -  Loss: 177.8818359375\n",
            "Epoch: 282/1000  -  Loss: 177.88162231445312\n",
            "Epoch: 283/1000  -  Loss: 177.88157653808594\n",
            "Epoch: 284/1000  -  Loss: 177.88162231445312\n",
            "Epoch: 285/1000  -  Loss: 177.8817901611328\n",
            "Epoch: 286/1000  -  Loss: 177.88174438476562\n",
            "Epoch: 287/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 288/1000  -  Loss: 177.8825225830078\n",
            "Epoch: 289/1000  -  Loss: 177.8828887939453\n",
            "Epoch: 290/1000  -  Loss: 177.88258361816406\n",
            "Epoch: 291/1000  -  Loss: 177.88185119628906\n",
            "Epoch: 292/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 293/1000  -  Loss: 177.88125610351562\n",
            "Epoch: 294/1000  -  Loss: 177.88174438476562\n",
            "Epoch: 295/1000  -  Loss: 177.8825225830078\n",
            "Epoch: 296/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 297/1000  -  Loss: 177.88197326660156\n",
            "Epoch: 298/1000  -  Loss: 177.88063049316406\n",
            "Epoch: 299/1000  -  Loss: 177.88211059570312\n",
            "Epoch: 300/1000  -  Loss: 177.8811492919922\n",
            "Epoch: 301/1000  -  Loss: 177.88211059570312\n",
            "Epoch: 302/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 303/1000  -  Loss: 177.8827667236328\n",
            "Epoch: 304/1000  -  Loss: 177.8821258544922\n",
            "Epoch: 305/1000  -  Loss: 177.8821258544922\n",
            "Epoch: 306/1000  -  Loss: 177.88307189941406\n",
            "Epoch: 307/1000  -  Loss: 177.88262939453125\n",
            "Epoch: 308/1000  -  Loss: 177.88232421875\n",
            "Epoch: 309/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 310/1000  -  Loss: 177.88153076171875\n",
            "Epoch: 311/1000  -  Loss: 177.8815460205078\n",
            "Epoch: 312/1000  -  Loss: 177.88125610351562\n",
            "Epoch: 313/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 314/1000  -  Loss: 177.8817596435547\n",
            "Epoch: 315/1000  -  Loss: 177.88218688964844\n",
            "Epoch: 316/1000  -  Loss: 177.8818359375\n",
            "Epoch: 317/1000  -  Loss: 177.88162231445312\n",
            "Epoch: 318/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 319/1000  -  Loss: 177.8818359375\n",
            "Epoch: 320/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 321/1000  -  Loss: 177.88177490234375\n",
            "Epoch: 322/1000  -  Loss: 177.88247680664062\n",
            "Epoch: 323/1000  -  Loss: 177.8828125\n",
            "Epoch: 324/1000  -  Loss: 177.8818817138672\n",
            "Epoch: 325/1000  -  Loss: 177.8819122314453\n",
            "Epoch: 326/1000  -  Loss: 177.88192749023438\n",
            "Epoch: 327/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 328/1000  -  Loss: 177.88204956054688\n",
            "Epoch: 329/1000  -  Loss: 177.88217163085938\n",
            "Epoch: 330/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 331/1000  -  Loss: 177.88125610351562\n",
            "Epoch: 332/1000  -  Loss: 177.88255310058594\n",
            "Epoch: 333/1000  -  Loss: 177.88217163085938\n",
            "Epoch: 334/1000  -  Loss: 177.88217163085938\n",
            "Epoch: 335/1000  -  Loss: 177.88198852539062\n",
            "Epoch: 336/1000  -  Loss: 177.8827362060547\n",
            "Epoch: 337/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 338/1000  -  Loss: 177.88169860839844\n",
            "Epoch: 339/1000  -  Loss: 177.88185119628906\n",
            "Epoch: 340/1000  -  Loss: 177.88192749023438\n",
            "Epoch: 341/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 342/1000  -  Loss: 177.8817138671875\n",
            "Epoch: 343/1000  -  Loss: 177.8822021484375\n",
            "Epoch: 344/1000  -  Loss: 177.88211059570312\n",
            "Epoch: 345/1000  -  Loss: 177.88197326660156\n",
            "Epoch: 346/1000  -  Loss: 177.88150024414062\n",
            "Epoch: 347/1000  -  Loss: 177.88217163085938\n",
            "Epoch: 348/1000  -  Loss: 177.8817138671875\n",
            "Epoch: 349/1000  -  Loss: 177.8817596435547\n",
            "Epoch: 350/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 351/1000  -  Loss: 177.8824005126953\n",
            "Epoch: 352/1000  -  Loss: 177.88253784179688\n",
            "Epoch: 353/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 354/1000  -  Loss: 177.8828887939453\n",
            "Epoch: 355/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 356/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 357/1000  -  Loss: 177.8818359375\n",
            "Epoch: 358/1000  -  Loss: 177.88267517089844\n",
            "Epoch: 359/1000  -  Loss: 177.8821258544922\n",
            "Epoch: 360/1000  -  Loss: 177.88165283203125\n",
            "Epoch: 361/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 362/1000  -  Loss: 177.88177490234375\n",
            "Epoch: 363/1000  -  Loss: 177.88165283203125\n",
            "Epoch: 364/1000  -  Loss: 177.88214111328125\n",
            "Epoch: 365/1000  -  Loss: 177.88214111328125\n",
            "Epoch: 366/1000  -  Loss: 177.8814697265625\n",
            "Epoch: 367/1000  -  Loss: 177.88156127929688\n",
            "Epoch: 368/1000  -  Loss: 177.88230895996094\n",
            "Epoch: 369/1000  -  Loss: 177.88182067871094\n",
            "Epoch: 370/1000  -  Loss: 177.88162231445312\n",
            "Epoch: 371/1000  -  Loss: 177.88197326660156\n",
            "Epoch: 372/1000  -  Loss: 177.88226318359375\n",
            "Epoch: 373/1000  -  Loss: 177.88214111328125\n",
            "Epoch: 374/1000  -  Loss: 177.8817138671875\n",
            "Epoch: 375/1000  -  Loss: 177.8822479248047\n",
            "Epoch: 376/1000  -  Loss: 177.8819122314453\n",
            "Epoch: 377/1000  -  Loss: 177.88241577148438\n",
            "Epoch: 378/1000  -  Loss: 177.88331604003906\n",
            "Epoch: 379/1000  -  Loss: 177.88209533691406\n",
            "Epoch: 380/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 381/1000  -  Loss: 177.88162231445312\n",
            "Epoch: 382/1000  -  Loss: 177.88125610351562\n",
            "Epoch: 383/1000  -  Loss: 177.88189697265625\n",
            "Epoch: 384/1000  -  Loss: 177.88116455078125\n",
            "Epoch: 385/1000  -  Loss: 177.8827667236328\n",
            "Epoch: 386/1000  -  Loss: 177.88101196289062\n",
            "Epoch: 387/1000  -  Loss: 177.8815460205078\n",
            "Epoch: 388/1000  -  Loss: 177.8822021484375\n",
            "Epoch: 389/1000  -  Loss: 177.88217163085938\n",
            "Epoch: 390/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 391/1000  -  Loss: 177.88232421875\n",
            "Epoch: 392/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 393/1000  -  Loss: 177.88265991210938\n",
            "Epoch: 394/1000  -  Loss: 177.88177490234375\n",
            "Epoch: 395/1000  -  Loss: 177.88287353515625\n",
            "Epoch: 396/1000  -  Loss: 177.88189697265625\n",
            "Epoch: 397/1000  -  Loss: 177.88192749023438\n",
            "Epoch: 398/1000  -  Loss: 177.88153076171875\n",
            "Epoch: 399/1000  -  Loss: 177.8816375732422\n",
            "Epoch: 400/1000  -  Loss: 177.88169860839844\n",
            "Epoch: 401/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 402/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 403/1000  -  Loss: 177.8828125\n",
            "Epoch: 404/1000  -  Loss: 177.88253784179688\n",
            "Epoch: 405/1000  -  Loss: 177.8819122314453\n",
            "Epoch: 406/1000  -  Loss: 177.88279724121094\n",
            "Epoch: 407/1000  -  Loss: 177.88197326660156\n",
            "Epoch: 408/1000  -  Loss: 177.88076782226562\n",
            "Epoch: 409/1000  -  Loss: 177.88174438476562\n",
            "Epoch: 410/1000  -  Loss: 177.88217163085938\n",
            "Epoch: 411/1000  -  Loss: 177.8817138671875\n",
            "Epoch: 412/1000  -  Loss: 177.88296508789062\n",
            "Epoch: 413/1000  -  Loss: 177.88148498535156\n",
            "Epoch: 414/1000  -  Loss: 177.88116455078125\n",
            "Epoch: 415/1000  -  Loss: 177.8817138671875\n",
            "Epoch: 416/1000  -  Loss: 177.8822479248047\n",
            "Epoch: 417/1000  -  Loss: 177.88125610351562\n",
            "Epoch: 418/1000  -  Loss: 177.88157653808594\n",
            "Epoch: 419/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 420/1000  -  Loss: 177.88262939453125\n",
            "Epoch: 421/1000  -  Loss: 177.88182067871094\n",
            "Epoch: 422/1000  -  Loss: 177.88238525390625\n",
            "Epoch: 423/1000  -  Loss: 177.88214111328125\n",
            "Epoch: 424/1000  -  Loss: 177.8818817138672\n",
            "Epoch: 425/1000  -  Loss: 177.88250732421875\n",
            "Epoch: 426/1000  -  Loss: 177.88238525390625\n",
            "Epoch: 427/1000  -  Loss: 177.8816375732422\n",
            "Epoch: 428/1000  -  Loss: 177.88198852539062\n",
            "Epoch: 429/1000  -  Loss: 177.88182067871094\n",
            "Epoch: 430/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 431/1000  -  Loss: 177.8827362060547\n",
            "Epoch: 432/1000  -  Loss: 177.8822784423828\n",
            "Epoch: 433/1000  -  Loss: 177.88148498535156\n",
            "Epoch: 434/1000  -  Loss: 177.88217163085938\n",
            "Epoch: 435/1000  -  Loss: 177.8818359375\n",
            "Epoch: 436/1000  -  Loss: 177.88198852539062\n",
            "Epoch: 437/1000  -  Loss: 177.88162231445312\n",
            "Epoch: 438/1000  -  Loss: 177.8824005126953\n",
            "Epoch: 439/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 440/1000  -  Loss: 177.88160705566406\n",
            "Epoch: 441/1000  -  Loss: 177.88192749023438\n",
            "Epoch: 442/1000  -  Loss: 177.88160705566406\n",
            "Epoch: 443/1000  -  Loss: 177.8822784423828\n",
            "Epoch: 444/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 445/1000  -  Loss: 177.88241577148438\n",
            "Epoch: 446/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 447/1000  -  Loss: 177.88198852539062\n",
            "Epoch: 448/1000  -  Loss: 177.8817596435547\n",
            "Epoch: 449/1000  -  Loss: 177.8819122314453\n",
            "Epoch: 450/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 451/1000  -  Loss: 177.88121032714844\n",
            "Epoch: 452/1000  -  Loss: 177.8822021484375\n",
            "Epoch: 453/1000  -  Loss: 177.8819122314453\n",
            "Epoch: 454/1000  -  Loss: 177.88214111328125\n",
            "Epoch: 455/1000  -  Loss: 177.88160705566406\n",
            "Epoch: 456/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 457/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 458/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 459/1000  -  Loss: 177.88255310058594\n",
            "Epoch: 460/1000  -  Loss: 177.8823699951172\n",
            "Epoch: 461/1000  -  Loss: 177.88287353515625\n",
            "Epoch: 462/1000  -  Loss: 177.8825225830078\n",
            "Epoch: 463/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 464/1000  -  Loss: 177.88148498535156\n",
            "Epoch: 465/1000  -  Loss: 177.88217163085938\n",
            "Epoch: 466/1000  -  Loss: 177.8824005126953\n",
            "Epoch: 467/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 468/1000  -  Loss: 177.8817596435547\n",
            "Epoch: 469/1000  -  Loss: 177.8819122314453\n",
            "Epoch: 470/1000  -  Loss: 177.8819122314453\n",
            "Epoch: 471/1000  -  Loss: 177.8818359375\n",
            "Epoch: 472/1000  -  Loss: 177.88218688964844\n",
            "Epoch: 473/1000  -  Loss: 177.88148498535156\n",
            "Epoch: 474/1000  -  Loss: 177.88314819335938\n",
            "Epoch: 475/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 476/1000  -  Loss: 177.88182067871094\n",
            "Epoch: 477/1000  -  Loss: 177.88160705566406\n",
            "Epoch: 478/1000  -  Loss: 177.88156127929688\n",
            "Epoch: 479/1000  -  Loss: 177.88185119628906\n",
            "Epoch: 480/1000  -  Loss: 177.88150024414062\n",
            "Epoch: 481/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 482/1000  -  Loss: 177.88265991210938\n",
            "Epoch: 483/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 484/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 485/1000  -  Loss: 177.88262939453125\n",
            "Epoch: 486/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 487/1000  -  Loss: 177.8822479248047\n",
            "Epoch: 488/1000  -  Loss: 177.8822021484375\n",
            "Epoch: 489/1000  -  Loss: 177.88209533691406\n",
            "Epoch: 490/1000  -  Loss: 177.8822479248047\n",
            "Epoch: 491/1000  -  Loss: 177.88125610351562\n",
            "Epoch: 492/1000  -  Loss: 177.8816375732422\n",
            "Epoch: 493/1000  -  Loss: 177.8819122314453\n",
            "Epoch: 494/1000  -  Loss: 177.88296508789062\n",
            "Epoch: 495/1000  -  Loss: 177.88226318359375\n",
            "Epoch: 496/1000  -  Loss: 177.88246154785156\n",
            "Epoch: 497/1000  -  Loss: 177.88238525390625\n",
            "Epoch: 498/1000  -  Loss: 177.88218688964844\n",
            "Epoch: 499/1000  -  Loss: 177.88165283203125\n",
            "Epoch: 500/1000  -  Loss: 177.8821258544922\n",
            "Epoch: 501/1000  -  Loss: 177.88185119628906\n",
            "Epoch: 502/1000  -  Loss: 177.88241577148438\n",
            "Epoch: 503/1000  -  Loss: 177.88192749023438\n",
            "Epoch: 504/1000  -  Loss: 177.88169860839844\n",
            "Epoch: 505/1000  -  Loss: 177.88156127929688\n",
            "Epoch: 506/1000  -  Loss: 177.8822479248047\n",
            "Epoch: 507/1000  -  Loss: 177.88143920898438\n",
            "Epoch: 508/1000  -  Loss: 177.8826141357422\n",
            "Epoch: 509/1000  -  Loss: 177.88218688964844\n",
            "Epoch: 510/1000  -  Loss: 177.8817138671875\n",
            "Epoch: 511/1000  -  Loss: 177.88162231445312\n",
            "Epoch: 512/1000  -  Loss: 177.8814697265625\n",
            "Epoch: 513/1000  -  Loss: 177.88128662109375\n",
            "Epoch: 514/1000  -  Loss: 177.8805694580078\n",
            "Epoch: 515/1000  -  Loss: 177.8822021484375\n",
            "Epoch: 516/1000  -  Loss: 177.88133239746094\n",
            "Epoch: 517/1000  -  Loss: 177.88267517089844\n",
            "Epoch: 518/1000  -  Loss: 177.88204956054688\n",
            "Epoch: 519/1000  -  Loss: 177.88182067871094\n",
            "Epoch: 520/1000  -  Loss: 177.88253784179688\n",
            "Epoch: 521/1000  -  Loss: 177.88133239746094\n",
            "Epoch: 522/1000  -  Loss: 177.8821258544922\n",
            "Epoch: 523/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 524/1000  -  Loss: 177.88226318359375\n",
            "Epoch: 525/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 526/1000  -  Loss: 177.8824005126953\n",
            "Epoch: 527/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 528/1000  -  Loss: 177.88153076171875\n",
            "Epoch: 529/1000  -  Loss: 177.88230895996094\n",
            "Epoch: 530/1000  -  Loss: 177.88121032714844\n",
            "Epoch: 531/1000  -  Loss: 177.88148498535156\n",
            "Epoch: 532/1000  -  Loss: 177.88258361816406\n",
            "Epoch: 533/1000  -  Loss: 177.8818359375\n",
            "Epoch: 534/1000  -  Loss: 177.88211059570312\n",
            "Epoch: 535/1000  -  Loss: 177.8825225830078\n",
            "Epoch: 536/1000  -  Loss: 177.88209533691406\n",
            "Epoch: 537/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 538/1000  -  Loss: 177.88121032714844\n",
            "Epoch: 539/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 540/1000  -  Loss: 177.88148498535156\n",
            "Epoch: 541/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 542/1000  -  Loss: 177.88265991210938\n",
            "Epoch: 543/1000  -  Loss: 177.88133239746094\n",
            "Epoch: 544/1000  -  Loss: 177.8822784423828\n",
            "Epoch: 545/1000  -  Loss: 177.88189697265625\n",
            "Epoch: 546/1000  -  Loss: 177.8815460205078\n",
            "Epoch: 547/1000  -  Loss: 177.88238525390625\n",
            "Epoch: 548/1000  -  Loss: 177.88116455078125\n",
            "Epoch: 549/1000  -  Loss: 177.8814697265625\n",
            "Epoch: 550/1000  -  Loss: 177.8823699951172\n",
            "Epoch: 551/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 552/1000  -  Loss: 177.88157653808594\n",
            "Epoch: 553/1000  -  Loss: 177.88134765625\n",
            "Epoch: 554/1000  -  Loss: 177.88189697265625\n",
            "Epoch: 555/1000  -  Loss: 177.88169860839844\n",
            "Epoch: 556/1000  -  Loss: 177.8818817138672\n",
            "Epoch: 557/1000  -  Loss: 177.88198852539062\n",
            "Epoch: 558/1000  -  Loss: 177.8817596435547\n",
            "Epoch: 559/1000  -  Loss: 177.88258361816406\n",
            "Epoch: 560/1000  -  Loss: 177.8822479248047\n",
            "Epoch: 561/1000  -  Loss: 177.8811492919922\n",
            "Epoch: 562/1000  -  Loss: 177.88153076171875\n",
            "Epoch: 563/1000  -  Loss: 177.8816375732422\n",
            "Epoch: 564/1000  -  Loss: 177.88134765625\n",
            "Epoch: 565/1000  -  Loss: 177.88092041015625\n",
            "Epoch: 566/1000  -  Loss: 177.88111877441406\n",
            "Epoch: 567/1000  -  Loss: 177.88116455078125\n",
            "Epoch: 568/1000  -  Loss: 177.880859375\n",
            "Epoch: 569/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 570/1000  -  Loss: 177.88308715820312\n",
            "Epoch: 571/1000  -  Loss: 177.8823699951172\n",
            "Epoch: 572/1000  -  Loss: 177.88165283203125\n",
            "Epoch: 573/1000  -  Loss: 177.88336181640625\n",
            "Epoch: 574/1000  -  Loss: 177.88128662109375\n",
            "Epoch: 575/1000  -  Loss: 177.8816375732422\n",
            "Epoch: 576/1000  -  Loss: 177.8822784423828\n",
            "Epoch: 577/1000  -  Loss: 177.88043212890625\n",
            "Epoch: 578/1000  -  Loss: 177.8817138671875\n",
            "Epoch: 579/1000  -  Loss: 177.88218688964844\n",
            "Epoch: 580/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 581/1000  -  Loss: 177.8818359375\n",
            "Epoch: 582/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 583/1000  -  Loss: 177.88232421875\n",
            "Epoch: 584/1000  -  Loss: 177.88169860839844\n",
            "Epoch: 585/1000  -  Loss: 177.88198852539062\n",
            "Epoch: 586/1000  -  Loss: 177.88233947753906\n",
            "Epoch: 587/1000  -  Loss: 177.8824005126953\n",
            "Epoch: 588/1000  -  Loss: 177.8818817138672\n",
            "Epoch: 589/1000  -  Loss: 177.88218688964844\n",
            "Epoch: 590/1000  -  Loss: 177.8816375732422\n",
            "Epoch: 591/1000  -  Loss: 177.8817596435547\n",
            "Epoch: 592/1000  -  Loss: 177.88226318359375\n",
            "Epoch: 593/1000  -  Loss: 177.88209533691406\n",
            "Epoch: 594/1000  -  Loss: 177.88230895996094\n",
            "Epoch: 595/1000  -  Loss: 177.8817901611328\n",
            "Epoch: 596/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 597/1000  -  Loss: 177.8822784423828\n",
            "Epoch: 598/1000  -  Loss: 177.88250732421875\n",
            "Epoch: 599/1000  -  Loss: 177.8824005126953\n",
            "Epoch: 600/1000  -  Loss: 177.88182067871094\n",
            "Epoch: 601/1000  -  Loss: 177.88128662109375\n",
            "Epoch: 602/1000  -  Loss: 177.88189697265625\n",
            "Epoch: 603/1000  -  Loss: 177.88121032714844\n",
            "Epoch: 604/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 605/1000  -  Loss: 177.8826904296875\n",
            "Epoch: 606/1000  -  Loss: 177.8824462890625\n",
            "Epoch: 607/1000  -  Loss: 177.8826904296875\n",
            "Epoch: 608/1000  -  Loss: 177.88192749023438\n",
            "Epoch: 609/1000  -  Loss: 177.88290405273438\n",
            "Epoch: 610/1000  -  Loss: 177.88232421875\n",
            "Epoch: 611/1000  -  Loss: 177.88209533691406\n",
            "Epoch: 612/1000  -  Loss: 177.8817901611328\n",
            "Epoch: 613/1000  -  Loss: 177.88192749023438\n",
            "Epoch: 614/1000  -  Loss: 177.88197326660156\n",
            "Epoch: 615/1000  -  Loss: 177.88162231445312\n",
            "Epoch: 616/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 617/1000  -  Loss: 177.8816375732422\n",
            "Epoch: 618/1000  -  Loss: 177.88230895996094\n",
            "Epoch: 619/1000  -  Loss: 177.88192749023438\n",
            "Epoch: 620/1000  -  Loss: 177.88148498535156\n",
            "Epoch: 621/1000  -  Loss: 177.8817138671875\n",
            "Epoch: 622/1000  -  Loss: 177.8827362060547\n",
            "Epoch: 623/1000  -  Loss: 177.88182067871094\n",
            "Epoch: 624/1000  -  Loss: 177.8825225830078\n",
            "Epoch: 625/1000  -  Loss: 177.88133239746094\n",
            "Epoch: 626/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 627/1000  -  Loss: 177.88113403320312\n",
            "Epoch: 628/1000  -  Loss: 177.8817138671875\n",
            "Epoch: 629/1000  -  Loss: 177.88217163085938\n",
            "Epoch: 630/1000  -  Loss: 177.88153076171875\n",
            "Epoch: 631/1000  -  Loss: 177.88296508789062\n",
            "Epoch: 632/1000  -  Loss: 177.8821258544922\n",
            "Epoch: 633/1000  -  Loss: 177.88230895996094\n",
            "Epoch: 634/1000  -  Loss: 177.88267517089844\n",
            "Epoch: 635/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 636/1000  -  Loss: 177.88226318359375\n",
            "Epoch: 637/1000  -  Loss: 177.8828582763672\n",
            "Epoch: 638/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 639/1000  -  Loss: 177.88197326660156\n",
            "Epoch: 640/1000  -  Loss: 177.88241577148438\n",
            "Epoch: 641/1000  -  Loss: 177.88182067871094\n",
            "Epoch: 642/1000  -  Loss: 177.8817901611328\n",
            "Epoch: 643/1000  -  Loss: 177.88226318359375\n",
            "Epoch: 644/1000  -  Loss: 177.8827362060547\n",
            "Epoch: 645/1000  -  Loss: 177.8818817138672\n",
            "Epoch: 646/1000  -  Loss: 177.88197326660156\n",
            "Epoch: 647/1000  -  Loss: 177.88217163085938\n",
            "Epoch: 648/1000  -  Loss: 177.8817901611328\n",
            "Epoch: 649/1000  -  Loss: 177.8807373046875\n",
            "Epoch: 650/1000  -  Loss: 177.88204956054688\n",
            "Epoch: 651/1000  -  Loss: 177.8817901611328\n",
            "Epoch: 652/1000  -  Loss: 177.8826141357422\n",
            "Epoch: 653/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 654/1000  -  Loss: 177.88238525390625\n",
            "Epoch: 655/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 656/1000  -  Loss: 177.88204956054688\n",
            "Epoch: 657/1000  -  Loss: 177.88214111328125\n",
            "Epoch: 658/1000  -  Loss: 177.8821258544922\n",
            "Epoch: 659/1000  -  Loss: 177.88177490234375\n",
            "Epoch: 660/1000  -  Loss: 177.8809051513672\n",
            "Epoch: 661/1000  -  Loss: 177.8824005126953\n",
            "Epoch: 662/1000  -  Loss: 177.88217163085938\n",
            "Epoch: 663/1000  -  Loss: 177.88192749023438\n",
            "Epoch: 664/1000  -  Loss: 177.88133239746094\n",
            "Epoch: 665/1000  -  Loss: 177.88209533691406\n",
            "Epoch: 666/1000  -  Loss: 177.88209533691406\n",
            "Epoch: 667/1000  -  Loss: 177.88156127929688\n",
            "Epoch: 668/1000  -  Loss: 177.88217163085938\n",
            "Epoch: 669/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 670/1000  -  Loss: 177.8815460205078\n",
            "Epoch: 671/1000  -  Loss: 177.88079833984375\n",
            "Epoch: 672/1000  -  Loss: 177.88265991210938\n",
            "Epoch: 673/1000  -  Loss: 177.8824462890625\n",
            "Epoch: 674/1000  -  Loss: 177.88218688964844\n",
            "Epoch: 675/1000  -  Loss: 177.88265991210938\n",
            "Epoch: 676/1000  -  Loss: 177.88156127929688\n",
            "Epoch: 677/1000  -  Loss: 177.8819122314453\n",
            "Epoch: 678/1000  -  Loss: 177.88218688964844\n",
            "Epoch: 679/1000  -  Loss: 177.88232421875\n",
            "Epoch: 680/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 681/1000  -  Loss: 177.88204956054688\n",
            "Epoch: 682/1000  -  Loss: 177.88233947753906\n",
            "Epoch: 683/1000  -  Loss: 177.88265991210938\n",
            "Epoch: 684/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 685/1000  -  Loss: 177.8818817138672\n",
            "Epoch: 686/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 687/1000  -  Loss: 177.8819122314453\n",
            "Epoch: 688/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 689/1000  -  Loss: 177.8813018798828\n",
            "Epoch: 690/1000  -  Loss: 177.88233947753906\n",
            "Epoch: 691/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 692/1000  -  Loss: 177.8824462890625\n",
            "Epoch: 693/1000  -  Loss: 177.8814697265625\n",
            "Epoch: 694/1000  -  Loss: 177.88177490234375\n",
            "Epoch: 695/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 696/1000  -  Loss: 177.8827667236328\n",
            "Epoch: 697/1000  -  Loss: 177.88211059570312\n",
            "Epoch: 698/1000  -  Loss: 177.88108825683594\n",
            "Epoch: 699/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 700/1000  -  Loss: 177.88182067871094\n",
            "Epoch: 701/1000  -  Loss: 177.88211059570312\n",
            "Epoch: 702/1000  -  Loss: 177.88311767578125\n",
            "Epoch: 703/1000  -  Loss: 177.8821258544922\n",
            "Epoch: 704/1000  -  Loss: 177.8810577392578\n",
            "Epoch: 705/1000  -  Loss: 177.88214111328125\n",
            "Epoch: 706/1000  -  Loss: 177.88177490234375\n",
            "Epoch: 707/1000  -  Loss: 177.88204956054688\n",
            "Epoch: 708/1000  -  Loss: 177.88192749023438\n",
            "Epoch: 709/1000  -  Loss: 177.88150024414062\n",
            "Epoch: 710/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 711/1000  -  Loss: 177.88255310058594\n",
            "Epoch: 712/1000  -  Loss: 177.88107299804688\n",
            "Epoch: 713/1000  -  Loss: 177.88323974609375\n",
            "Epoch: 714/1000  -  Loss: 177.8822479248047\n",
            "Epoch: 715/1000  -  Loss: 177.88157653808594\n",
            "Epoch: 716/1000  -  Loss: 177.88174438476562\n",
            "Epoch: 717/1000  -  Loss: 177.88198852539062\n",
            "Epoch: 718/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 719/1000  -  Loss: 177.88174438476562\n",
            "Epoch: 720/1000  -  Loss: 177.8822784423828\n",
            "Epoch: 721/1000  -  Loss: 177.88267517089844\n",
            "Epoch: 722/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 723/1000  -  Loss: 177.88230895996094\n",
            "Epoch: 724/1000  -  Loss: 177.8817901611328\n",
            "Epoch: 725/1000  -  Loss: 177.8822479248047\n",
            "Epoch: 726/1000  -  Loss: 177.88241577148438\n",
            "Epoch: 727/1000  -  Loss: 177.88198852539062\n",
            "Epoch: 728/1000  -  Loss: 177.8821258544922\n",
            "Epoch: 729/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 730/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 731/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 732/1000  -  Loss: 177.88265991210938\n",
            "Epoch: 733/1000  -  Loss: 177.88174438476562\n",
            "Epoch: 734/1000  -  Loss: 177.88125610351562\n",
            "Epoch: 735/1000  -  Loss: 177.8813934326172\n",
            "Epoch: 736/1000  -  Loss: 177.88197326660156\n",
            "Epoch: 737/1000  -  Loss: 177.88150024414062\n",
            "Epoch: 738/1000  -  Loss: 177.88113403320312\n",
            "Epoch: 739/1000  -  Loss: 177.88279724121094\n",
            "Epoch: 740/1000  -  Loss: 177.88253784179688\n",
            "Epoch: 741/1000  -  Loss: 177.88275146484375\n",
            "Epoch: 742/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 743/1000  -  Loss: 177.8824005126953\n",
            "Epoch: 744/1000  -  Loss: 177.88272094726562\n",
            "Epoch: 745/1000  -  Loss: 177.88217163085938\n",
            "Epoch: 746/1000  -  Loss: 177.88214111328125\n",
            "Epoch: 747/1000  -  Loss: 177.88192749023438\n",
            "Epoch: 748/1000  -  Loss: 177.8824005126953\n",
            "Epoch: 749/1000  -  Loss: 177.88279724121094\n",
            "Epoch: 750/1000  -  Loss: 177.88160705566406\n",
            "Epoch: 751/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 752/1000  -  Loss: 177.88232421875\n",
            "Epoch: 753/1000  -  Loss: 177.88150024414062\n",
            "Epoch: 754/1000  -  Loss: 177.88197326660156\n",
            "Epoch: 755/1000  -  Loss: 177.8822784423828\n",
            "Epoch: 756/1000  -  Loss: 177.88218688964844\n",
            "Epoch: 757/1000  -  Loss: 177.88204956054688\n",
            "Epoch: 758/1000  -  Loss: 177.8823699951172\n",
            "Epoch: 759/1000  -  Loss: 177.88209533691406\n",
            "Epoch: 760/1000  -  Loss: 177.8814239501953\n",
            "Epoch: 761/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 762/1000  -  Loss: 177.8814239501953\n",
            "Epoch: 763/1000  -  Loss: 177.88233947753906\n",
            "Epoch: 764/1000  -  Loss: 177.88162231445312\n",
            "Epoch: 765/1000  -  Loss: 177.88192749023438\n",
            "Epoch: 766/1000  -  Loss: 177.8809814453125\n",
            "Epoch: 767/1000  -  Loss: 177.8828125\n",
            "Epoch: 768/1000  -  Loss: 177.88246154785156\n",
            "Epoch: 769/1000  -  Loss: 177.88157653808594\n",
            "Epoch: 770/1000  -  Loss: 177.8817138671875\n",
            "Epoch: 771/1000  -  Loss: 177.8818359375\n",
            "Epoch: 772/1000  -  Loss: 177.88250732421875\n",
            "Epoch: 773/1000  -  Loss: 177.8822021484375\n",
            "Epoch: 774/1000  -  Loss: 177.88160705566406\n",
            "Epoch: 775/1000  -  Loss: 177.88101196289062\n",
            "Epoch: 776/1000  -  Loss: 177.88067626953125\n",
            "Epoch: 777/1000  -  Loss: 177.8822021484375\n",
            "Epoch: 778/1000  -  Loss: 177.88214111328125\n",
            "Epoch: 779/1000  -  Loss: 177.88230895996094\n",
            "Epoch: 780/1000  -  Loss: 177.88134765625\n",
            "Epoch: 781/1000  -  Loss: 177.88148498535156\n",
            "Epoch: 782/1000  -  Loss: 177.8817596435547\n",
            "Epoch: 783/1000  -  Loss: 177.88134765625\n",
            "Epoch: 784/1000  -  Loss: 177.88113403320312\n",
            "Epoch: 785/1000  -  Loss: 177.88238525390625\n",
            "Epoch: 786/1000  -  Loss: 177.8824005126953\n",
            "Epoch: 787/1000  -  Loss: 177.88150024414062\n",
            "Epoch: 788/1000  -  Loss: 177.88204956054688\n",
            "Epoch: 789/1000  -  Loss: 177.8805694580078\n",
            "Epoch: 790/1000  -  Loss: 177.88133239746094\n",
            "Epoch: 791/1000  -  Loss: 177.88250732421875\n",
            "Epoch: 792/1000  -  Loss: 177.8817138671875\n",
            "Epoch: 793/1000  -  Loss: 177.8821258544922\n",
            "Epoch: 794/1000  -  Loss: 177.88265991210938\n",
            "Epoch: 795/1000  -  Loss: 177.88211059570312\n",
            "Epoch: 796/1000  -  Loss: 177.88259887695312\n",
            "Epoch: 797/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 798/1000  -  Loss: 177.88148498535156\n",
            "Epoch: 799/1000  -  Loss: 177.88165283203125\n",
            "Epoch: 800/1000  -  Loss: 177.88217163085938\n",
            "Epoch: 801/1000  -  Loss: 177.88230895996094\n",
            "Epoch: 802/1000  -  Loss: 177.8814697265625\n",
            "Epoch: 803/1000  -  Loss: 177.8818359375\n",
            "Epoch: 804/1000  -  Loss: 177.88116455078125\n",
            "Epoch: 805/1000  -  Loss: 177.8822784423828\n",
            "Epoch: 806/1000  -  Loss: 177.88233947753906\n",
            "Epoch: 807/1000  -  Loss: 177.88294982910156\n",
            "Epoch: 808/1000  -  Loss: 177.88169860839844\n",
            "Epoch: 809/1000  -  Loss: 177.88165283203125\n",
            "Epoch: 810/1000  -  Loss: 177.8822479248047\n",
            "Epoch: 811/1000  -  Loss: 177.8816375732422\n",
            "Epoch: 812/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 813/1000  -  Loss: 177.88143920898438\n",
            "Epoch: 814/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 815/1000  -  Loss: 177.88160705566406\n",
            "Epoch: 816/1000  -  Loss: 177.88113403320312\n",
            "Epoch: 817/1000  -  Loss: 177.8822479248047\n",
            "Epoch: 818/1000  -  Loss: 177.8821258544922\n",
            "Epoch: 819/1000  -  Loss: 177.8813934326172\n",
            "Epoch: 820/1000  -  Loss: 177.88233947753906\n",
            "Epoch: 821/1000  -  Loss: 177.88116455078125\n",
            "Epoch: 822/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 823/1000  -  Loss: 177.88182067871094\n",
            "Epoch: 824/1000  -  Loss: 177.88148498535156\n",
            "Epoch: 825/1000  -  Loss: 177.88211059570312\n",
            "Epoch: 826/1000  -  Loss: 177.88162231445312\n",
            "Epoch: 827/1000  -  Loss: 177.88217163085938\n",
            "Epoch: 828/1000  -  Loss: 177.88247680664062\n",
            "Epoch: 829/1000  -  Loss: 177.88209533691406\n",
            "Epoch: 830/1000  -  Loss: 177.88108825683594\n",
            "Epoch: 831/1000  -  Loss: 177.8816375732422\n",
            "Epoch: 832/1000  -  Loss: 177.88169860839844\n",
            "Epoch: 833/1000  -  Loss: 177.8821258544922\n",
            "Epoch: 834/1000  -  Loss: 177.8813934326172\n",
            "Epoch: 835/1000  -  Loss: 177.8817596435547\n",
            "Epoch: 836/1000  -  Loss: 177.8833770751953\n",
            "Epoch: 837/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 838/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 839/1000  -  Loss: 177.8813934326172\n",
            "Epoch: 840/1000  -  Loss: 177.88111877441406\n",
            "Epoch: 841/1000  -  Loss: 177.88160705566406\n",
            "Epoch: 842/1000  -  Loss: 177.8820343017578\n",
            "Epoch: 843/1000  -  Loss: 177.88253784179688\n",
            "Epoch: 844/1000  -  Loss: 177.88226318359375\n",
            "Epoch: 845/1000  -  Loss: 177.88192749023438\n",
            "Epoch: 846/1000  -  Loss: 177.8813934326172\n",
            "Epoch: 847/1000  -  Loss: 177.88156127929688\n",
            "Epoch: 848/1000  -  Loss: 177.88226318359375\n",
            "Epoch: 849/1000  -  Loss: 177.88182067871094\n",
            "Epoch: 850/1000  -  Loss: 177.8822021484375\n",
            "Epoch: 851/1000  -  Loss: 177.88241577148438\n",
            "Epoch: 852/1000  -  Loss: 177.8824005126953\n",
            "Epoch: 853/1000  -  Loss: 177.88226318359375\n",
            "Epoch: 854/1000  -  Loss: 177.88198852539062\n",
            "Epoch: 855/1000  -  Loss: 177.8815460205078\n",
            "Epoch: 856/1000  -  Loss: 177.8818817138672\n",
            "Epoch: 857/1000  -  Loss: 177.8824462890625\n",
            "Epoch: 858/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 859/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 860/1000  -  Loss: 177.88189697265625\n",
            "Epoch: 861/1000  -  Loss: 177.88165283203125\n",
            "Epoch: 862/1000  -  Loss: 177.88241577148438\n",
            "Epoch: 863/1000  -  Loss: 177.88169860839844\n",
            "Epoch: 864/1000  -  Loss: 177.88197326660156\n",
            "Epoch: 865/1000  -  Loss: 177.88316345214844\n",
            "Epoch: 866/1000  -  Loss: 177.88218688964844\n",
            "Epoch: 867/1000  -  Loss: 177.8822479248047\n",
            "Epoch: 868/1000  -  Loss: 177.88238525390625\n",
            "Epoch: 869/1000  -  Loss: 177.88174438476562\n",
            "Epoch: 870/1000  -  Loss: 177.88189697265625\n",
            "Epoch: 871/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 872/1000  -  Loss: 177.8810577392578\n",
            "Epoch: 873/1000  -  Loss: 177.8812713623047\n",
            "Epoch: 874/1000  -  Loss: 177.88169860839844\n",
            "Epoch: 875/1000  -  Loss: 177.88143920898438\n",
            "Epoch: 876/1000  -  Loss: 177.88104248046875\n",
            "Epoch: 877/1000  -  Loss: 177.88182067871094\n",
            "Epoch: 878/1000  -  Loss: 177.8818817138672\n",
            "Epoch: 879/1000  -  Loss: 177.88197326660156\n",
            "Epoch: 880/1000  -  Loss: 177.88246154785156\n",
            "Epoch: 881/1000  -  Loss: 177.88143920898438\n",
            "Epoch: 882/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 883/1000  -  Loss: 177.88247680664062\n",
            "Epoch: 884/1000  -  Loss: 177.88265991210938\n",
            "Epoch: 885/1000  -  Loss: 177.8822784423828\n",
            "Epoch: 886/1000  -  Loss: 177.88174438476562\n",
            "Epoch: 887/1000  -  Loss: 177.8817901611328\n",
            "Epoch: 888/1000  -  Loss: 177.8818359375\n",
            "Epoch: 889/1000  -  Loss: 177.88226318359375\n",
            "Epoch: 890/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 891/1000  -  Loss: 177.88185119628906\n",
            "Epoch: 892/1000  -  Loss: 177.8822784423828\n",
            "Epoch: 893/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 894/1000  -  Loss: 177.88192749023438\n",
            "Epoch: 895/1000  -  Loss: 177.88192749023438\n",
            "Epoch: 896/1000  -  Loss: 177.88250732421875\n",
            "Epoch: 897/1000  -  Loss: 177.8824005126953\n",
            "Epoch: 898/1000  -  Loss: 177.8817596435547\n",
            "Epoch: 899/1000  -  Loss: 177.88211059570312\n",
            "Epoch: 900/1000  -  Loss: 177.8822784423828\n",
            "Epoch: 901/1000  -  Loss: 177.88218688964844\n",
            "Epoch: 902/1000  -  Loss: 177.88165283203125\n",
            "Epoch: 903/1000  -  Loss: 177.88162231445312\n",
            "Epoch: 904/1000  -  Loss: 177.8814697265625\n",
            "Epoch: 905/1000  -  Loss: 177.8828125\n",
            "Epoch: 906/1000  -  Loss: 177.8822021484375\n",
            "Epoch: 907/1000  -  Loss: 177.88255310058594\n",
            "Epoch: 908/1000  -  Loss: 177.88177490234375\n",
            "Epoch: 909/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 910/1000  -  Loss: 177.88169860839844\n",
            "Epoch: 911/1000  -  Loss: 177.88214111328125\n",
            "Epoch: 912/1000  -  Loss: 177.8821258544922\n",
            "Epoch: 913/1000  -  Loss: 177.88233947753906\n",
            "Epoch: 914/1000  -  Loss: 177.8824005126953\n",
            "Epoch: 915/1000  -  Loss: 177.88160705566406\n",
            "Epoch: 916/1000  -  Loss: 177.88255310058594\n",
            "Epoch: 917/1000  -  Loss: 177.88160705566406\n",
            "Epoch: 918/1000  -  Loss: 177.88209533691406\n",
            "Epoch: 919/1000  -  Loss: 177.88307189941406\n",
            "Epoch: 920/1000  -  Loss: 177.88209533691406\n",
            "Epoch: 921/1000  -  Loss: 177.8818817138672\n",
            "Epoch: 922/1000  -  Loss: 177.88232421875\n",
            "Epoch: 923/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 924/1000  -  Loss: 177.8816375732422\n",
            "Epoch: 925/1000  -  Loss: 177.88247680664062\n",
            "Epoch: 926/1000  -  Loss: 177.8822021484375\n",
            "Epoch: 927/1000  -  Loss: 177.88214111328125\n",
            "Epoch: 928/1000  -  Loss: 177.88185119628906\n",
            "Epoch: 929/1000  -  Loss: 177.8818817138672\n",
            "Epoch: 930/1000  -  Loss: 177.88233947753906\n",
            "Epoch: 931/1000  -  Loss: 177.88247680664062\n",
            "Epoch: 932/1000  -  Loss: 177.88230895996094\n",
            "Epoch: 933/1000  -  Loss: 177.8817901611328\n",
            "Epoch: 934/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 935/1000  -  Loss: 177.8822021484375\n",
            "Epoch: 936/1000  -  Loss: 177.8819122314453\n",
            "Epoch: 937/1000  -  Loss: 177.8817138671875\n",
            "Epoch: 938/1000  -  Loss: 177.88198852539062\n",
            "Epoch: 939/1000  -  Loss: 177.88157653808594\n",
            "Epoch: 940/1000  -  Loss: 177.8817138671875\n",
            "Epoch: 941/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 942/1000  -  Loss: 177.88226318359375\n",
            "Epoch: 943/1000  -  Loss: 177.8810577392578\n",
            "Epoch: 944/1000  -  Loss: 177.88116455078125\n",
            "Epoch: 945/1000  -  Loss: 177.88119506835938\n",
            "Epoch: 946/1000  -  Loss: 177.88217163085938\n",
            "Epoch: 947/1000  -  Loss: 177.88214111328125\n",
            "Epoch: 948/1000  -  Loss: 177.88169860839844\n",
            "Epoch: 949/1000  -  Loss: 177.88087463378906\n",
            "Epoch: 950/1000  -  Loss: 177.88111877441406\n",
            "Epoch: 951/1000  -  Loss: 177.8814697265625\n",
            "Epoch: 952/1000  -  Loss: 177.88412475585938\n",
            "Epoch: 953/1000  -  Loss: 177.88299560546875\n",
            "Epoch: 954/1000  -  Loss: 177.88230895996094\n",
            "Epoch: 955/1000  -  Loss: 177.88192749023438\n",
            "Epoch: 956/1000  -  Loss: 177.88253784179688\n",
            "Epoch: 957/1000  -  Loss: 177.88148498535156\n",
            "Epoch: 958/1000  -  Loss: 177.88113403320312\n",
            "Epoch: 959/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 960/1000  -  Loss: 177.88189697265625\n",
            "Epoch: 961/1000  -  Loss: 177.88214111328125\n",
            "Epoch: 962/1000  -  Loss: 177.8817138671875\n",
            "Epoch: 963/1000  -  Loss: 177.88125610351562\n",
            "Epoch: 964/1000  -  Loss: 177.8832550048828\n",
            "Epoch: 965/1000  -  Loss: 177.88258361816406\n",
            "Epoch: 966/1000  -  Loss: 177.88143920898438\n",
            "Epoch: 967/1000  -  Loss: 177.8817901611328\n",
            "Epoch: 968/1000  -  Loss: 177.8822479248047\n",
            "Epoch: 969/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 970/1000  -  Loss: 177.88168334960938\n",
            "Epoch: 971/1000  -  Loss: 177.8819580078125\n",
            "Epoch: 972/1000  -  Loss: 177.88148498535156\n",
            "Epoch: 973/1000  -  Loss: 177.88169860839844\n",
            "Epoch: 974/1000  -  Loss: 177.8805694580078\n",
            "Epoch: 975/1000  -  Loss: 177.8830108642578\n",
            "Epoch: 976/1000  -  Loss: 177.88282775878906\n",
            "Epoch: 977/1000  -  Loss: 177.8817596435547\n",
            "Epoch: 978/1000  -  Loss: 177.88282775878906\n",
            "Epoch: 979/1000  -  Loss: 177.88134765625\n",
            "Epoch: 980/1000  -  Loss: 177.88204956054688\n",
            "Epoch: 981/1000  -  Loss: 177.8816375732422\n",
            "Epoch: 982/1000  -  Loss: 177.88174438476562\n",
            "Epoch: 983/1000  -  Loss: 177.88201904296875\n",
            "Epoch: 984/1000  -  Loss: 177.88232421875\n",
            "Epoch: 985/1000  -  Loss: 177.88223266601562\n",
            "Epoch: 986/1000  -  Loss: 177.88156127929688\n",
            "Epoch: 987/1000  -  Loss: 177.8824005126953\n",
            "Epoch: 988/1000  -  Loss: 177.88279724121094\n",
            "Epoch: 989/1000  -  Loss: 177.88282775878906\n",
            "Epoch: 990/1000  -  Loss: 177.8818359375\n",
            "Epoch: 991/1000  -  Loss: 177.8827362060547\n",
            "Epoch: 992/1000  -  Loss: 177.8817596435547\n",
            "Epoch: 993/1000  -  Loss: 177.8814697265625\n",
            "Epoch: 994/1000  -  Loss: 177.88162231445312\n",
            "Epoch: 995/1000  -  Loss: 177.88204956054688\n",
            "Epoch: 996/1000  -  Loss: 177.8809051513672\n",
            "Epoch: 997/1000  -  Loss: 177.88238525390625\n",
            "Epoch: 998/1000  -  Loss: 177.88206481933594\n",
            "Epoch: 999/1000  -  Loss: 177.8822784423828\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "num_topics = 5\n",
        "t_hidden_size = 100\n",
        "rho_size = len(embedding_data[0])\n",
        "emb_size = len(embedding_data[0])\n",
        "theta_act = \"relu\"\n",
        "\n",
        "class TrainArguments:\n",
        "      def __init__(self, epochs, batch_size, log_interval):\n",
        "          self.epochs = epochs\n",
        "          self.batch_size = batch_size\n",
        "          self.log_interval = log_interval\n",
        "\n",
        "class OptimizerArguments:\n",
        "      def __init__(self, optimizer_name, lr, wdecay):\n",
        "            self.optimizer = optimizer_name\n",
        "            self.lr = lr\n",
        "            self.wdecay = wdecay\n",
        "            \n",
        "train_args = TrainArguments(epochs=1000, batch_size=6, log_interval=None)\n",
        "optimizer_args = OptimizerArguments(optimizer_name=\"adam\", lr=0.001, wdecay=0.1)\n",
        "\n",
        "print(train_args.epochs)\n",
        "print(optimizer_args.optimizer)\n",
        "\n",
        "training_set = train_set\n",
        "\n",
        "# define the ETM-model with setting-parameters\n",
        "etm_model = ETM(\n",
        "      num_topics, \n",
        "      vocab_size, \n",
        "      t_hidden_size, rho_size, emb_size, theta_act, \n",
        "      embedding_data, \n",
        "      enc_drop=0.5)\n",
        "\n",
        "# start training\n",
        "train_class = TrainETM().train(\n",
        "    etm_model,\n",
        "    vocab_size, \n",
        "    train_args, optimizer_args, training_set) \n",
        "    #num_topics, t_hidden_size, rho_size, emb_size, theta_act, embedding_data, 0.5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "notebook_replication.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
