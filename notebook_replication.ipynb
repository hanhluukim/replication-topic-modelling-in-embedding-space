{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook_replication.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanhluukim/replication-topic-modelling-in-embedding-space/blob/main/notebook_replication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Das Projekt aus dem Github klonen und in den Projektsordner**"
      ],
      "metadata": {
        "id": "PJ7P852F7yzU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "riOxinNHJcIB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "341d1082-7806-4c77-84e8-acda0450338e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'replication-topic-modelling-in-embedding-space'...\n",
            "remote: Enumerating objects: 313, done.\u001b[K\n",
            "remote: Counting objects: 100% (88/88), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 313 (delta 53), reused 54 (delta 23), pack-reused 225\u001b[K\n",
            "Receiving objects: 100% (313/313), 4.62 MiB | 12.39 MiB/s, done.\n",
            "Resolving deltas: 100% (163/163), done.\n"
          ]
        }
      ],
      "source": [
        "#wenn die Ordner noch nicht geklont ist, soll dieser Fehler zuerst durchgeführt werden.\n",
        "!git clone https://github.com/hanhluukim/replication-topic-modelling-in-embedding-space.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/replication-topic-modelling-in-embedding-space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_6em-5qJg5e",
        "outputId": "e3da1937-8b0c-4f86-9ef3-4c695c6fa545"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/replication-topic-modelling-in-embedding-space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Die benötige Paketen für das Projekt mittels requirements.txt installieren**\n",
        "\n"
      ],
      "metadata": {
        "id": "6AAG98vV1JCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Falls die Packages noch nicht installiert wurden, \n",
        "!pip install -r \"/content/replication-topic-modelling-in-embedding-space/requirements.txt\""
      ],
      "metadata": {
        "id": "YcBay625sD5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baf8c149-664f-46d9-aef6-ac1aba3e3d58"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 2)) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 3)) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 4)) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 6)) (1.11.0+cu113)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 28.4 MB/s \n",
            "\u001b[?25hCollecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting plotly==5.7.0\n",
            "  Downloading plotly-5.7.0-py2.py3-none-any.whl (28.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.8 MB 74.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pathlib in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 10)) (1.0.1)\n",
            "Collecting pyyaml==5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 47.7 MB/s \n",
            "\u001b[?25hCollecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 79.9 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly==5.7.0->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 9)) (8.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly==5.7.0->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 9)) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 1)) (6.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 6)) (4.2.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 48.7 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 62.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (3.0.8)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 8)) (0.51.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.6.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 26.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 8)) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 8)) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (3.8.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (7.1.2)\n",
            "Building wheels for collected packages: umap-learn, pynndescent, sacremoses\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=16ce204c72ee1b195aa46c8d95a69cb65c526ee93d63dbc6feb5cf60db9eba4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=bd9d61c3cd6ca00de407a8399731e6f084005298af9e40a73fbcc9efe6079e1e\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f1/56/f80d72741e400345b5a5b50ec3d929aca581bf45e0225d5c50\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=53a64c7a90ef5923ee36341c716978b2a275b38814ab4cb75d32b5e67f01caa5\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built umap-learn pynndescent sacremoses\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, pynndescent, huggingface-hub, umap-learn, transformers, plotly, kaleido\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 5.5.0\n",
            "    Uninstalling plotly-5.5.0:\n",
            "      Successfully uninstalled plotly-5.5.0\n",
            "Successfully installed huggingface-hub-0.5.1 kaleido-0.2.1 plotly-5.7.0 pynndescent-0.5.6 pyyaml-5.4.1 sacremoses-0.0.53 tokenizers-0.12.1 transformers-4.18.0 umap-learn-0.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gebrauchte Paketen importieren**"
      ],
      "metadata": {
        "id": "k7xiPgja8eZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import umap.umap_ as umap\n",
        "import time\n",
        "import plotly.express as px\n",
        "from sklearn import cluster\n",
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "uV7KZhGq1P7g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "719474b7-192e-4ab2-bc41-e58c6722d36b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
            "  defaults = yaml.load(f)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vorverarbeitung und BOW-Repräsentationen für Textdaten durchführen**\n",
        "1. Vocabular erstellen\n",
        "2. BOW-Repräsentationen für allen Teildatensätzen"
      ],
      "metadata": {
        "id": "QzWqQhPQdJWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.preprare_dataset import TextDataLoader"
      ],
      "metadata": {
        "id": "1OCULr82pfgk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init TextDataLoader für die Datenquelle 20 News Groups\n",
        "# Daten abrufen vom Sklearn, tokenisieren und besondere Charaktern entfernen\n",
        "textsloader = TextDataLoader(source=\"20newsgroups\", train_size=None, test_size=None)\n",
        "textsloader.load_tokenize_texts(\"20newsgroups\")\n",
        "# Beispiel von Textdaten\n",
        "textsloader.show_example_raw_texts(n_docs=2)"
      ],
      "metadata": {
        "id": "cy0PpjxEpbrR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b80a0c06-caaf-42ba-c14e-1608cf68978e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading texts: ...\n",
            "finished load!\n",
            "check some sample texts of the dataset\n",
            "['From', ':', 'lerxst', '@', 'wam', '.', 'umd', '.', 'edu', '(', \"where's\", 'my', 'thing', ')', 'Subject', ':', 'WHAT', 'car', 'is', 'this', '!', '?', 'Nntp', 'Posting', 'Host', ':', 'rac3', '.', 'wam', '.', 'umd', '.', 'edu', 'Organization', ':', 'University', 'of', 'Maryland', ',', 'College', 'Park', 'Lines', ':', '15', 'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.', 'It', 'was', 'a', '2', 'door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s', '/', 'early', '70s', '.', 'It', 'was', 'called', 'a', 'Bricklin', '.', 'The', 'doors', 'were', 'really', 'small', '.', 'In', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'This', 'is', 'all', 'I', 'know', '.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e', 'mail', '.', 'Thanks', ',', 'IL', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst']\n",
            "====================================================================================================\n",
            "['From', ':', 'guykuo', '@', 'carson', '.', 'u', '.', 'washington', '.', 'edu', '(', 'Guy', 'Kuo', ')', 'Subject', ':', 'SI', 'Clock', 'Poll', 'Final', 'Call', 'Summary', ':', 'Final', 'call', 'for', 'SI', 'clock', 'reports', 'Keywords', ':', 'SI', ',', 'acceleration', ',', 'clock', ',', 'upgrade', 'Article', 'I', '.', 'D', '.', ':', 'shelley', '.', '1qvfo9INNc3s', 'Organization', ':', 'University', 'of', 'Washington', 'Lines', ':', '11', 'NNTP', 'Posting', 'Host', ':', 'carson', '.', 'u', '.', 'washington', '.', 'edu', 'A', 'fair', 'number', 'of', 'brave', 'souls', 'who', 'upgraded', 'their', 'SI', 'clock', 'oscillator', 'have', 'shared', 'their', 'experiences', 'for', 'this', 'poll', '.', 'Please', 'send', 'a', 'brief', 'message', 'detailing', 'your', 'experiences', 'with', 'the', 'procedure', '.', 'Top', 'speed', 'attained', ',', 'CPU', 'rated', 'speed', ',', 'add', 'on', 'cards', 'and', 'adapters', ',', 'heat', 'sinks', ',', 'hour', 'of', 'usage', 'per', 'day', ',', 'floppy', 'disk', 'functionality', 'with', '800', 'and', '1', '.', '4', 'm', 'floppies', 'are', 'especially', 'requested', '.', 'I', 'will', 'be', 'summarizing', 'in', 'the', 'next', 'two', 'days', ',', 'so', 'please', 'add', 'to', 'the', 'network', 'knowledge', 'base', 'if', 'you', 'have', 'done', 'the', 'clock', 'upgrade', 'and', \"haven't\", 'answered', 'this', 'poll', '.', 'Thanks', '.', 'Guy', 'Kuo', '<', 'guykuo', '@', 'u', '.', 'washington', '.', 'edu', '>']\n",
            "====================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vorverarbeitung von Daten mit folgenden Schritten:\n",
        "textsloader.preprocess_texts(length_one_remove=True, punctuation_lower = True, stopwords_filter = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5odpQDJ7qPTt",
        "outputId": "cefe3c85-c925-435a-e5ee-23b860e5608b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start: preprocessing: ...\n",
            "finised: preprocessing!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Daten zerlegen für Train, Test und Validation. Erstellen Vocabular aus dem Trainset\n",
        "textsloader.split_and_create_voca_from_trainset(max_df=0.7, min_df=10, stopwords_remove_from_voca=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRRCNPa9qXfq",
        "outputId": "c1e73748-0f6f-4d7b-e528-d8fd7c4e6a93"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start creating vocabulary ...\n",
            "length of the vocabulary: 348\n",
            "sample ten words of the vocabulary: ['engineering', 'computer', 'dept', 'left', 'center', 'buy', 'word', 'steve', 'major', 'bit']\n",
            "length word2id list: 348\n",
            "length id2word list: 348\n",
            "finished: creating vocabulary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Erstellen BOW-Repräsentation für ETM Modell\n",
        "for_lda_model = False \n",
        "word2id, id2word, train_set, test_set, val_set = textsloader.create_bow_and_savebow_for_each_set(for_lda_model=for_lda_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etzyjh_nqi19",
        "outputId": "d50f8852-469b-49ee-958a-d0570e38dc9b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length train-documents-indices : 4561\n",
            "length of the vocabulary: 348\n",
            "\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 149\n",
            "min doc-id: 0\n",
            "all docs: 4561\n",
            "all words: 4561\n",
            "docidx unique 150\n",
            "words unique: 348\n",
            "ndocs: 150\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 49\n",
            "min doc-id: 0\n",
            "all docs: 1555\n",
            "all words: 1555\n",
            "docidx unique 50\n",
            "words unique: 326\n",
            "ndocs: 50\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "max word-id: 346\n",
            "min word-id: 1\n",
            "max doc-id: 24\n",
            "min doc-id: 0\n",
            "all docs: 797\n",
            "all words: 797\n",
            "docidx unique 25\n",
            "words unique: 263\n",
            "ndocs: 25\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 3, 4, 5, 7, 9, 10, 11, 12]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 24\n",
            "min doc-id: 0\n",
            "all docs: 758\n",
            "all words: 758\n",
            "docidx unique 25\n",
            "words unique: 263\n",
            "ndocs: 25\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 99\n",
            "min doc-id: 0\n",
            "all docs: 2998\n",
            "all words: 2998\n",
            "docidx unique 100\n",
            "words unique: 346\n",
            "ndocs: 100\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vocabular und IDs anzeigen als Beispiel**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r-DXUMguC8zM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show for samples: 100 word2id and id2 word\n",
        "word2id_df_100 = pd.DataFrame()\n",
        "word2id_df_100['word'] = list(word2id.keys())[:100]\n",
        "word2id_df_100['id'] = list(word2id.values())[:100]\n",
        "word2id_df_100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "6RBJYhLHCfwy",
        "outputId": "670cf5a1-bac4-4614-8575-c8d01da18b25"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           word  id\n",
              "0   engineering   0\n",
              "1      computer   1\n",
              "2          dept   2\n",
              "3          left   3\n",
              "4        center   4\n",
              "..          ...  ..\n",
              "95         read  95\n",
              "96          car  96\n",
              "97         hold  97\n",
              "98     problems  98\n",
              "99          man  99\n",
              "\n",
              "[100 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3683626a-15de-4eb3-8cf2-d7f3aa12a5e4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>engineering</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>computer</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>dept</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>left</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>center</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>read</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>car</td>\n",
              "      <td>96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>hold</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>problems</td>\n",
              "      <td>98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>man</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3683626a-15de-4eb3-8cf2-d7f3aa12a5e4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3683626a-15de-4eb3-8cf2-d7f3aa12a5e4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3683626a-15de-4eb3-8cf2-d7f3aa12a5e4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Die Größe von Datensätzen kontrollieren**"
      ],
      "metadata": {
        "id": "tupeI6Pw85_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Kontrollieren die Größen von verschiedenen Datensätzen\n",
        "print(f'Size of the vocabulary after prprocessing ist: {len(textsloader.vocabulary)}')\n",
        "print(f'Size of train set: {len(train_set[\"tokens\"])}')\n",
        "print(f'Size of val set: {len(val_set[\"tokens\"])}')\n",
        "print(f'Size of test set: {len(test_set[\"test\"][\"tokens\"])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1d-5ji3qwE8",
        "outputId": "f2e503b3-6031-4701-edd7-d6757f302421"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the vocabulary after prprocessing ist: 348\n",
            "Size of train set: 150\n",
            "Size of val set: 100\n",
            "Size of test set: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dokumenten wiederstellen für Word2Vec Embedding**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "bxQL5jQtDb1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# re-erstellen von Dokumenten nach der Vorverarbeitungen. Die Dokumenten sind in Wörtern und werden für Word-Embedding Training benutzt\n",
        "docs_tr, docs_t, docs_v = textsloader.get_docs_in_words_for_each_set()\n",
        "train_docs_df = pd.DataFrame()\n",
        "train_docs_df['text-after-preprocessing'] = [' '.join(doc) for doc in docs_tr[:100]]\n",
        "train_docs_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "PDXEEBHfq3Cy",
        "outputId": "c1d2bbf5-ea02-466e-a735-a49a5b6ff0f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             text-after-preprocessing\n",
              "0   sale corporation distribution nntp posting hos...\n",
              "1   windows line reply windows line pc line window...\n",
              "2   cs reply world message apr originator cs nntp ...\n",
              "3   computer systems division distribution world n...\n",
              "4   required systems laboratory distribution usa a...\n",
              "..                                                ...\n",
              "95  university newsreader tin version mike cc wrot...\n",
              "96  washington guy memory university washington nn...\n",
              "97  day article reply distribution world article c...\n",
              "98  state software nntp posting host state state u...\n",
              "99  washington question university washington nntp...\n",
              "\n",
              "[100 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ab227664-5ff2-40b1-a0ea-4494c34b1bea\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text-after-preprocessing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sale corporation distribution nntp posting hos...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>windows line reply windows line pc line window...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cs reply world message apr originator cs nntp ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>computer systems division distribution world n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>required systems laboratory distribution usa a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>university newsreader tin version mike cc wrot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>washington guy memory university washington nn...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>day article reply distribution world article c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>state software nntp posting host state state u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>washington question university washington nntp...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ab227664-5ff2-40b1-a0ea-4494c34b1bea')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ab227664-5ff2-40b1-a0ea-4494c34b1bea button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ab227664-5ff2-40b1-a0ea-4494c34b1bea');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word-Embedding trainieren mit dem Traindatensatz**"
      ],
      "metadata": {
        "id": "Ds_KuUTQrK5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.embedding import WordEmbeddingCreator\n",
        "save_path = Path.joinpath(Path.cwd(), \"vocab_embedding.txt\")\n",
        "wb_creator = WordEmbeddingCreator(model_name=\"cbow\", documents = docs_tr, save_path= save_path)\n",
        "wb_creator.train(min_count=0, embedding_size= 10)\n",
        "vocab = list(word2id.keys())\n",
        "wb_creator.create_and_save_vocab_embedding(vocab, save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBexKIVf8Qs5",
        "outputId": "c6697c8d-55cc-4ef2-e53d-6d4a4a5dcbad"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word-embedding train begins\n",
            "word-embedding train finished\n",
            "length of vocabulary from word-embedding model 348\n",
            "length of the vocabulary of prepraring-dataset-vocabulary: 348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 348/348 [00:00<00:00, 31351.87it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "v = list(wb_creator.model.wv.vocab)[0]\n",
        "vec = list(wb_creator.model.wv.__getitem__(v))\n",
        "print(f'word-embedding of the word-- {v}: ')\n",
        "print(f'vector: {vec}')\n",
        "print(f'dim of vector: {len(vec)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f23xipx7MSV4",
        "outputId": "7cdd37d7-2f1b-4249-d0f5-279f5ec0f268"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word-embedding of the word-- sale: \n",
            "vector: [-0.049709663, -0.041012865, 0.052273255, -0.013403522, 0.024686739, -0.0043096845, -0.0047836527, -0.03332941, 0.022274528, -0.041171346]\n",
            "dim of vector: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word-Embeddings visualieren als Beispiel**\n"
      ],
      "metadata": {
        "id": "l53_jkUS-hl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read word-embedding files\n",
        "with open(save_path) as f:\n",
        "  lines = f.readlines()\n",
        "embedding_data = []\n",
        "words_data = []\n",
        "for t in lines:\n",
        "  w = t.split(\"\\t\")[0]\n",
        "  v = [float(e) for e in t.split(\"\\t\")[1].split(\" \")]\n",
        "  words_data.append(w)\n",
        "  embedding_data.append(v)"
      ],
      "metadata": {
        "id": "o96LsIWkNrZS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clustering words with KMeans and Words-Vectors\n",
        "kmeans = cluster.KMeans(n_clusters=10)\n",
        "kmeans.fit(embedding_data)\n",
        " \n",
        "labels = kmeans.labels_\n",
        "centroids = kmeans.cluster_centers_\n",
        " \n",
        "print (\"Cluster id labels for inputted data\")\n",
        "print (labels)\n",
        "print (\"Centroids data\")\n",
        "print (centroids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99hYOKPwB5aw",
        "outputId": "afb70b52-2d8c-4cb8-b2a0-cead71d3ee74"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster id labels for inputted data\n",
            "[3 8 2 9 7 5 1 3 6 8 1 9 1 1 4 8 2 9 4 0 8 3 9 4 9 7 7 2 3 2 5 5 6 6 1 1 2\n",
            " 4 4 1 9 4 5 4 9 4 6 8 5 2 9 4 6 6 1 8 8 4 1 8 8 3 9 1 5 2 8 0 4 5 9 3 6 9\n",
            " 4 0 6 1 0 1 3 6 1 3 7 4 8 4 8 4 8 0 4 6 8 5 2 2 8 7 4 4 3 9 2 3 3 9 9 7 8\n",
            " 4 4 9 9 1 4 0 9 4 6 5 9 3 4 1 8 9 6 0 4 8 2 2 6 6 3 6 6 8 1 6 6 4 9 6 4 0\n",
            " 9 1 1 8 5 2 5 4 4 0 8 0 0 4 0 4 8 6 9 2 4 1 0 4 9 7 9 1 0 2 7 6 8 8 1 1 1\n",
            " 1 8 2 9 4 2 4 3 1 8 4 9 4 9 6 6 1 3 1 3 3 0 8 8 5 8 4 3 6 0 5 8 8 8 0 9 6\n",
            " 8 6 9 0 3 9 5 9 6 6 8 5 8 6 8 3 4 4 4 4 8 3 6 1 1 1 4 9 8 3 8 6 8 1 4 6 7\n",
            " 0 8 0 4 5 4 9 8 8 0 0 1 0 4 8 6 3 2 6 4 1 5 0 3 2 6 6 3 4 8 0 6 2 3 4 1 8\n",
            " 8 0 9 1 4 9 6 0 2 6 1 9 4 2 9 4 6 4 4 3 9 1 0 0 6 3 8 5 3 6 9 3 9 6 6 1 6\n",
            " 1 1 6 2 6 3 2 0 5 0 6 3 3 1 0]\n",
            "Centroids data\n",
            "[[-1.23607045e-01 -1.71837481e-02 -1.34354138e-02  9.74559663e-03\n",
            "   3.79654279e-02  4.84658094e-04  4.33758833e-02  9.72691284e-03\n",
            "  -3.28201787e-03 -1.05894539e-02]\n",
            " [-2.18011953e-01 -3.50844598e-02  5.59736544e-03 -1.63410166e-03\n",
            "   4.60094457e-02 -2.95312389e-02  4.83945388e-02  4.79998865e-02\n",
            "   2.85899345e-02 -5.38221452e-02]\n",
            " [-3.13623089e-01 -1.70777468e-02  3.20399825e-02 -3.65884565e-02\n",
            "   4.46000313e-02 -2.36480067e-02  8.09609028e-02  3.53813784e-02\n",
            "   2.14788878e-02 -8.13861077e-02]\n",
            " [-8.26992826e-02 -1.42064882e-03  1.57058167e-02 -1.85593765e-02\n",
            "   1.57009335e-02  2.14280555e-03  2.34583491e-02  3.27474636e-03\n",
            "   2.37241355e-02 -3.58163056e-02]\n",
            " [-2.71051398e-01 -1.43551417e-02  3.74966144e-02 -1.36975299e-02\n",
            "   8.06427493e-02 -1.61749326e-02  5.01989350e-02  4.79279063e-02\n",
            "   2.77539931e-02 -6.65573504e-02]\n",
            " [-3.91950959e-01 -1.95045976e-02  3.45282170e-02 -1.08458331e-02\n",
            "   8.62692603e-02 -4.71084639e-02  5.78735704e-02  7.03978959e-02\n",
            "   5.43294934e-02 -1.10523920e-01]\n",
            " [-1.49522978e-01 -2.54718930e-02  3.56677159e-02 -8.57428685e-03\n",
            "   4.64334111e-02 -1.20981901e-02  3.11493057e-02  3.69088233e-02\n",
            "   2.97098930e-02 -3.12762447e-02]\n",
            " [-5.03416422e-01 -3.83200072e-02  5.18183837e-02 -2.45662019e-02\n",
            "   1.29103879e-01 -2.32145840e-02  9.53648901e-02  8.95752933e-02\n",
            "   5.09860976e-02 -1.21245972e-01]\n",
            " [-1.92287732e-01 -7.03275208e-04  1.04239131e-02 -8.43356971e-03\n",
            "   3.02106001e-02  1.67724896e-03  2.76380886e-02  1.90902480e-02\n",
            "   4.17138460e-03 -5.54319371e-02]\n",
            " [-3.42232475e-01 -4.57556714e-02  2.10703065e-02 -7.53601270e-03\n",
            "   9.08410674e-02 -1.64564661e-02  7.84652259e-02  5.43813386e-02\n",
            "   3.01303715e-02 -8.59488801e-02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dimension reduction with umap\n",
        "start = time.time()\n",
        "reducer = umap.UMAP(random_state=42,n_components=3)\n",
        "embedding = reducer.fit_transform(embedding_data)\n",
        "print('Duration: {} seconds'.format(time.time() - start))\n",
        "\n",
        "# show samples after dim-reduction in dataframe\n",
        "wb = pd.DataFrame(embedding, columns=['x', 'y', 'z'])\n",
        "wb['word'] = words_data\n",
        "wb['cluster'] = ['cluster ' + str(c) for c in labels]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAMgZ9aIE9A6",
        "outputId": "e92dd14c-5b69-4003-8ccc-759465a3b321"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  warnings.warn(problem)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 15.641063451766968 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization\n",
        "fig = px.scatter_3d(wb, \n",
        "                    text = wb['word'],\n",
        "                    x='x', y='y', z='z',\n",
        "                    color = wb['cluster'],\n",
        "                    title =\"word-embedding-samples\")\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "spomMOt_yy0W",
        "outputId": "b951056b-bc78-4c4c-856b-d190feaaea90"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.11.1.min.js\"></script>                <div id=\"e014cf0c-ec27-4fd2-8d8f-efbdf4d18d8a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e014cf0c-ec27-4fd2-8d8f-efbdf4d18d8a\")) {                    Plotly.newPlot(                        \"e014cf0c-ec27-4fd2-8d8f-efbdf4d18d8a\",                        [{\"hovertemplate\":\"cluster=cluster 3<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 3\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 3\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"sale\",\"type\",\"school\",\"live\",\"company\",\"job\",\"expressed\",\"write\",\"stop\",\"top\",\"interest\",\"advance\",\"technical\",\"dept\",\"hell\",\"easy\",\"jim\",\"makes\",\"hear\",\"institute\",\"interesting\",\"written\",\"home\",\"result\",\"including\",\"note\",\"light\",\"experience\",\"buy\",\"related\",\"parts\",\"canada\",\"lost\"],\"x\":[8.960073471069336,8.283648490905762,8.433103561401367,8.131542205810547,9.017013549804688,8.941232681274414,8.822431564331055,8.52795696258545,8.53735637664795,9.055594444274902,8.259549140930176,8.451436042785645,8.510674476623535,8.564888000488281,8.495882034301758,9.226860046386719,8.981219291687012,8.919591903686523,9.149810791015625,9.188602447509766,8.773153305053711,8.387031555175781,9.052170753479004,7.833678245544434,8.790339469909668,8.533628463745117,7.15306282043457,9.043363571166992,8.662872314453125,9.08874797821045,8.57140064239502,9.112370491027832,9.092985153198242],\"y\":[7.652756690979004,6.771430492401123,7.80853271484375,6.758764266967773,7.239640712738037,7.611188888549805,7.1218461990356445,7.17986536026001,7.608226776123047,7.409726142883301,7.746417045593262,7.722565650939941,7.544928073883057,7.599826812744141,7.683742523193359,7.281795978546143,7.491862773895264,7.530737400054932,7.271992206573486,7.3990631103515625,7.081132411956787,7.932258605957031,7.325193405151367,8.082890510559082,7.86494255065918,8.06585693359375,7.396364212036133,7.4507646560668945,7.058493137359619,7.387584686279297,7.66881799697876,7.325569152832031,7.451822280883789],\"z\":[3.3355939388275146,3.485081911087036,3.813403844833374,3.8511250019073486,3.9350056648254395,2.8718111515045166,3.7433698177337646,3.5975027084350586,2.8139002323150635,3.7631213665008545,3.481412410736084,3.005932569503784,3.9460384845733643,3.7909650802612305,3.6957345008850098,3.1548006534576416,3.0235869884490967,3.9154889583587646,3.30769681930542,3.379410743713379,3.8634462356567383,2.9688172340393066,3.7314560413360596,3.3261897563934326,3.725041389465332,3.1423556804656982,3.699146032333374,3.177215814590454,3.9044606685638428,3.1560511589050293,2.9170119762420654,3.2173752784729004,3.092900514602661],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 8<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 8\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 8\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"corporation\",\"contact\",\"division\",\"originator\",\"address\",\"tom\",\"lot\",\"total\",\"cost\",\"money\",\"couple\",\"similar\",\"free\",\"price\",\"city\",\"ms\",\"interested\",\"image\",\"form\",\"faster\",\"mac\",\"simple\",\"ago\",\"means\",\"thinking\",\"project\",\"folks\",\"months\",\"center\",\"steve\",\"david\",\"considered\",\"calls\",\"clear\",\"error\",\"possibly\",\"mentioned\",\"rest\",\"common\",\"sort\",\"numbers\",\"higher\",\"speed\",\"change\",\"late\",\"gov\",\"engineering\",\"deleted\"],\"x\":[7.106508255004883,5.733330726623535,6.326947212219238,6.871467113494873,5.8288140296936035,6.068179607391357,7.338950157165527,6.496075630187988,6.0369672775268555,6.05504035949707,5.289447784423828,6.7002668380737305,7.633796691894531,6.872355937957764,6.600481986999512,6.206546783447266,7.599907875061035,5.886387825012207,6.685390949249268,6.302010536193848,6.378542900085449,7.823028087615967,5.784162521362305,5.4970703125,6.492944717407227,5.320923328399658,6.027567386627197,5.295070171356201,5.907179355621338,6.623636722564697,5.785360813140869,6.215793609619141,7.3343682289123535,6.278351306915283,5.862343788146973,6.454617500305176,6.253280162811279,5.757675647735596,6.91251802444458,5.347307205200195,5.8277692794799805,5.658799648284912,7.021667003631592,6.218050956726074,6.060248374938965,5.438494682312012,6.3270416259765625,5.4803571701049805],\"y\":[6.801914215087891,6.917482852935791,6.921700477600098,7.318212985992432,6.677303791046143,6.748829364776611,7.733484745025635,8.000747680664062,7.877983570098877,7.285975456237793,7.256711483001709,7.362236499786377,7.6938676834106445,7.666012287139893,8.084456443786621,7.588891506195068,7.8695068359375,6.646066665649414,6.705550193786621,7.024430751800537,6.849897384643555,6.774746894836426,6.719697952270508,7.790050029754639,7.104574203491211,7.099928855895996,6.711243629455566,7.582862854003906,6.678061485290527,7.611293792724609,6.845813274383545,6.880672454833984,7.929896354675293,7.359635829925537,6.816082954406738,7.209977149963379,6.88888692855835,6.7197465896606445,7.761148452758789,6.640564918518066,6.699032306671143,6.609432220458984,7.786922931671143,6.543239593505859,7.1285858154296875,6.910776138305664,6.861546516418457,7.422732353210449],\"z\":[3.338935375213623,4.311819076538086,4.327210426330566,4.083707809448242,3.7602641582489014,4.070031642913818,2.9852330684661865,2.7707464694976807,3.868579149246216,4.259243488311768,4.430560111999512,4.3889288902282715,2.6915271282196045,4.079242706298828,2.8663485050201416,4.140708923339844,3.2311947345733643,3.9215970039367676,3.6788082122802734,3.66861891746521,4.114614009857178,3.8595824241638184,3.679588556289673,3.3256659507751465,3.282904624938965,4.428834915161133,4.251125335693359,3.748427629470825,4.021708011627197,3.989882469177246,3.941535711288452,3.7185306549072266,4.104499816894531,4.320184230804443,4.141008377075195,4.191198825836182,3.6231818199157715,3.5508179664611816,2.891371726989746,4.0650835037231445,3.9583044052124023,4.112018585205078,2.6663308143615723,3.8006086349487305,4.064653396606445,3.9708411693573,4.145968914031982,4.703878402709961],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 2<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 2\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 2\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"distribution\",\"cs\",\"great\",\"guy\",\"usa\",\"information\",\"today\",\"ca\",\"john\",\"current\",\"work\",\"technology\",\"read\",\"simply\",\"matter\",\"feel\",\"understand\",\"access\",\"key\",\"summary\",\"drive\",\"uk\",\"front\",\"game\"],\"x\":[2.069077730178833,2.3323752880096436,2.750789165496826,2.8028714656829834,2.8942863941192627,2.496119976043701,2.7253637313842773,2.5600404739379883,2.8049519062042236,3.383845567703247,2.4562666416168213,2.4374570846557617,2.9307973384857178,2.7934772968292236,2.693113088607788,3.527791738510132,2.4111695289611816,2.903184652328491,3.132185220718384,4.193447589874268,2.7663331031799316,1.9912774562835693,2.8209002017974854,2.6413943767547607],\"y\":[7.319705963134766,8.029321670532227,7.2193403244018555,8.444989204406738,8.378588676452637,7.759488582611084,7.195711135864258,7.701868057250977,7.550588130950928,7.675195217132568,8.288448333740234,7.366787433624268,7.425039291381836,7.276437759399414,7.423455238342285,7.434676170349121,8.487070083618164,7.407963275909424,8.4760103225708,7.744884014129639,7.997292995452881,7.5587158203125,7.557924747467041,8.26793098449707],\"z\":[5.260048866271973,5.186073303222656,4.904646396636963,5.530615329742432,5.474119186401367,5.313426494598389,4.994904518127441,4.775693416595459,5.0140814781188965,5.26654577255249,5.876198768615723,5.226528644561768,4.885770320892334,4.975276947021484,5.642975807189941,5.149646282196045,5.926563739776611,4.970662593841553,5.408226013183594,5.260319232940674,5.656223297119141,5.193967819213867,4.833573818206787,4.642902851104736],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 9<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 9\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 9\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"nntp\",\"windows\",\"world\",\"computer\",\"university\",\"version\",\"low\",\"thing\",\"word\",\"call\",\"bike\",\"heard\",\"understanding\",\"major\",\"cc\",\"sun\",\"file\",\"run\",\"au\",\"made\",\"bit\",\"reason\",\"book\",\"question\",\"case\",\"hold\",\"things\",\"called\",\"post\",\"left\",\"end\",\"hand\",\"car\",\"population\",\"times\",\"ibm\",\"control\",\"ac\",\"gun\",\"children\"],\"x\":[2.638561248779297,1.6558268070220947,1.3863617181777954,2.448117971420288,1.416939616203308,2.4552555084228516,2.2691690921783447,1.564228892326355,1.8215810060501099,1.7209632396697998,1.6930397748947144,2.401064157485962,2.2616875171661377,2.074511766433716,2.1490418910980225,1.6978797912597656,2.4098730087280273,2.215426206588745,2.19701886177063,2.4763312339782715,2.035679817199707,2.3417866230010986,1.7146209478378296,1.8837090730667114,2.3779900074005127,2.014960527420044,1.5074238777160645,1.9112277030944824,2.1419098377227783,1.7177754640579224,2.1545004844665527,2.2046847343444824,2.6699042320251465,2.1427435874938965,2.297621011734009,2.085653066635132,2.7377326488494873,2.007725715637207,1.6467030048370361,2.207350969314575],\"y\":[8.725336074829102,8.741257667541504,7.951070785522461,8.773297309875488,7.7141876220703125,8.323410987854004,7.778314113616943,8.634719848632812,7.649596214294434,8.890815734863281,7.822402000427246,8.855131149291992,8.472369194030762,8.528607368469238,8.88495922088623,8.591988563537598,8.758919715881348,7.516191482543945,7.874875068664551,8.545503616333008,8.833864212036133,7.999203205108643,8.711228370666504,8.787999153137207,7.789922714233398,8.764806747436523,8.559896469116211,7.719277381896973,8.792549133300781,8.662464141845703,8.85738468170166,8.423050880432129,8.830370903015137,7.646663188934326,8.739587783813477,8.766550064086914,8.761208534240723,8.36146068572998,8.733442306518555,8.808159828186035],\"z\":[5.653492450714111,5.4340362548828125,5.7684221267700195,5.775890350341797,5.275936603546143,4.450789451599121,4.564062118530273,5.382505416870117,5.44541072845459,5.435397148132324,5.0849928855896,5.399468898773193,4.626008033752441,4.69856071472168,5.386744022369385,5.016245365142822,5.891799449920654,4.940940856933594,4.631804466247559,5.635881423950195,5.35336446762085,4.9455461502075195,5.7487874031066895,5.804144859313965,4.607542514801025,5.2306342124938965,5.3325676918029785,5.002889156341553,5.854950428009033,5.231075763702393,5.298624038696289,4.61857271194458,5.017464637756348,5.109623908996582,4.93214750289917,5.85125207901001,5.176578521728516,4.859951972961426,5.895519733428955,5.826934814453125],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 7<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 7\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 7\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"posting\",\"article\",\"writes\",\"good\",\"time\",\"years\",\"fact\",\"people\",\"point\"],\"x\":[0.46280014514923096,0.6715797185897827,0.3868351876735687,0.38215067982673645,0.336544394493103,0.42715635895729065,0.6631972193717957,0.4719913899898529,0.535237729549408],\"y\":[8.384430885314941,8.753344535827637,8.622811317443848,8.682991027832031,8.529581069946289,8.510090827941895,8.586328506469727,8.629045486450195,8.631957054138184],\"z\":[5.675877571105957,6.190273761749268,6.009129524230957,6.072653770446777,5.914979457855225,5.873138427734375,5.948767185211182,6.011361598968506,6.037980079650879],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 5<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 5\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 5\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"host\",\"talking\",\"real\",\"wrote\",\"number\",\"state\",\"true\",\"bad\",\"back\",\"problem\",\"system\",\"make\",\"place\",\"power\",\"memory\",\"group\",\"data\",\"turn\",\"source\"],\"x\":[0.835263729095459,1.3994998931884766,1.0254215002059937,0.9369310140609741,0.9496452212333679,2.226849317550659,1.9768739938735962,0.8988037109375,0.6202953457832336,1.272050142288208,0.8952036499977112,1.1534020900726318,1.9568514823913574,1.1558018922805786,0.5604252219200134,1.385428547859192,1.2383992671966553,1.2294025421142578,1.0628654956817627],\"y\":[8.449151039123535,8.517314910888672,8.56583309173584,8.703062057495117,8.198746681213379,7.77083158493042,7.961693286895752,8.416783332824707,8.367452621459961,8.437024116516113,8.50715446472168,8.696322441101074,8.013750076293945,8.222108840942383,8.497795104980469,8.026904106140137,8.233695030212402,7.902928352355957,8.383952140808105],\"z\":[5.530291557312012,5.291682243347168,5.965036869049072,5.976352691650391,5.618940830230713,5.870926380157471,5.913384914398193,5.542099475860596,5.663140773773193,5.262979984283447,5.908977031707764,5.602840423583984,5.835432052612305,5.807977676391602,5.8183274269104,5.846240997314453,5.8207573890686035,5.511210918426514,5.469760417938232],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 1<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 1\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 1\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"box\",\"mail\",\"line\",\"reply\",\"phone\",\"fax\",\"tin\",\"laboratory\",\"give\",\"takes\",\"network\",\"opinions\",\"date\",\"problems\",\"de\",\"important\",\"ram\",\"general\",\"religion\",\"history\",\"big\",\"washington\",\"coming\",\"day\",\"reading\",\"net\",\"view\",\"side\",\"kind\",\"situation\",\"law\",\"org\",\"space\",\"white\",\"worse\",\"rate\",\"order\",\"east\",\"single\",\"toronto\",\"rights\"],\"x\":[6.76638650894165,4.914737224578857,5.4484453201293945,5.143913745880127,6.024281024932861,5.749277591705322,5.231668472290039,6.037342548370361,4.433897018432617,5.572600841522217,4.9614667892456055,4.889575481414795,6.055469989776611,5.309648036956787,5.682353973388672,4.982149600982666,5.13173770904541,4.502649784088135,6.136003017425537,5.983807563781738,5.716109275817871,5.454205513000488,5.843264102935791,5.4836530685424805,5.569345951080322,5.800144672393799,4.174782752990723,5.497567653656006,5.3694748878479,5.934706687927246,4.397666931152344,6.08867883682251,4.1063032150268555,5.705502033233643,5.5173139572143555,6.511179447174072,6.101154327392578,5.71010684967041,6.27553129196167,4.471245765686035,6.350146770477295],\"y\":[7.790307521820068,8.471842765808105,8.718355178833008,8.395480155944824,8.774104118347168,8.229784965515137,6.588710308074951,8.451162338256836,8.451797485351562,8.14869213104248,8.304415702819824,8.484045028686523,9.003789901733398,7.8916778564453125,8.910346984863281,8.591496467590332,8.521291732788086,8.516545295715332,9.00645923614502,8.181077003479004,8.965482711791992,8.495038032531738,8.516379356384277,7.562061309814453,8.751626968383789,8.625081062316895,7.6239118576049805,8.71994400024414,8.396559715270996,8.960284233093262,8.141339302062988,7.583120346069336,7.907087326049805,8.836963653564453,8.385758399963379,7.952364921569824,8.17715835571289,8.454090118408203,8.083351135253906,7.993429183959961,8.212935447692871],\"z\":[3.904812812805176,4.771729946136475,4.570768356323242,4.0234856605529785,4.237827301025391,3.9870686531066895,4.067208290100098,4.1858320236206055,4.547015190124512,3.9742393493652344,3.971419334411621,4.265737533569336,3.8638041019439697,3.5363693237304688,3.805123805999756,4.575308322906494,4.724320888519287,4.361767768859863,3.719705581665039,3.9785614013671875,4.0573906898498535,4.013880729675293,4.022383689880371,4.858152389526367,4.4405293464660645,4.050388336181641,5.222370624542236,4.469606399536133,3.996938705444336,3.9139742851257324,4.503899574279785,4.839786529541016,3.8867735862731934,3.9634177684783936,4.033116340637207,2.981964111328125,3.0897841453552246,4.436746120452881,2.8984246253967285,4.207066059112549,3.8817267417907715],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 6<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 6\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 6\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"stuff\",\"agree\",\"internet\",\"nice\",\"sense\",\"required\",\"dod\",\"copy\",\"bill\",\"figure\",\"disk\",\"friend\",\"machine\",\"hardware\",\"results\",\"final\",\"due\",\"college\",\"international\",\"bible\",\"told\",\"claim\",\"sci\",\"week\",\"service\",\"man\",\"gas\",\"asked\",\"set\",\"mind\",\"open\",\"design\",\"worth\",\"hope\",\"high\",\"original\",\"short\",\"assuming\",\"idea\",\"add\",\"mike\",\"response\",\"hp\",\"full\",\"western\",\"james\",\"religious\",\"wanted\"],\"x\":[8.193011283874512,6.528512477874756,8.410823822021484,8.040411949157715,8.006732940673828,7.4037017822265625,7.020139694213867,7.978185653686523,6.680835723876953,7.304194927215576,6.921568870544434,8.092718124389648,8.113390922546387,7.509997367858887,7.563762187957764,7.654060363769531,7.851905345916748,7.897797107696533,7.881016731262207,6.886412620544434,7.037830829620361,7.740109920501709,6.818482398986816,6.790460109710693,7.445620059967041,7.392611980438232,8.332639694213867,6.279412269592285,8.278642654418945,8.224769592285156,7.4218058586120605,8.169378280639648,6.38239860534668,7.829415798187256,6.36631441116333,7.3614373207092285,6.744848251342773,7.998604774475098,6.24564266204834,8.1280517578125,8.410426139831543,6.080410957336426,7.035505294799805,6.782625675201416,7.894776821136475,6.736385345458984,8.491536140441895,8.371146202087402],\"y\":[7.8715972900390625,7.692106246948242,7.778420448303223,7.926849365234375,8.078630447387695,8.121617317199707,8.592144012451172,8.011979103088379,8.191859245300293,7.720656394958496,8.57684326171875,6.94762659072876,7.989924430847168,8.332944869995117,8.023179054260254,8.087984085083008,6.849786758422852,7.246448993682861,8.430627822875977,7.38839054107666,7.965054512023926,8.025699615478516,7.732249736785889,7.166987419128418,8.343011856079102,8.460801124572754,8.300016403198242,8.676546096801758,7.6069769859313965,7.931995391845703,8.488513946533203,8.041078567504883,8.734869003295898,8.259055137634277,8.68678092956543,8.266733169555664,8.293216705322266,8.12435245513916,8.741218566894531,7.81362771987915,7.819849014282227,8.889667510986328,8.194234848022461,7.4985785484313965,8.30452823638916,8.152463912963867,8.001404762268066,7.736190319061279],\"z\":[3.5453882217407227,4.614655494689941,3.7502059936523438,3.447988271713257,4.493006706237793,3.937657117843628,3.3740413188934326,4.583970546722412,2.8979501724243164,4.522440433502197,4.4902024269104,4.035338878631592,4.213846683502197,3.694796085357666,4.222670555114746,4.730618953704834,3.9128012657165527,4.029672145843506,4.24266242980957,4.594521522521973,4.025333881378174,4.615760326385498,3.757455587387085,4.427705764770508,3.876145839691162,4.3273115158081055,3.422252893447876,4.428980350494385,4.009181499481201,4.362557411193848,4.099429130554199,3.95524263381958,4.383885383605957,4.452218055725098,4.49193000793457,3.913992166519165,3.526535987854004,4.375247478485107,4.545300006866455,3.264940023422241,4.371108055114746,3.848383903503418,3.151740074157715,4.143752098083496,3.264148235321045,3.7083702087402344,4.039323806762695,4.243302345275879],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 4<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 4\",\"marker\":{\"color\":\"#FF97FF\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 4\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"pc\",\"message\",\"science\",\"systems\",\"newsreader\",\"robert\",\"chip\",\"pretty\",\"person\",\"guess\",\"large\",\"disclaimer\",\"software\",\"program\",\"send\",\"small\",\"effect\",\"year\",\"dos\",\"found\",\"start\",\"hard\",\"keywords\",\"national\",\"email\",\"card\",\"strong\",\"opinion\",\"nasa\",\"god\",\"truth\",\"life\",\"thought\",\"mark\",\"government\",\"code\",\"research\",\"issue\",\"receive\",\"part\",\"news\",\"appreciated\",\"care\",\"put\",\"info\",\"jews\",\"long\",\"days\",\"based\",\"find\",\"remember\",\"public\",\"list\",\"wrong\"],\"x\":[4.584445476531982,3.463019371032715,4.703619956970215,3.6471822261810303,3.210386276245117,3.282503843307495,3.1163434982299805,3.4469234943389893,2.6802611351013184,3.8003902435302734,3.7979862689971924,2.801443576812744,3.9229085445404053,3.7809271812438965,2.931363344192505,3.6982510089874268,4.8990960121154785,3.5774922370910645,3.7297744750976562,2.6878015995025635,4.680267333984375,3.8879852294921875,3.6557068824768066,4.0079450607299805,2.7953460216522217,2.558762311935425,3.3308234214782715,4.2302680015563965,3.153454303741455,3.1061952114105225,3.570774793624878,3.076434373855591,2.464056968688965,2.2352232933044434,3.083735704421997,3.2798643112182617,3.705087900161743,3.0282163619995117,3.836456298828125,5.258347988128662,3.154622793197632,3.1275665760040283,5.394505977630615,4.288740158081055,3.311047077178955,3.0793447494506836,2.980027437210083,2.996088981628418,2.9636034965515137,2.9264378547668457,4.029481410980225,3.018681526184082,3.6748383045196533,4.133002281188965],\"y\":[7.77142858505249,7.61928653717041,7.680912971496582,7.8673295974731445,8.080188751220703,8.282072067260742,7.312149524688721,7.323889255523682,7.446388244628906,7.264782428741455,7.995988368988037,8.215841293334961,8.007438659667969,8.247838020324707,8.263957977294922,8.127817153930664,6.712763786315918,8.373811721801758,7.154144763946533,7.625028610229492,7.507707595825195,7.82968807220459,8.254923820495605,7.807072639465332,7.600786209106445,7.40920352935791,7.951755523681641,7.349096775054932,7.923405170440674,7.303627014160156,7.298492908477783,7.941627025604248,7.807492256164551,7.5416717529296875,8.452973365783691,7.789885520935059,7.098311424255371,7.949682235717773,8.171432495117188,7.524459362030029,7.282886505126953,7.304421424865723,7.578581809997559,7.690469264984131,7.331488132476807,8.077436447143555,8.730378150939941,7.418273448944092,7.386844635009766,7.637625694274902,7.915594100952148,7.387089729309082,7.10603141784668,7.8076910972595215],\"z\":[4.507439613342285,4.618320465087891,5.119904518127441,4.306160926818848,4.236265182495117,4.126021385192871,5.517138957977295,4.083998203277588,5.661149978637695,4.413936614990234,4.319875717163086,4.468011856079102,4.3575239181518555,5.288641929626465,4.293896198272705,3.828014612197876,4.145786762237549,5.38057804107666,4.6958112716674805,5.819345474243164,4.174921989440918,5.256396293640137,4.091493129730225,3.787120819091797,5.850865364074707,5.499156475067139,4.271041393280029,4.581993579864502,4.41005802154541,4.190139293670654,5.160820484161377,4.972619533538818,4.544070243835449,4.855385780334473,5.2912516593933105,5.54695987701416,4.7373046875,5.131096839904785,3.921619176864624,4.9015793800354,5.218114852905273,5.402624130249023,4.75248908996582,5.324320316314697,4.110610008239746,4.312721252441406,4.891136169433594,4.317015647888184,4.440268039703369,4.7749762535095215,3.958794593811035,5.346044540405273,4.696417331695557,3.8465497493743896],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 0<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 0\",\"marker\":{\"color\":\"#FECB52\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 0\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"apr\",\"pay\",\"board\",\"posted\",\"support\",\"display\",\"computing\",\"questions\",\"early\",\"department\",\"christian\",\"love\",\"show\",\"class\",\"mine\",\"answer\",\"texas\",\"wondering\",\"taking\",\"making\",\"application\",\"california\",\"continue\",\"close\",\"radio\",\"miles\",\"sell\",\"fine\",\"references\",\"sound\",\"exists\",\"minutes\"],\"x\":[7.588584899902344,7.458989143371582,7.484053611755371,8.350299835205078,7.68945837020874,7.222399711608887,7.67542839050293,7.870327472686768,7.5239577293396,6.9798264503479,7.698998928070068,8.040983200073242,8.927891731262207,7.9410319328308105,8.078883171081543,7.799415588378906,8.699052810668945,7.327828884124756,8.050592422485352,7.085181713104248,7.52865743637085,7.796570777893066,8.230012893676758,7.114048957824707,7.408339500427246,7.676379680633545,7.857974529266357,7.505979061126709,7.706155776977539,7.856362342834473,7.935287952423096,8.07461929321289],\"y\":[6.747934818267822,7.792304992675781,7.070497035980225,8.036523818969727,7.468217849731445,7.226729393005371,8.044304847717285,6.848142623901367,7.900897026062012,8.012852668762207,7.240532398223877,7.787489891052246,7.23482084274292,6.838194370269775,7.554950714111328,7.431701183319092,7.462970733642578,7.277500152587891,6.915665149688721,7.929774284362793,7.430840015411377,6.810515880584717,7.49514627456665,7.906056880950928,7.090602397918701,7.173439979553223,8.345969200134277,6.948866844177246,6.969569206237793,6.8945207595825195,7.711160659790039,6.854517459869385],\"z\":[3.6744909286499023,3.4574568271636963,3.1399874687194824,2.8708696365356445,3.141324758529663,3.3168435096740723,3.316295862197876,3.6152124404907227,2.5829734802246094,2.6817705631256104,2.8970444202423096,2.61330246925354,2.975895404815674,3.4328110218048096,2.6203508377075195,2.676518678665161,2.8422622680664062,2.9719958305358887,3.824369430541992,2.584918260574341,2.9594361782073975,3.3383238315582275,2.797196626663208,2.5827932357788086,3.2257673740386963,2.931248903274536,3.321251153945923,3.4416582584381104,3.1099376678466797,3.110548257827759,2.633148431777954,3.8688857555389404],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"x\"}},\"yaxis\":{\"title\":{\"text\":\"y\"}},\"zaxis\":{\"title\":{\"text\":\"z\"}}},\"legend\":{\"title\":{\"text\":\"cluster\"},\"tracegroupgap\":0},\"title\":{\"text\":\"word-embedding-samples\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e014cf0c-ec27-4fd2-8d8f-efbdf4d18d8a');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ETM-Model trainieren**"
      ],
      "metadata": {
        "id": "P9jSI12r9zqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using DocSet to use easier the modul DataSet from torch\n",
        "from src.train_etm import DocSet, ETMTrain\n",
        "from src.etm import ETM\n",
        "\n",
        "vocab_size = len(list(word2id.keys()))\n",
        "tr_set = DocSet(\"train\", vocab_size, train_set)\n",
        "print(len(tr_set))\n",
        "print(tr_set.__getitem__(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwiU9zWCIUh_",
        "outputId": "701f7972-5c36-4f17-b26d-b440bc26e5e9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_topics = 5\n",
        "t_hidden_size = 100\n",
        "rho_size = len(embedding_data[0])\n",
        "emb_size = len(embedding_data[0])\n",
        "theta_act = \"relu\"\n",
        "\n",
        "class TrainArguments:\n",
        "      def __init__(self, epochs, batch_size, log_interval):\n",
        "          self.epochs = epochs\n",
        "          self.batch_size = batch_size\n",
        "          self.log_interval = log_interval\n",
        "\n",
        "class OptimizerArguments:\n",
        "      def __init__(self, optimizer_name, lr, wdecay):\n",
        "            self.optimizer = optimizer_name\n",
        "            self.lr = lr\n",
        "            self.wdecay = wdecay\n",
        "            \n",
        "train_args = TrainArguments(epochs=1000, batch_size=6, log_interval=None)\n",
        "optimizer_args = OptimizerArguments(optimizer_name=\"adam\", lr=0.001, wdecay=0.1)\n",
        "\n",
        "print(train_args.epochs)\n",
        "print(optimizer_args.optimizer)\n",
        "\n",
        "training_set = train_set\n",
        "\n",
        "# define the ETM-model with setting-parameters\n",
        "etm_model = ETM(\n",
        "      num_topics, \n",
        "      vocab_size, \n",
        "      t_hidden_size, rho_size, emb_size, theta_act, \n",
        "      embedding_data, enc_drop=0.5)\n",
        "\n",
        "# start training\n",
        "train_class = ETMTrain().train(\n",
        "    etm_model,\n",
        "    vocab_size, \n",
        "    train_args, optimizer_args, training_set, \n",
        "    num_topics, t_hidden_size, rho_size, emb_size, theta_act, embedding_data, 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kbJIWLZpJRnG",
        "outputId": "e0a8f376-fd35-4b27-8177-c2cbd77643ff"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "adam\n",
            "Epoch: 0/1000  -  Loss: 29.656722310384115\n",
            "Epoch: 1/1000  -  Loss: 29.65573181152344\n",
            "Epoch: 2/1000  -  Loss: 29.65271280924479\n",
            "Epoch: 3/1000  -  Loss: 29.651862182617183\n",
            "Epoch: 4/1000  -  Loss: 29.651345520019532\n",
            "Epoch: 5/1000  -  Loss: 29.651809844970703\n",
            "Epoch: 6/1000  -  Loss: 29.650133310953773\n",
            "Epoch: 7/1000  -  Loss: 29.647878417968744\n",
            "Epoch: 8/1000  -  Loss: 29.647778803507485\n",
            "Epoch: 9/1000  -  Loss: 29.64680501302083\n",
            "Epoch: 10/1000  -  Loss: 29.644760437011715\n",
            "Epoch: 11/1000  -  Loss: 29.64467264811198\n",
            "Epoch: 12/1000  -  Loss: 29.644389750162762\n",
            "Epoch: 13/1000  -  Loss: 29.643319295247398\n",
            "Epoch: 14/1000  -  Loss: 29.64268900553385\n",
            "Epoch: 15/1000  -  Loss: 29.64216140747071\n",
            "Epoch: 16/1000  -  Loss: 29.640478566487626\n",
            "Epoch: 17/1000  -  Loss: 29.640883890787762\n",
            "Epoch: 18/1000  -  Loss: 29.63998339335124\n",
            "Epoch: 19/1000  -  Loss: 29.640569966634118\n",
            "Epoch: 20/1000  -  Loss: 29.63940612792969\n",
            "Epoch: 21/1000  -  Loss: 29.639454650878907\n",
            "Epoch: 22/1000  -  Loss: 29.639705251057947\n",
            "Epoch: 23/1000  -  Loss: 29.638581746419266\n",
            "Epoch: 24/1000  -  Loss: 29.638334299723304\n",
            "Epoch: 25/1000  -  Loss: 29.638708140055336\n",
            "Epoch: 26/1000  -  Loss: 29.637427012125656\n",
            "Epoch: 27/1000  -  Loss: 29.63758804321289\n",
            "Epoch: 28/1000  -  Loss: 29.637618509928387\n",
            "Epoch: 29/1000  -  Loss: 29.63663955688477\n",
            "Epoch: 30/1000  -  Loss: 29.63702260335286\n",
            "Epoch: 31/1000  -  Loss: 29.636522827148443\n",
            "Epoch: 32/1000  -  Loss: 29.63642822265625\n",
            "Epoch: 33/1000  -  Loss: 29.636233011881508\n",
            "Epoch: 34/1000  -  Loss: 29.635961608886728\n",
            "Epoch: 35/1000  -  Loss: 29.636458892822265\n",
            "Epoch: 36/1000  -  Loss: 29.635999094645186\n",
            "Epoch: 37/1000  -  Loss: 29.63547409057617\n",
            "Epoch: 38/1000  -  Loss: 29.63533025105794\n",
            "Epoch: 39/1000  -  Loss: 29.635425821940107\n",
            "Epoch: 40/1000  -  Loss: 29.635351562499995\n",
            "Epoch: 41/1000  -  Loss: 29.635408528645836\n",
            "Epoch: 42/1000  -  Loss: 29.635032043457034\n",
            "Epoch: 43/1000  -  Loss: 29.63529037475585\n",
            "Epoch: 44/1000  -  Loss: 29.635093383789066\n",
            "Epoch: 45/1000  -  Loss: 29.634837849934897\n",
            "Epoch: 46/1000  -  Loss: 29.63487854003906\n",
            "Epoch: 47/1000  -  Loss: 29.634650522867833\n",
            "Epoch: 48/1000  -  Loss: 29.634647928873697\n",
            "Epoch: 49/1000  -  Loss: 29.634591267903645\n",
            "Epoch: 50/1000  -  Loss: 29.63462020874023\n",
            "Epoch: 51/1000  -  Loss: 29.63465377807617\n",
            "Epoch: 52/1000  -  Loss: 29.634345906575525\n",
            "Epoch: 53/1000  -  Loss: 29.634444783528643\n",
            "Epoch: 54/1000  -  Loss: 29.63420908610026\n",
            "Epoch: 55/1000  -  Loss: 29.63448455810547\n",
            "Epoch: 56/1000  -  Loss: 29.634399261474606\n",
            "Epoch: 57/1000  -  Loss: 29.63422704060873\n",
            "Epoch: 58/1000  -  Loss: 29.634047902425138\n",
            "Epoch: 59/1000  -  Loss: 29.63420908610026\n",
            "Epoch: 60/1000  -  Loss: 29.634142583211265\n",
            "Epoch: 61/1000  -  Loss: 29.63411666870117\n",
            "Epoch: 62/1000  -  Loss: 29.634114430745445\n",
            "Epoch: 63/1000  -  Loss: 29.63401962280274\n",
            "Epoch: 64/1000  -  Loss: 29.63432393391927\n",
            "Epoch: 65/1000  -  Loss: 29.633906656901043\n",
            "Epoch: 66/1000  -  Loss: 29.634067535400394\n",
            "Epoch: 67/1000  -  Loss: 29.63408554077148\n",
            "Epoch: 68/1000  -  Loss: 29.63423650105795\n",
            "Epoch: 69/1000  -  Loss: 29.63407170613607\n",
            "Epoch: 70/1000  -  Loss: 29.634166971842443\n",
            "Epoch: 71/1000  -  Loss: 29.63403340657552\n",
            "Epoch: 72/1000  -  Loss: 29.634145609537768\n",
            "Epoch: 73/1000  -  Loss: 29.634059092203778\n",
            "Epoch: 74/1000  -  Loss: 29.634076080322256\n",
            "Epoch: 75/1000  -  Loss: 29.633803710937496\n",
            "Epoch: 76/1000  -  Loss: 29.633850250244137\n",
            "Epoch: 77/1000  -  Loss: 29.633864949544268\n",
            "Epoch: 78/1000  -  Loss: 29.633911234537756\n",
            "Epoch: 79/1000  -  Loss: 29.63403615315755\n",
            "Epoch: 80/1000  -  Loss: 29.633836669921877\n",
            "Epoch: 81/1000  -  Loss: 29.63395014444987\n",
            "Epoch: 82/1000  -  Loss: 29.633891805013022\n",
            "Epoch: 83/1000  -  Loss: 29.63371744791667\n",
            "Epoch: 84/1000  -  Loss: 29.633929697672524\n",
            "Epoch: 85/1000  -  Loss: 29.633825683593745\n",
            "Epoch: 86/1000  -  Loss: 29.633846282958988\n",
            "Epoch: 87/1000  -  Loss: 29.633950856526695\n",
            "Epoch: 88/1000  -  Loss: 29.633829294840492\n",
            "Epoch: 89/1000  -  Loss: 29.633846588134762\n",
            "Epoch: 90/1000  -  Loss: 29.63395426432292\n",
            "Epoch: 91/1000  -  Loss: 29.633869527180995\n",
            "Epoch: 92/1000  -  Loss: 29.63386189778646\n",
            "Epoch: 93/1000  -  Loss: 29.633783009846997\n",
            "Epoch: 94/1000  -  Loss: 29.63392379760743\n",
            "Epoch: 95/1000  -  Loss: 29.633879597981768\n",
            "Epoch: 96/1000  -  Loss: 29.63388127644857\n",
            "Epoch: 97/1000  -  Loss: 29.633740539550782\n",
            "Epoch: 98/1000  -  Loss: 29.633789520263672\n",
            "Epoch: 99/1000  -  Loss: 29.633738962809247\n",
            "Epoch: 100/1000  -  Loss: 29.63371592203776\n",
            "Epoch: 101/1000  -  Loss: 29.633907063802084\n",
            "Epoch: 102/1000  -  Loss: 29.633821614583336\n",
            "Epoch: 103/1000  -  Loss: 29.63381881713867\n",
            "Epoch: 104/1000  -  Loss: 29.633732961018882\n",
            "Epoch: 105/1000  -  Loss: 29.633747558593747\n",
            "Epoch: 106/1000  -  Loss: 29.633886667887378\n",
            "Epoch: 107/1000  -  Loss: 29.633860168457023\n",
            "Epoch: 108/1000  -  Loss: 29.633718566894526\n",
            "Epoch: 109/1000  -  Loss: 29.633735198974613\n",
            "Epoch: 110/1000  -  Loss: 29.633633473714198\n",
            "Epoch: 111/1000  -  Loss: 29.633698527018232\n",
            "Epoch: 112/1000  -  Loss: 29.633978373209647\n",
            "Epoch: 113/1000  -  Loss: 29.63392242431641\n",
            "Epoch: 114/1000  -  Loss: 29.63374435424805\n",
            "Epoch: 115/1000  -  Loss: 29.63379842122396\n",
            "Epoch: 116/1000  -  Loss: 29.633739217122397\n",
            "Epoch: 117/1000  -  Loss: 29.633941599527994\n",
            "Epoch: 118/1000  -  Loss: 29.633748423258456\n",
            "Epoch: 119/1000  -  Loss: 29.633724416097007\n",
            "Epoch: 120/1000  -  Loss: 29.63370061238607\n",
            "Epoch: 121/1000  -  Loss: 29.633725179036457\n",
            "Epoch: 122/1000  -  Loss: 29.633803609212237\n",
            "Epoch: 123/1000  -  Loss: 29.63382822672526\n",
            "Epoch: 124/1000  -  Loss: 29.633589426676437\n",
            "Epoch: 125/1000  -  Loss: 29.63364471435547\n",
            "Epoch: 126/1000  -  Loss: 29.63366638183594\n",
            "Epoch: 127/1000  -  Loss: 29.633833109537765\n",
            "Epoch: 128/1000  -  Loss: 29.633779195149735\n",
            "Epoch: 129/1000  -  Loss: 29.633715311686196\n",
            "Epoch: 130/1000  -  Loss: 29.633613484700522\n",
            "Epoch: 131/1000  -  Loss: 29.63383209228516\n",
            "Epoch: 132/1000  -  Loss: 29.6339047241211\n",
            "Epoch: 133/1000  -  Loss: 29.63386886596679\n",
            "Epoch: 134/1000  -  Loss: 29.633853708903004\n",
            "Epoch: 135/1000  -  Loss: 29.633664449055985\n",
            "Epoch: 136/1000  -  Loss: 29.633741353352868\n",
            "Epoch: 137/1000  -  Loss: 29.63367467244466\n",
            "Epoch: 138/1000  -  Loss: 29.633773956298825\n",
            "Epoch: 139/1000  -  Loss: 29.63385070800781\n",
            "Epoch: 140/1000  -  Loss: 29.633657226562494\n",
            "Epoch: 141/1000  -  Loss: 29.633693593343093\n",
            "Epoch: 142/1000  -  Loss: 29.633794403076173\n",
            "Epoch: 143/1000  -  Loss: 29.633844858805332\n",
            "Epoch: 144/1000  -  Loss: 29.633751932779948\n",
            "Epoch: 145/1000  -  Loss: 29.633736877441407\n",
            "Epoch: 146/1000  -  Loss: 29.63381922403971\n",
            "Epoch: 147/1000  -  Loss: 29.63370422363281\n",
            "Epoch: 148/1000  -  Loss: 29.633682047526044\n",
            "Epoch: 149/1000  -  Loss: 29.633758341471356\n",
            "Epoch: 150/1000  -  Loss: 29.633897145589195\n",
            "Epoch: 151/1000  -  Loss: 29.633756103515626\n",
            "Epoch: 152/1000  -  Loss: 29.63379745483399\n",
            "Epoch: 153/1000  -  Loss: 29.633721466064454\n",
            "Epoch: 154/1000  -  Loss: 29.633595377604166\n",
            "Epoch: 155/1000  -  Loss: 29.633972218831378\n",
            "Epoch: 156/1000  -  Loss: 29.633660532633456\n",
            "Epoch: 157/1000  -  Loss: 29.63380462646484\n",
            "Epoch: 158/1000  -  Loss: 29.633822123209644\n",
            "Epoch: 159/1000  -  Loss: 29.63360107421875\n",
            "Epoch: 160/1000  -  Loss: 29.63374974568685\n",
            "Epoch: 161/1000  -  Loss: 29.633809254964195\n",
            "Epoch: 162/1000  -  Loss: 29.633811696370447\n",
            "Epoch: 163/1000  -  Loss: 29.63367757161458\n",
            "Epoch: 164/1000  -  Loss: 29.633787790934253\n",
            "Epoch: 165/1000  -  Loss: 29.63375432332356\n",
            "Epoch: 166/1000  -  Loss: 29.63378977457683\n",
            "Epoch: 167/1000  -  Loss: 29.63363438924153\n",
            "Epoch: 168/1000  -  Loss: 29.63369400024414\n",
            "Epoch: 169/1000  -  Loss: 29.63378280639648\n",
            "Epoch: 170/1000  -  Loss: 29.63381825764974\n",
            "Epoch: 171/1000  -  Loss: 29.633773752848306\n",
            "Epoch: 172/1000  -  Loss: 29.633757171630865\n",
            "Epoch: 173/1000  -  Loss: 29.633683217366528\n",
            "Epoch: 174/1000  -  Loss: 29.633714243570964\n",
            "Epoch: 175/1000  -  Loss: 29.633768819173184\n",
            "Epoch: 176/1000  -  Loss: 29.63388153076172\n",
            "Epoch: 177/1000  -  Loss: 29.633712768554688\n",
            "Epoch: 178/1000  -  Loss: 29.633632354736328\n",
            "Epoch: 179/1000  -  Loss: 29.63363255818685\n",
            "Epoch: 180/1000  -  Loss: 29.63371322631836\n",
            "Epoch: 181/1000  -  Loss: 29.633872528076168\n",
            "Epoch: 182/1000  -  Loss: 29.633740946451827\n",
            "Epoch: 183/1000  -  Loss: 29.63401514689127\n",
            "Epoch: 184/1000  -  Loss: 29.633769073486327\n",
            "Epoch: 185/1000  -  Loss: 29.633744761149092\n",
            "Epoch: 186/1000  -  Loss: 29.633697052001953\n",
            "Epoch: 187/1000  -  Loss: 29.63359680175781\n",
            "Epoch: 188/1000  -  Loss: 29.63386896769206\n",
            "Epoch: 189/1000  -  Loss: 29.63377232869466\n",
            "Epoch: 190/1000  -  Loss: 29.633748372395843\n",
            "Epoch: 191/1000  -  Loss: 29.633868509928384\n",
            "Epoch: 192/1000  -  Loss: 29.63369684855144\n",
            "Epoch: 193/1000  -  Loss: 29.633782806396486\n",
            "Epoch: 194/1000  -  Loss: 29.63386642456054\n",
            "Epoch: 195/1000  -  Loss: 29.633820597330732\n",
            "Epoch: 196/1000  -  Loss: 29.63376241048177\n",
            "Epoch: 197/1000  -  Loss: 29.633776346842446\n",
            "Epoch: 198/1000  -  Loss: 29.63364832560221\n",
            "Epoch: 199/1000  -  Loss: 29.633697764078775\n",
            "Epoch: 200/1000  -  Loss: 29.633737640380858\n",
            "Epoch: 201/1000  -  Loss: 29.633823699951172\n",
            "Epoch: 202/1000  -  Loss: 29.633837381998696\n",
            "Epoch: 203/1000  -  Loss: 29.633820749918613\n",
            "Epoch: 204/1000  -  Loss: 29.633821258544923\n",
            "Epoch: 205/1000  -  Loss: 29.633764851888017\n",
            "Epoch: 206/1000  -  Loss: 29.633742065429693\n",
            "Epoch: 207/1000  -  Loss: 29.633704884847003\n",
            "Epoch: 208/1000  -  Loss: 29.633676096598307\n",
            "Epoch: 209/1000  -  Loss: 29.633723347981768\n",
            "Epoch: 210/1000  -  Loss: 29.633644409179688\n",
            "Epoch: 211/1000  -  Loss: 29.63376510620117\n",
            "Epoch: 212/1000  -  Loss: 29.633681182861327\n",
            "Epoch: 213/1000  -  Loss: 29.633607737223304\n",
            "Epoch: 214/1000  -  Loss: 29.633729604085282\n",
            "Epoch: 215/1000  -  Loss: 29.633581288655606\n",
            "Epoch: 216/1000  -  Loss: 29.633587646484376\n",
            "Epoch: 217/1000  -  Loss: 29.633946075439457\n",
            "Epoch: 218/1000  -  Loss: 29.633982594807943\n",
            "Epoch: 219/1000  -  Loss: 29.633784942626956\n",
            "Epoch: 220/1000  -  Loss: 29.63364023844401\n",
            "Epoch: 221/1000  -  Loss: 29.63369939168295\n",
            "Epoch: 222/1000  -  Loss: 29.633713175455732\n",
            "Epoch: 223/1000  -  Loss: 29.633739115397134\n",
            "Epoch: 224/1000  -  Loss: 29.63384841918946\n",
            "Epoch: 225/1000  -  Loss: 29.633714752197264\n",
            "Epoch: 226/1000  -  Loss: 29.633881581624355\n",
            "Epoch: 227/1000  -  Loss: 29.633796132405596\n",
            "Epoch: 228/1000  -  Loss: 29.6338013458252\n",
            "Epoch: 229/1000  -  Loss: 29.63366480509439\n",
            "Epoch: 230/1000  -  Loss: 29.63381398518881\n",
            "Epoch: 231/1000  -  Loss: 29.633802846272786\n",
            "Epoch: 232/1000  -  Loss: 29.63377894083658\n",
            "Epoch: 233/1000  -  Loss: 29.63374694824219\n",
            "Epoch: 234/1000  -  Loss: 29.633827718098956\n",
            "Epoch: 235/1000  -  Loss: 29.63383239746094\n",
            "Epoch: 236/1000  -  Loss: 29.63381144205729\n",
            "Epoch: 237/1000  -  Loss: 29.63366541544596\n",
            "Epoch: 238/1000  -  Loss: 29.6336996459961\n",
            "Epoch: 239/1000  -  Loss: 29.633784637451182\n",
            "Epoch: 240/1000  -  Loss: 29.63367451985677\n",
            "Epoch: 241/1000  -  Loss: 29.633713073730465\n",
            "Epoch: 242/1000  -  Loss: 29.63369135538738\n",
            "Epoch: 243/1000  -  Loss: 29.633937225341796\n",
            "Epoch: 244/1000  -  Loss: 29.633761189778646\n",
            "Epoch: 245/1000  -  Loss: 29.633735351562496\n",
            "Epoch: 246/1000  -  Loss: 29.633761749267578\n",
            "Epoch: 247/1000  -  Loss: 29.633820851643875\n",
            "Epoch: 248/1000  -  Loss: 29.633701477050785\n",
            "Epoch: 249/1000  -  Loss: 29.633848012288407\n",
            "Epoch: 250/1000  -  Loss: 29.63383056640625\n",
            "Epoch: 251/1000  -  Loss: 29.633708445231118\n",
            "Epoch: 252/1000  -  Loss: 29.6336419169108\n",
            "Epoch: 253/1000  -  Loss: 29.63375971476237\n",
            "Epoch: 254/1000  -  Loss: 29.63388600667318\n",
            "Epoch: 255/1000  -  Loss: 29.633760630289718\n",
            "Epoch: 256/1000  -  Loss: 29.6336545308431\n",
            "Epoch: 257/1000  -  Loss: 29.633802032470705\n",
            "Epoch: 258/1000  -  Loss: 29.633810729980468\n",
            "Epoch: 259/1000  -  Loss: 29.63378016153971\n",
            "Epoch: 260/1000  -  Loss: 29.633655242919925\n",
            "Epoch: 261/1000  -  Loss: 29.633734334309892\n",
            "Epoch: 262/1000  -  Loss: 29.633777770996097\n",
            "Epoch: 263/1000  -  Loss: 29.633717447916666\n",
            "Epoch: 264/1000  -  Loss: 29.633720804850256\n",
            "Epoch: 265/1000  -  Loss: 29.633836390177407\n",
            "Epoch: 266/1000  -  Loss: 29.63378000895182\n",
            "Epoch: 267/1000  -  Loss: 29.633793385823573\n",
            "Epoch: 268/1000  -  Loss: 29.633724568684897\n",
            "Epoch: 269/1000  -  Loss: 29.63364354451497\n",
            "Epoch: 270/1000  -  Loss: 29.633890228271483\n",
            "Epoch: 271/1000  -  Loss: 29.63387395222982\n",
            "Epoch: 272/1000  -  Loss: 29.63367497762044\n",
            "Epoch: 273/1000  -  Loss: 29.633715616861974\n",
            "Epoch: 274/1000  -  Loss: 29.633572591145832\n",
            "Epoch: 275/1000  -  Loss: 29.633744913736976\n",
            "Epoch: 276/1000  -  Loss: 29.633723271687824\n",
            "Epoch: 277/1000  -  Loss: 29.63386734008789\n",
            "Epoch: 278/1000  -  Loss: 29.63377436319987\n",
            "Epoch: 279/1000  -  Loss: 29.633844655354824\n",
            "Epoch: 280/1000  -  Loss: 29.633900553385416\n",
            "Epoch: 281/1000  -  Loss: 29.63379135131837\n",
            "Epoch: 282/1000  -  Loss: 29.633752390543624\n",
            "Epoch: 283/1000  -  Loss: 29.63383387247721\n",
            "Epoch: 284/1000  -  Loss: 29.63377644856772\n",
            "Epoch: 285/1000  -  Loss: 29.6336203511556\n",
            "Epoch: 286/1000  -  Loss: 29.633758087158206\n",
            "Epoch: 287/1000  -  Loss: 29.633864237467446\n",
            "Epoch: 288/1000  -  Loss: 29.633881225585935\n",
            "Epoch: 289/1000  -  Loss: 29.633824666341148\n",
            "Epoch: 290/1000  -  Loss: 29.6337722269694\n",
            "Epoch: 291/1000  -  Loss: 29.633740488688154\n",
            "Epoch: 292/1000  -  Loss: 29.633779195149742\n",
            "Epoch: 293/1000  -  Loss: 29.633709971110026\n",
            "Epoch: 294/1000  -  Loss: 29.63370417277018\n",
            "Epoch: 295/1000  -  Loss: 29.633709055582685\n",
            "Epoch: 296/1000  -  Loss: 29.63382181803385\n",
            "Epoch: 297/1000  -  Loss: 29.633818766276043\n",
            "Epoch: 298/1000  -  Loss: 29.63377919514973\n",
            "Epoch: 299/1000  -  Loss: 29.633743794759116\n",
            "Epoch: 300/1000  -  Loss: 29.633707885742187\n",
            "Epoch: 301/1000  -  Loss: 29.633693593343096\n",
            "Epoch: 302/1000  -  Loss: 29.63371027628581\n",
            "Epoch: 303/1000  -  Loss: 29.633749593098965\n",
            "Epoch: 304/1000  -  Loss: 29.633813781738287\n",
            "Epoch: 305/1000  -  Loss: 29.633651529947915\n",
            "Epoch: 306/1000  -  Loss: 29.6337553914388\n",
            "Epoch: 307/1000  -  Loss: 29.633744252522778\n",
            "Epoch: 308/1000  -  Loss: 29.633794097900395\n",
            "Epoch: 309/1000  -  Loss: 29.633808542887365\n",
            "Epoch: 310/1000  -  Loss: 29.633846740722657\n",
            "Epoch: 311/1000  -  Loss: 29.633708546956385\n",
            "Epoch: 312/1000  -  Loss: 29.63370585123697\n",
            "Epoch: 313/1000  -  Loss: 29.633815256754552\n",
            "Epoch: 314/1000  -  Loss: 29.63370742797851\n",
            "Epoch: 315/1000  -  Loss: 29.633668975830084\n",
            "Epoch: 316/1000  -  Loss: 29.6336865234375\n",
            "Epoch: 317/1000  -  Loss: 29.63364568074544\n",
            "Epoch: 318/1000  -  Loss: 29.633817799886064\n",
            "Epoch: 319/1000  -  Loss: 29.633856709798174\n",
            "Epoch: 320/1000  -  Loss: 29.633642018636074\n",
            "Epoch: 321/1000  -  Loss: 29.63384623209635\n",
            "Epoch: 322/1000  -  Loss: 29.633600667317708\n",
            "Epoch: 323/1000  -  Loss: 29.633689575195316\n",
            "Epoch: 324/1000  -  Loss: 29.633845774332677\n",
            "Epoch: 325/1000  -  Loss: 29.633724060058583\n",
            "Epoch: 326/1000  -  Loss: 29.633840026855474\n",
            "Epoch: 327/1000  -  Loss: 29.633756459554032\n",
            "Epoch: 328/1000  -  Loss: 29.6337218729655\n",
            "Epoch: 329/1000  -  Loss: 29.63368357340495\n",
            "Epoch: 330/1000  -  Loss: 29.63385279337565\n",
            "Epoch: 331/1000  -  Loss: 29.633737538655595\n",
            "Epoch: 332/1000  -  Loss: 29.633755137125647\n",
            "Epoch: 333/1000  -  Loss: 29.63376052856446\n",
            "Epoch: 334/1000  -  Loss: 29.63370086669922\n",
            "Epoch: 335/1000  -  Loss: 29.63375015258789\n",
            "Epoch: 336/1000  -  Loss: 29.63385777791341\n",
            "Epoch: 337/1000  -  Loss: 29.633729807535797\n",
            "Epoch: 338/1000  -  Loss: 29.633542531331376\n",
            "Epoch: 339/1000  -  Loss: 29.63379221598308\n",
            "Epoch: 340/1000  -  Loss: 29.633777821858722\n",
            "Epoch: 341/1000  -  Loss: 29.633681131998696\n",
            "Epoch: 342/1000  -  Loss: 29.63384699503581\n",
            "Epoch: 343/1000  -  Loss: 29.633796234130855\n",
            "Epoch: 344/1000  -  Loss: 29.633900553385423\n",
            "Epoch: 345/1000  -  Loss: 29.633838958740238\n",
            "Epoch: 346/1000  -  Loss: 29.633812459309887\n",
            "Epoch: 347/1000  -  Loss: 29.633771667480463\n",
            "Epoch: 348/1000  -  Loss: 29.633726298014327\n",
            "Epoch: 349/1000  -  Loss: 29.633817392985033\n",
            "Epoch: 350/1000  -  Loss: 29.633769098917647\n",
            "Epoch: 351/1000  -  Loss: 29.63370737711589\n",
            "Epoch: 352/1000  -  Loss: 29.633791859944658\n",
            "Epoch: 353/1000  -  Loss: 29.633728485107422\n",
            "Epoch: 354/1000  -  Loss: 29.633740844726567\n",
            "Epoch: 355/1000  -  Loss: 29.633703257242836\n",
            "Epoch: 356/1000  -  Loss: 29.63375447591146\n",
            "Epoch: 357/1000  -  Loss: 29.633648376464844\n",
            "Epoch: 358/1000  -  Loss: 29.633646392822264\n",
            "Epoch: 359/1000  -  Loss: 29.633828226725264\n",
            "Epoch: 360/1000  -  Loss: 29.633787129720044\n",
            "Epoch: 361/1000  -  Loss: 29.633724416097007\n",
            "Epoch: 362/1000  -  Loss: 29.633781331380213\n",
            "Epoch: 363/1000  -  Loss: 29.633775685628244\n",
            "Epoch: 364/1000  -  Loss: 29.63378067016601\n",
            "Epoch: 365/1000  -  Loss: 29.63381754557291\n",
            "Epoch: 366/1000  -  Loss: 29.63372650146484\n",
            "Epoch: 367/1000  -  Loss: 29.63377563476562\n",
            "Epoch: 368/1000  -  Loss: 29.633779296875005\n",
            "Epoch: 369/1000  -  Loss: 29.63371358235677\n",
            "Epoch: 370/1000  -  Loss: 29.633728078206378\n",
            "Epoch: 371/1000  -  Loss: 29.633768768310553\n",
            "Epoch: 372/1000  -  Loss: 29.633625132242834\n",
            "Epoch: 373/1000  -  Loss: 29.633764495849608\n",
            "Epoch: 374/1000  -  Loss: 29.63373530069987\n",
            "Epoch: 375/1000  -  Loss: 29.633704833984375\n",
            "Epoch: 376/1000  -  Loss: 29.63370427449544\n",
            "Epoch: 377/1000  -  Loss: 29.633716684977212\n",
            "Epoch: 378/1000  -  Loss: 29.633817189534508\n",
            "Epoch: 379/1000  -  Loss: 29.63382385253906\n",
            "Epoch: 380/1000  -  Loss: 29.633753356933592\n",
            "Epoch: 381/1000  -  Loss: 29.63380482991537\n",
            "Epoch: 382/1000  -  Loss: 29.633730875651043\n",
            "Epoch: 383/1000  -  Loss: 29.633736572265626\n",
            "Epoch: 384/1000  -  Loss: 29.633736419677735\n",
            "Epoch: 385/1000  -  Loss: 29.633789469401044\n",
            "Epoch: 386/1000  -  Loss: 29.633836212158208\n",
            "Epoch: 387/1000  -  Loss: 29.633750661214187\n",
            "Epoch: 388/1000  -  Loss: 29.633767903645836\n",
            "Epoch: 389/1000  -  Loss: 29.63376368204753\n",
            "Epoch: 390/1000  -  Loss: 29.633816375732426\n",
            "Epoch: 391/1000  -  Loss: 29.633784840901704\n",
            "Epoch: 392/1000  -  Loss: 29.63372243245443\n",
            "Epoch: 393/1000  -  Loss: 29.633684539794928\n",
            "Epoch: 394/1000  -  Loss: 29.633662719726562\n",
            "Epoch: 395/1000  -  Loss: 29.633768666585283\n",
            "Epoch: 396/1000  -  Loss: 29.633707885742197\n",
            "Epoch: 397/1000  -  Loss: 29.633665873209637\n",
            "Epoch: 398/1000  -  Loss: 29.63374303181966\n",
            "Epoch: 399/1000  -  Loss: 29.633732096354162\n",
            "Epoch: 400/1000  -  Loss: 29.633729044596354\n",
            "Epoch: 401/1000  -  Loss: 29.63361872355144\n",
            "Epoch: 402/1000  -  Loss: 29.63378311157227\n",
            "Epoch: 403/1000  -  Loss: 29.633663940429688\n",
            "Epoch: 404/1000  -  Loss: 29.633722178141273\n",
            "Epoch: 405/1000  -  Loss: 29.63392862955729\n",
            "Epoch: 406/1000  -  Loss: 29.633859100341798\n",
            "Epoch: 407/1000  -  Loss: 29.63381001790365\n",
            "Epoch: 408/1000  -  Loss: 29.633918711344407\n",
            "Epoch: 409/1000  -  Loss: 29.633825734456376\n",
            "Epoch: 410/1000  -  Loss: 29.633789571126293\n",
            "Epoch: 411/1000  -  Loss: 29.63377909342448\n",
            "Epoch: 412/1000  -  Loss: 29.633783976236984\n",
            "Epoch: 413/1000  -  Loss: 29.633774820963545\n",
            "Epoch: 414/1000  -  Loss: 29.633716735839844\n",
            "Epoch: 415/1000  -  Loss: 29.63380477905274\n",
            "Epoch: 416/1000  -  Loss: 29.633835042317706\n",
            "Epoch: 417/1000  -  Loss: 29.633886210123705\n",
            "Epoch: 418/1000  -  Loss: 29.633819783528647\n",
            "Epoch: 419/1000  -  Loss: 29.63379155476888\n",
            "Epoch: 420/1000  -  Loss: 29.633589070638013\n",
            "Epoch: 421/1000  -  Loss: 29.63374267578126\n",
            "Epoch: 422/1000  -  Loss: 29.633711090087896\n",
            "Epoch: 423/1000  -  Loss: 29.633731079101565\n",
            "Epoch: 424/1000  -  Loss: 29.633605194091807\n",
            "Epoch: 425/1000  -  Loss: 29.63373952229818\n",
            "Epoch: 426/1000  -  Loss: 29.633779729207358\n",
            "Epoch: 427/1000  -  Loss: 29.63384002685547\n",
            "Epoch: 428/1000  -  Loss: 29.63381342569987\n",
            "Epoch: 429/1000  -  Loss: 29.633848114013666\n",
            "Epoch: 430/1000  -  Loss: 29.63370366414388\n",
            "Epoch: 431/1000  -  Loss: 29.633811289469403\n",
            "Epoch: 432/1000  -  Loss: 29.63382339477539\n",
            "Epoch: 433/1000  -  Loss: 29.63371342976888\n",
            "Epoch: 434/1000  -  Loss: 29.633747355143228\n",
            "Epoch: 435/1000  -  Loss: 29.63366251627604\n",
            "Epoch: 436/1000  -  Loss: 29.63381113688151\n",
            "Epoch: 437/1000  -  Loss: 29.63366694132487\n",
            "Epoch: 438/1000  -  Loss: 29.633737182617192\n",
            "Epoch: 439/1000  -  Loss: 29.63372975667317\n",
            "Epoch: 440/1000  -  Loss: 29.63373036702474\n",
            "Epoch: 441/1000  -  Loss: 29.633769989013675\n",
            "Epoch: 442/1000  -  Loss: 29.63356404622396\n",
            "Epoch: 443/1000  -  Loss: 29.63368581136067\n",
            "Epoch: 444/1000  -  Loss: 29.63387288411458\n",
            "Epoch: 445/1000  -  Loss: 29.63376408894857\n",
            "Epoch: 446/1000  -  Loss: 29.633771921793613\n",
            "Epoch: 447/1000  -  Loss: 29.633842010498046\n",
            "Epoch: 448/1000  -  Loss: 29.633801371256514\n",
            "Epoch: 449/1000  -  Loss: 29.633721415201816\n",
            "Epoch: 450/1000  -  Loss: 29.633741099039707\n",
            "Epoch: 451/1000  -  Loss: 29.63381744384765\n",
            "Epoch: 452/1000  -  Loss: 29.63379053751628\n",
            "Epoch: 453/1000  -  Loss: 29.633678843180338\n",
            "Epoch: 454/1000  -  Loss: 29.633717295328776\n",
            "Epoch: 455/1000  -  Loss: 29.63384546915691\n",
            "Epoch: 456/1000  -  Loss: 29.633828735351557\n",
            "Epoch: 457/1000  -  Loss: 29.633841451009115\n",
            "Epoch: 458/1000  -  Loss: 29.633747049967447\n",
            "Epoch: 459/1000  -  Loss: 29.633708902994798\n",
            "Epoch: 460/1000  -  Loss: 29.633763478597007\n",
            "Epoch: 461/1000  -  Loss: 29.633841807047524\n",
            "Epoch: 462/1000  -  Loss: 29.63375666300455\n",
            "Epoch: 463/1000  -  Loss: 29.633768564860024\n",
            "Epoch: 464/1000  -  Loss: 29.633850250244127\n",
            "Epoch: 465/1000  -  Loss: 29.633879241943365\n",
            "Epoch: 466/1000  -  Loss: 29.6337489827474\n",
            "Epoch: 467/1000  -  Loss: 29.633866271972657\n",
            "Epoch: 468/1000  -  Loss: 29.633674112955738\n",
            "Epoch: 469/1000  -  Loss: 29.633711853027343\n",
            "Epoch: 470/1000  -  Loss: 29.63374898274739\n",
            "Epoch: 471/1000  -  Loss: 29.63370442708333\n",
            "Epoch: 472/1000  -  Loss: 29.633783060709632\n",
            "Epoch: 473/1000  -  Loss: 29.633731816609696\n",
            "Epoch: 474/1000  -  Loss: 29.633710988362626\n",
            "Epoch: 475/1000  -  Loss: 29.63371231079101\n",
            "Epoch: 476/1000  -  Loss: 29.633771769205733\n",
            "Epoch: 477/1000  -  Loss: 29.633760935465492\n",
            "Epoch: 478/1000  -  Loss: 29.63376388549804\n",
            "Epoch: 479/1000  -  Loss: 29.633875630696622\n",
            "Epoch: 480/1000  -  Loss: 29.633743336995447\n",
            "Epoch: 481/1000  -  Loss: 29.633719431559243\n",
            "Epoch: 482/1000  -  Loss: 29.63380376180013\n",
            "Epoch: 483/1000  -  Loss: 29.633770395914713\n",
            "Epoch: 484/1000  -  Loss: 29.633831278483072\n",
            "Epoch: 485/1000  -  Loss: 29.633801854451498\n",
            "Epoch: 486/1000  -  Loss: 29.633898518880205\n",
            "Epoch: 487/1000  -  Loss: 29.633657582600915\n",
            "Epoch: 488/1000  -  Loss: 29.633628056844078\n",
            "Epoch: 489/1000  -  Loss: 29.633772989908856\n",
            "Epoch: 490/1000  -  Loss: 29.63372487386068\n",
            "Epoch: 491/1000  -  Loss: 29.63375045776367\n",
            "Epoch: 492/1000  -  Loss: 29.633769175211587\n",
            "Epoch: 493/1000  -  Loss: 29.63369791666667\n",
            "Epoch: 494/1000  -  Loss: 29.633697458902997\n",
            "Epoch: 495/1000  -  Loss: 29.63372533162435\n",
            "Epoch: 496/1000  -  Loss: 29.6338900756836\n",
            "Epoch: 497/1000  -  Loss: 29.633700714111338\n",
            "Epoch: 498/1000  -  Loss: 29.63365793863932\n",
            "Epoch: 499/1000  -  Loss: 29.63350153605143\n",
            "Epoch: 500/1000  -  Loss: 29.63374181111653\n",
            "Epoch: 501/1000  -  Loss: 29.63370081583659\n",
            "Epoch: 502/1000  -  Loss: 29.63370157877604\n",
            "Epoch: 503/1000  -  Loss: 29.63385599772136\n",
            "Epoch: 504/1000  -  Loss: 29.634006652832028\n",
            "Epoch: 505/1000  -  Loss: 29.633777898152672\n",
            "Epoch: 506/1000  -  Loss: 29.63370956420899\n",
            "Epoch: 507/1000  -  Loss: 29.633848876953124\n",
            "Epoch: 508/1000  -  Loss: 29.633735758463544\n",
            "Epoch: 509/1000  -  Loss: 29.6336843363444\n",
            "Epoch: 510/1000  -  Loss: 29.633713175455732\n",
            "Epoch: 511/1000  -  Loss: 29.63368937174479\n",
            "Epoch: 512/1000  -  Loss: 29.633765970865884\n",
            "Epoch: 513/1000  -  Loss: 29.633558044433595\n",
            "Epoch: 514/1000  -  Loss: 29.63362513224284\n",
            "Epoch: 515/1000  -  Loss: 29.633613382975263\n",
            "Epoch: 516/1000  -  Loss: 29.63374888102213\n",
            "Epoch: 517/1000  -  Loss: 29.633756663004554\n",
            "Epoch: 518/1000  -  Loss: 29.63370264689128\n",
            "Epoch: 519/1000  -  Loss: 29.634028930664062\n",
            "Epoch: 520/1000  -  Loss: 29.633775583903\n",
            "Epoch: 521/1000  -  Loss: 29.63366511027018\n",
            "Epoch: 522/1000  -  Loss: 29.633687489827476\n",
            "Epoch: 523/1000  -  Loss: 29.633756815592445\n",
            "Epoch: 524/1000  -  Loss: 29.63367609659831\n",
            "Epoch: 525/1000  -  Loss: 29.633883717854825\n",
            "Epoch: 526/1000  -  Loss: 29.633824412027995\n",
            "Epoch: 527/1000  -  Loss: 29.63383407592773\n",
            "Epoch: 528/1000  -  Loss: 29.633808085123693\n",
            "Epoch: 529/1000  -  Loss: 29.63381896972656\n",
            "Epoch: 530/1000  -  Loss: 29.633830922444663\n",
            "Epoch: 531/1000  -  Loss: 29.63382008870443\n",
            "Epoch: 532/1000  -  Loss: 29.633830820719396\n",
            "Epoch: 533/1000  -  Loss: 29.633756154378258\n",
            "Epoch: 534/1000  -  Loss: 29.633746363321944\n",
            "Epoch: 535/1000  -  Loss: 29.633677825927734\n",
            "Epoch: 536/1000  -  Loss: 29.63381495157877\n",
            "Epoch: 537/1000  -  Loss: 29.633707784016927\n",
            "Epoch: 538/1000  -  Loss: 29.633658243815102\n",
            "Epoch: 539/1000  -  Loss: 29.633702545166017\n",
            "Epoch: 540/1000  -  Loss: 29.633657480875648\n",
            "Epoch: 541/1000  -  Loss: 29.633861287434897\n",
            "Epoch: 542/1000  -  Loss: 29.633726908365887\n",
            "Epoch: 543/1000  -  Loss: 29.63371022542318\n",
            "Epoch: 544/1000  -  Loss: 29.633652445475263\n",
            "Epoch: 545/1000  -  Loss: 29.63373255411784\n",
            "Epoch: 546/1000  -  Loss: 29.63379776000977\n",
            "Epoch: 547/1000  -  Loss: 29.63375930786133\n",
            "Epoch: 548/1000  -  Loss: 29.63373489379883\n",
            "Epoch: 549/1000  -  Loss: 29.633765157063802\n",
            "Epoch: 550/1000  -  Loss: 29.6336208597819\n",
            "Epoch: 551/1000  -  Loss: 29.633786977132164\n",
            "Epoch: 552/1000  -  Loss: 29.633861745198566\n",
            "Epoch: 553/1000  -  Loss: 29.633772786458334\n",
            "Epoch: 554/1000  -  Loss: 29.63380477905274\n",
            "Epoch: 555/1000  -  Loss: 29.6338015238444\n",
            "Epoch: 556/1000  -  Loss: 29.633723907470703\n",
            "Epoch: 557/1000  -  Loss: 29.633830362955738\n",
            "Epoch: 558/1000  -  Loss: 29.63375300089518\n",
            "Epoch: 559/1000  -  Loss: 29.633717753092448\n",
            "Epoch: 560/1000  -  Loss: 29.63379328409831\n",
            "Epoch: 561/1000  -  Loss: 29.633757476806636\n",
            "Epoch: 562/1000  -  Loss: 29.633696899414065\n",
            "Epoch: 563/1000  -  Loss: 29.633845621744786\n",
            "Epoch: 564/1000  -  Loss: 29.633780873616534\n",
            "Epoch: 565/1000  -  Loss: 29.63380126953125\n",
            "Epoch: 566/1000  -  Loss: 29.633823649088537\n",
            "Epoch: 567/1000  -  Loss: 29.633777618408214\n",
            "Epoch: 568/1000  -  Loss: 29.63387379964193\n",
            "Epoch: 569/1000  -  Loss: 29.633726908365883\n",
            "Epoch: 570/1000  -  Loss: 29.633719278971352\n",
            "Epoch: 571/1000  -  Loss: 29.63385899861653\n",
            "Epoch: 572/1000  -  Loss: 29.63371602376302\n",
            "Epoch: 573/1000  -  Loss: 29.633726781209308\n",
            "Epoch: 574/1000  -  Loss: 29.633778279622394\n",
            "Epoch: 575/1000  -  Loss: 29.633698425292962\n",
            "Epoch: 576/1000  -  Loss: 29.633680623372392\n",
            "Epoch: 577/1000  -  Loss: 29.633735097249357\n",
            "Epoch: 578/1000  -  Loss: 29.633728434244794\n",
            "Epoch: 579/1000  -  Loss: 29.633736419677735\n",
            "Epoch: 580/1000  -  Loss: 29.633711039225258\n",
            "Epoch: 581/1000  -  Loss: 29.633611145019536\n",
            "Epoch: 582/1000  -  Loss: 29.63370534261068\n",
            "Epoch: 583/1000  -  Loss: 29.633690236409507\n",
            "Epoch: 584/1000  -  Loss: 29.633602752685544\n",
            "Epoch: 585/1000  -  Loss: 29.633727264404296\n",
            "Epoch: 586/1000  -  Loss: 29.63373357137044\n",
            "Epoch: 587/1000  -  Loss: 29.63377268473307\n",
            "Epoch: 588/1000  -  Loss: 29.633822987874346\n",
            "Epoch: 589/1000  -  Loss: 29.63376668294271\n",
            "Epoch: 590/1000  -  Loss: 29.63373647054037\n",
            "Epoch: 591/1000  -  Loss: 29.63362970987956\n",
            "Epoch: 592/1000  -  Loss: 29.633784739176427\n",
            "Epoch: 593/1000  -  Loss: 29.63372314453125\n",
            "Epoch: 594/1000  -  Loss: 29.63368820190429\n",
            "Epoch: 595/1000  -  Loss: 29.63378153483073\n",
            "Epoch: 596/1000  -  Loss: 29.6337575785319\n",
            "Epoch: 597/1000  -  Loss: 29.63382451375326\n",
            "Epoch: 598/1000  -  Loss: 29.6338065592448\n",
            "Epoch: 599/1000  -  Loss: 29.633636118570966\n",
            "Epoch: 600/1000  -  Loss: 29.633608093261724\n",
            "Epoch: 601/1000  -  Loss: 29.633760121663414\n",
            "Epoch: 602/1000  -  Loss: 29.633914082845052\n",
            "Epoch: 603/1000  -  Loss: 29.633771769205737\n",
            "Epoch: 604/1000  -  Loss: 29.63372095743815\n",
            "Epoch: 605/1000  -  Loss: 29.633860626220702\n",
            "Epoch: 606/1000  -  Loss: 29.63366806030273\n",
            "Epoch: 607/1000  -  Loss: 29.633576609293623\n",
            "Epoch: 608/1000  -  Loss: 29.633625284830728\n",
            "Epoch: 609/1000  -  Loss: 29.63394368489583\n",
            "Epoch: 610/1000  -  Loss: 29.633766377766925\n",
            "Epoch: 611/1000  -  Loss: 29.6338515218099\n",
            "Epoch: 612/1000  -  Loss: 29.633716328938796\n",
            "Epoch: 613/1000  -  Loss: 29.6337739054362\n",
            "Epoch: 614/1000  -  Loss: 29.63371231079101\n",
            "Epoch: 615/1000  -  Loss: 29.63380844116211\n",
            "Epoch: 616/1000  -  Loss: 29.633697331746415\n",
            "Epoch: 617/1000  -  Loss: 29.633596903483078\n",
            "Epoch: 618/1000  -  Loss: 29.6337488301595\n",
            "Epoch: 619/1000  -  Loss: 29.633855285644525\n",
            "Epoch: 620/1000  -  Loss: 29.633782145182295\n",
            "Epoch: 621/1000  -  Loss: 29.63383850097656\n",
            "Epoch: 622/1000  -  Loss: 29.633789265950522\n",
            "Epoch: 623/1000  -  Loss: 29.63379185994466\n",
            "Epoch: 624/1000  -  Loss: 29.633770192464194\n",
            "Epoch: 625/1000  -  Loss: 29.63396138509115\n",
            "Epoch: 626/1000  -  Loss: 29.633697153727216\n",
            "Epoch: 627/1000  -  Loss: 29.63375203450521\n",
            "Epoch: 628/1000  -  Loss: 29.63373168945313\n",
            "Epoch: 629/1000  -  Loss: 29.633749643961593\n",
            "Epoch: 630/1000  -  Loss: 29.633707122802733\n",
            "Epoch: 631/1000  -  Loss: 29.63383855183919\n",
            "Epoch: 632/1000  -  Loss: 29.633803049723305\n",
            "Epoch: 633/1000  -  Loss: 29.633845316569005\n",
            "Epoch: 634/1000  -  Loss: 29.63374562581381\n",
            "Epoch: 635/1000  -  Loss: 29.633798828124995\n",
            "Epoch: 636/1000  -  Loss: 29.6336642964681\n",
            "Epoch: 637/1000  -  Loss: 29.633646850585944\n",
            "Epoch: 638/1000  -  Loss: 29.633831532796215\n",
            "Epoch: 639/1000  -  Loss: 29.63381017049153\n",
            "Epoch: 640/1000  -  Loss: 29.63380020141602\n",
            "Epoch: 641/1000  -  Loss: 29.63378540039063\n",
            "Epoch: 642/1000  -  Loss: 29.63382074991862\n",
            "Epoch: 643/1000  -  Loss: 29.63383534749349\n",
            "Epoch: 644/1000  -  Loss: 29.63367126464844\n",
            "Epoch: 645/1000  -  Loss: 29.633660024007167\n",
            "Epoch: 646/1000  -  Loss: 29.633757960001628\n",
            "Epoch: 647/1000  -  Loss: 29.63377410888672\n",
            "Epoch: 648/1000  -  Loss: 29.633830413818355\n",
            "Epoch: 649/1000  -  Loss: 29.633852437337236\n",
            "Epoch: 650/1000  -  Loss: 29.633763478597007\n",
            "Epoch: 651/1000  -  Loss: 29.633843383789063\n",
            "Epoch: 652/1000  -  Loss: 29.63372512817383\n",
            "Epoch: 653/1000  -  Loss: 29.633766428629556\n",
            "Epoch: 654/1000  -  Loss: 29.633780314127595\n",
            "Epoch: 655/1000  -  Loss: 29.633713378906254\n",
            "Epoch: 656/1000  -  Loss: 29.63377232869466\n",
            "Epoch: 657/1000  -  Loss: 29.63372838338216\n",
            "Epoch: 658/1000  -  Loss: 29.633734079996742\n",
            "Epoch: 659/1000  -  Loss: 29.63379633585612\n",
            "Epoch: 660/1000  -  Loss: 29.633721160888676\n",
            "Epoch: 661/1000  -  Loss: 29.633727518717446\n",
            "Epoch: 662/1000  -  Loss: 29.63373189290365\n",
            "Epoch: 663/1000  -  Loss: 29.633639119466142\n",
            "Epoch: 664/1000  -  Loss: 29.6335873413086\n",
            "Epoch: 665/1000  -  Loss: 29.63369702657064\n",
            "Epoch: 666/1000  -  Loss: 29.63364171346028\n",
            "Epoch: 667/1000  -  Loss: 29.633714904785155\n",
            "Epoch: 668/1000  -  Loss: 29.633754577636722\n",
            "Epoch: 669/1000  -  Loss: 29.63369435628256\n",
            "Epoch: 670/1000  -  Loss: 29.633879191080723\n",
            "Epoch: 671/1000  -  Loss: 29.633798878987626\n",
            "Epoch: 672/1000  -  Loss: 29.633844553629554\n",
            "Epoch: 673/1000  -  Loss: 29.63365346272786\n",
            "Epoch: 674/1000  -  Loss: 29.633846918741856\n",
            "Epoch: 675/1000  -  Loss: 29.633821360270186\n",
            "Epoch: 676/1000  -  Loss: 29.63362854003906\n",
            "Epoch: 677/1000  -  Loss: 29.633859608968102\n",
            "Epoch: 678/1000  -  Loss: 29.63371002197266\n",
            "Epoch: 679/1000  -  Loss: 29.6336996459961\n",
            "Epoch: 680/1000  -  Loss: 29.633803405761718\n",
            "Epoch: 681/1000  -  Loss: 29.63382293701172\n",
            "Epoch: 682/1000  -  Loss: 29.63374369303386\n",
            "Epoch: 683/1000  -  Loss: 29.63376485188802\n",
            "Epoch: 684/1000  -  Loss: 29.633707377115893\n",
            "Epoch: 685/1000  -  Loss: 29.633699289957686\n",
            "Epoch: 686/1000  -  Loss: 29.633700408935557\n",
            "Epoch: 687/1000  -  Loss: 29.633747711181645\n",
            "Epoch: 688/1000  -  Loss: 29.633604380289707\n",
            "Epoch: 689/1000  -  Loss: 29.633722839355467\n",
            "Epoch: 690/1000  -  Loss: 29.63371302286784\n",
            "Epoch: 691/1000  -  Loss: 29.633801218668623\n",
            "Epoch: 692/1000  -  Loss: 29.63372517903646\n",
            "Epoch: 693/1000  -  Loss: 29.6337843322754\n",
            "Epoch: 694/1000  -  Loss: 29.63376739501954\n",
            "Epoch: 695/1000  -  Loss: 29.63367462158203\n",
            "Epoch: 696/1000  -  Loss: 29.633759969075513\n",
            "Epoch: 697/1000  -  Loss: 29.633873443603512\n",
            "Epoch: 698/1000  -  Loss: 29.633867390950513\n",
            "Epoch: 699/1000  -  Loss: 29.63378458658855\n",
            "Epoch: 700/1000  -  Loss: 29.633749287923173\n",
            "Epoch: 701/1000  -  Loss: 29.63376373291016\n",
            "Epoch: 702/1000  -  Loss: 29.633772023518883\n",
            "Epoch: 703/1000  -  Loss: 29.633826700846356\n",
            "Epoch: 704/1000  -  Loss: 29.633727416992187\n",
            "Epoch: 705/1000  -  Loss: 29.63365814208985\n",
            "Epoch: 706/1000  -  Loss: 29.633684438069654\n",
            "Epoch: 707/1000  -  Loss: 29.63379801432291\n",
            "Epoch: 708/1000  -  Loss: 29.633734079996742\n",
            "Epoch: 709/1000  -  Loss: 29.633828531901045\n",
            "Epoch: 710/1000  -  Loss: 29.633801269531254\n",
            "Epoch: 711/1000  -  Loss: 29.63378901163737\n",
            "Epoch: 712/1000  -  Loss: 29.63382461547852\n",
            "Epoch: 713/1000  -  Loss: 29.633785705566407\n",
            "Epoch: 714/1000  -  Loss: 29.633666025797535\n",
            "Epoch: 715/1000  -  Loss: 29.633756866455077\n",
            "Epoch: 716/1000  -  Loss: 29.633751322428388\n",
            "Epoch: 717/1000  -  Loss: 29.63375940958659\n",
            "Epoch: 718/1000  -  Loss: 29.63381403605143\n",
            "Epoch: 719/1000  -  Loss: 29.63359527587891\n",
            "Epoch: 720/1000  -  Loss: 29.633829803466792\n",
            "Epoch: 721/1000  -  Loss: 29.633630269368492\n",
            "Epoch: 722/1000  -  Loss: 29.63387237548828\n",
            "Epoch: 723/1000  -  Loss: 29.633745981852208\n",
            "Epoch: 724/1000  -  Loss: 29.63386957804362\n",
            "Epoch: 725/1000  -  Loss: 29.633764902750645\n",
            "Epoch: 726/1000  -  Loss: 29.63374435424805\n",
            "Epoch: 727/1000  -  Loss: 29.63384658813477\n",
            "Epoch: 728/1000  -  Loss: 29.63365661621094\n",
            "Epoch: 729/1000  -  Loss: 29.63374445597331\n",
            "Epoch: 730/1000  -  Loss: 29.6336811319987\n",
            "Epoch: 731/1000  -  Loss: 29.63403355916341\n",
            "Epoch: 732/1000  -  Loss: 29.633802795410155\n",
            "Epoch: 733/1000  -  Loss: 29.633696950276693\n",
            "Epoch: 734/1000  -  Loss: 29.633757222493482\n",
            "Epoch: 735/1000  -  Loss: 29.63376978556315\n",
            "Epoch: 736/1000  -  Loss: 29.63381256103516\n",
            "Epoch: 737/1000  -  Loss: 29.633830057779956\n",
            "Epoch: 738/1000  -  Loss: 29.633789621988935\n",
            "Epoch: 739/1000  -  Loss: 29.633836364746095\n",
            "Epoch: 740/1000  -  Loss: 29.63385437011719\n",
            "Epoch: 741/1000  -  Loss: 29.633720245361324\n",
            "Epoch: 742/1000  -  Loss: 29.633687438964845\n",
            "Epoch: 743/1000  -  Loss: 29.6337750752767\n",
            "Epoch: 744/1000  -  Loss: 29.63369506835937\n",
            "Epoch: 745/1000  -  Loss: 29.633740844726557\n",
            "Epoch: 746/1000  -  Loss: 29.633764444986976\n",
            "Epoch: 747/1000  -  Loss: 29.63380599975586\n",
            "Epoch: 748/1000  -  Loss: 29.63373153686523\n",
            "Epoch: 749/1000  -  Loss: 29.633780721028653\n",
            "Epoch: 750/1000  -  Loss: 29.633819122314453\n",
            "Epoch: 751/1000  -  Loss: 29.633714243570964\n",
            "Epoch: 752/1000  -  Loss: 29.63366516113281\n",
            "Epoch: 753/1000  -  Loss: 29.63381561279298\n",
            "Epoch: 754/1000  -  Loss: 29.63382771809896\n",
            "Epoch: 755/1000  -  Loss: 29.633723958333334\n",
            "Epoch: 756/1000  -  Loss: 29.63366307576498\n",
            "Epoch: 757/1000  -  Loss: 29.633679656982423\n",
            "Epoch: 758/1000  -  Loss: 29.633776499430333\n",
            "Epoch: 759/1000  -  Loss: 29.633764597574874\n",
            "Epoch: 760/1000  -  Loss: 29.63380345662435\n",
            "Epoch: 761/1000  -  Loss: 29.6336941019694\n",
            "Epoch: 762/1000  -  Loss: 29.63386505126953\n",
            "Epoch: 763/1000  -  Loss: 29.633774515787763\n",
            "Epoch: 764/1000  -  Loss: 29.633792673746747\n",
            "Epoch: 765/1000  -  Loss: 29.633788808186853\n",
            "Epoch: 766/1000  -  Loss: 29.633711293538408\n",
            "Epoch: 767/1000  -  Loss: 29.633749491373692\n",
            "Epoch: 768/1000  -  Loss: 29.633772125244146\n",
            "Epoch: 769/1000  -  Loss: 29.633718312581376\n",
            "Epoch: 770/1000  -  Loss: 29.633711547851554\n",
            "Epoch: 771/1000  -  Loss: 29.633762919108065\n",
            "Epoch: 772/1000  -  Loss: 29.633737131754565\n",
            "Epoch: 773/1000  -  Loss: 29.633791046142584\n",
            "Epoch: 774/1000  -  Loss: 29.633856658935546\n",
            "Epoch: 775/1000  -  Loss: 29.63370422363282\n",
            "Epoch: 776/1000  -  Loss: 29.633729146321613\n",
            "Epoch: 777/1000  -  Loss: 29.63373092651367\n",
            "Epoch: 778/1000  -  Loss: 29.633694763183588\n",
            "Epoch: 779/1000  -  Loss: 29.633801829020182\n",
            "Epoch: 780/1000  -  Loss: 29.633801523844408\n",
            "Epoch: 781/1000  -  Loss: 29.633762359619144\n",
            "Epoch: 782/1000  -  Loss: 29.633755035400387\n",
            "Epoch: 783/1000  -  Loss: 29.633785349527997\n",
            "Epoch: 784/1000  -  Loss: 29.633843485514326\n",
            "Epoch: 785/1000  -  Loss: 29.633859812418624\n",
            "Epoch: 786/1000  -  Loss: 29.63367462158203\n",
            "Epoch: 787/1000  -  Loss: 29.633777923583985\n",
            "Epoch: 788/1000  -  Loss: 29.63384765625\n",
            "Epoch: 789/1000  -  Loss: 29.6337543741862\n",
            "Epoch: 790/1000  -  Loss: 29.63375920613607\n",
            "Epoch: 791/1000  -  Loss: 29.633791503906245\n",
            "Epoch: 792/1000  -  Loss: 29.633791402180986\n",
            "Epoch: 793/1000  -  Loss: 29.633721211751304\n",
            "Epoch: 794/1000  -  Loss: 29.63374038696289\n",
            "Epoch: 795/1000  -  Loss: 29.63371342976887\n",
            "Epoch: 796/1000  -  Loss: 29.63389551798502\n",
            "Epoch: 797/1000  -  Loss: 29.633726399739587\n",
            "Epoch: 798/1000  -  Loss: 29.63378184000651\n",
            "Epoch: 799/1000  -  Loss: 29.633885294596357\n",
            "Epoch: 800/1000  -  Loss: 29.63393310546875\n",
            "Epoch: 801/1000  -  Loss: 29.633734537760418\n",
            "Epoch: 802/1000  -  Loss: 29.63365692138672\n",
            "Epoch: 803/1000  -  Loss: 29.633681386311846\n",
            "Epoch: 804/1000  -  Loss: 29.633740793863932\n",
            "Epoch: 805/1000  -  Loss: 29.633750915527347\n",
            "Epoch: 806/1000  -  Loss: 29.633730214436852\n",
            "Epoch: 807/1000  -  Loss: 29.633701019287106\n",
            "Epoch: 808/1000  -  Loss: 29.63367757161458\n",
            "Epoch: 809/1000  -  Loss: 29.633827514648438\n",
            "Epoch: 810/1000  -  Loss: 29.63369283040364\n",
            "Epoch: 811/1000  -  Loss: 29.633835296630863\n",
            "Epoch: 812/1000  -  Loss: 29.63373250325521\n",
            "Epoch: 813/1000  -  Loss: 29.63379048665364\n",
            "Epoch: 814/1000  -  Loss: 29.63376469930013\n",
            "Epoch: 815/1000  -  Loss: 29.633725585937494\n",
            "Epoch: 816/1000  -  Loss: 29.63377034505208\n",
            "Epoch: 817/1000  -  Loss: 29.633717905680342\n",
            "Epoch: 818/1000  -  Loss: 29.63369247436523\n",
            "Epoch: 819/1000  -  Loss: 29.633749440511068\n",
            "Epoch: 820/1000  -  Loss: 29.63364046732585\n",
            "Epoch: 821/1000  -  Loss: 29.63362991333008\n",
            "Epoch: 822/1000  -  Loss: 29.633680725097655\n",
            "Epoch: 823/1000  -  Loss: 29.633722483317054\n",
            "Epoch: 824/1000  -  Loss: 29.63371180216471\n",
            "Epoch: 825/1000  -  Loss: 29.63381011962891\n",
            "Epoch: 826/1000  -  Loss: 29.633766021728512\n",
            "Epoch: 827/1000  -  Loss: 29.633813171386713\n",
            "Epoch: 828/1000  -  Loss: 29.63378463745117\n",
            "Epoch: 829/1000  -  Loss: 29.63381998697917\n",
            "Epoch: 830/1000  -  Loss: 29.63373250325521\n",
            "Epoch: 831/1000  -  Loss: 29.633737792968745\n",
            "Epoch: 832/1000  -  Loss: 29.63369725545248\n",
            "Epoch: 833/1000  -  Loss: 29.633708394368497\n",
            "Epoch: 834/1000  -  Loss: 29.633646748860684\n",
            "Epoch: 835/1000  -  Loss: 29.633827718098964\n",
            "Epoch: 836/1000  -  Loss: 29.633746592203778\n",
            "Epoch: 837/1000  -  Loss: 29.633676808675126\n",
            "Epoch: 838/1000  -  Loss: 29.63372543334961\n",
            "Epoch: 839/1000  -  Loss: 29.63374298095703\n",
            "Epoch: 840/1000  -  Loss: 29.633608042399093\n",
            "Epoch: 841/1000  -  Loss: 29.633631083170577\n",
            "Epoch: 842/1000  -  Loss: 29.633861389160156\n",
            "Epoch: 843/1000  -  Loss: 29.63372350056966\n",
            "Epoch: 844/1000  -  Loss: 29.633854268391925\n",
            "Epoch: 845/1000  -  Loss: 29.6338276163737\n",
            "Epoch: 846/1000  -  Loss: 29.633836517333986\n",
            "Epoch: 847/1000  -  Loss: 29.633802744547523\n",
            "Epoch: 848/1000  -  Loss: 29.633752644856774\n",
            "Epoch: 849/1000  -  Loss: 29.633763783772782\n",
            "Epoch: 850/1000  -  Loss: 29.633756357828773\n",
            "Epoch: 851/1000  -  Loss: 29.633787078857427\n",
            "Epoch: 852/1000  -  Loss: 29.633686777750643\n",
            "Epoch: 853/1000  -  Loss: 29.63377059936524\n",
            "Epoch: 854/1000  -  Loss: 29.633685506184896\n",
            "Epoch: 855/1000  -  Loss: 29.633655293782553\n",
            "Epoch: 856/1000  -  Loss: 29.633659210205078\n",
            "Epoch: 857/1000  -  Loss: 29.633816274007163\n",
            "Epoch: 858/1000  -  Loss: 29.633712005615237\n",
            "Epoch: 859/1000  -  Loss: 29.633722991943355\n",
            "Epoch: 860/1000  -  Loss: 29.633598429361978\n",
            "Epoch: 861/1000  -  Loss: 29.633508249918624\n",
            "Epoch: 862/1000  -  Loss: 29.633600158691408\n",
            "Epoch: 863/1000  -  Loss: 29.633827921549482\n",
            "Epoch: 864/1000  -  Loss: 29.633686981201173\n",
            "Epoch: 865/1000  -  Loss: 29.63406717936198\n",
            "Epoch: 866/1000  -  Loss: 29.633644256591797\n",
            "Epoch: 867/1000  -  Loss: 29.63381958007812\n",
            "Epoch: 868/1000  -  Loss: 29.633857167561846\n",
            "Epoch: 869/1000  -  Loss: 29.633845977783213\n",
            "Epoch: 870/1000  -  Loss: 29.63380579630534\n",
            "Epoch: 871/1000  -  Loss: 29.633815561930337\n",
            "Epoch: 872/1000  -  Loss: 29.633938852945967\n",
            "Epoch: 873/1000  -  Loss: 29.6337690226237\n",
            "Epoch: 874/1000  -  Loss: 29.63374272664388\n",
            "Epoch: 875/1000  -  Loss: 29.633702036539706\n",
            "Epoch: 876/1000  -  Loss: 29.633716379801434\n",
            "Epoch: 877/1000  -  Loss: 29.633837331136064\n",
            "Epoch: 878/1000  -  Loss: 29.633782348632813\n",
            "Epoch: 879/1000  -  Loss: 29.63380391438802\n",
            "Epoch: 880/1000  -  Loss: 29.633832651774096\n",
            "Epoch: 881/1000  -  Loss: 29.63373738606771\n",
            "Epoch: 882/1000  -  Loss: 29.63382481892904\n",
            "Epoch: 883/1000  -  Loss: 29.633700002034505\n",
            "Epoch: 884/1000  -  Loss: 29.633850809733076\n",
            "Epoch: 885/1000  -  Loss: 29.63365137736002\n",
            "Epoch: 886/1000  -  Loss: 29.633672637939455\n",
            "Epoch: 887/1000  -  Loss: 29.633684285481774\n",
            "Epoch: 888/1000  -  Loss: 29.633788045247396\n",
            "Epoch: 889/1000  -  Loss: 29.633601684570312\n",
            "Epoch: 890/1000  -  Loss: 29.633716786702472\n",
            "Epoch: 891/1000  -  Loss: 29.633720601399737\n",
            "Epoch: 892/1000  -  Loss: 29.633590952555338\n",
            "Epoch: 893/1000  -  Loss: 29.633821614583326\n",
            "Epoch: 894/1000  -  Loss: 29.633684438069665\n",
            "Epoch: 895/1000  -  Loss: 29.633839314778644\n",
            "Epoch: 896/1000  -  Loss: 29.63387537638346\n",
            "Epoch: 897/1000  -  Loss: 29.633766937255864\n",
            "Epoch: 898/1000  -  Loss: 29.633885955810555\n",
            "Epoch: 899/1000  -  Loss: 29.63359975179037\n",
            "Epoch: 900/1000  -  Loss: 29.63364262898763\n",
            "Epoch: 901/1000  -  Loss: 29.633747355143242\n",
            "Epoch: 902/1000  -  Loss: 29.63381713867188\n",
            "Epoch: 903/1000  -  Loss: 29.63376810709636\n",
            "Epoch: 904/1000  -  Loss: 29.63368713378907\n",
            "Epoch: 905/1000  -  Loss: 29.633726399739587\n",
            "Epoch: 906/1000  -  Loss: 29.63372436523437\n",
            "Epoch: 907/1000  -  Loss: 29.63361236572265\n",
            "Epoch: 908/1000  -  Loss: 29.63396891276042\n",
            "Epoch: 909/1000  -  Loss: 29.633839365641276\n",
            "Epoch: 910/1000  -  Loss: 29.63376088460287\n",
            "Epoch: 911/1000  -  Loss: 29.63388692220052\n",
            "Epoch: 912/1000  -  Loss: 29.633746134440102\n",
            "Epoch: 913/1000  -  Loss: 29.63360295613607\n",
            "Epoch: 914/1000  -  Loss: 29.63379079182943\n",
            "Epoch: 915/1000  -  Loss: 29.633858032226563\n",
            "Epoch: 916/1000  -  Loss: 29.633818155924473\n",
            "Epoch: 917/1000  -  Loss: 29.633800964355473\n",
            "Epoch: 918/1000  -  Loss: 29.6336821492513\n",
            "Epoch: 919/1000  -  Loss: 29.633798319498695\n",
            "Epoch: 920/1000  -  Loss: 29.633740692138666\n",
            "Epoch: 921/1000  -  Loss: 29.633715159098315\n",
            "Epoch: 922/1000  -  Loss: 29.633676859537758\n",
            "Epoch: 923/1000  -  Loss: 29.63374252319336\n",
            "Epoch: 924/1000  -  Loss: 29.63378733317058\n",
            "Epoch: 925/1000  -  Loss: 29.63380381266276\n",
            "Epoch: 926/1000  -  Loss: 29.633626810709632\n",
            "Epoch: 927/1000  -  Loss: 29.63375015258789\n",
            "Epoch: 928/1000  -  Loss: 29.633815561930348\n",
            "Epoch: 929/1000  -  Loss: 29.63381195068359\n",
            "Epoch: 930/1000  -  Loss: 29.633641052246094\n",
            "Epoch: 931/1000  -  Loss: 29.63369628906251\n",
            "Epoch: 932/1000  -  Loss: 29.633655548095703\n",
            "Epoch: 933/1000  -  Loss: 29.633888346354166\n",
            "Epoch: 934/1000  -  Loss: 29.63375513712565\n",
            "Epoch: 935/1000  -  Loss: 29.633795369466146\n",
            "Epoch: 936/1000  -  Loss: 29.633672281901045\n",
            "Epoch: 937/1000  -  Loss: 29.633750813802084\n",
            "Epoch: 938/1000  -  Loss: 29.63372055053712\n",
            "Epoch: 939/1000  -  Loss: 29.633802057902017\n",
            "Epoch: 940/1000  -  Loss: 29.633646850585933\n",
            "Epoch: 941/1000  -  Loss: 29.633715362548834\n",
            "Epoch: 942/1000  -  Loss: 29.6337331644694\n",
            "Epoch: 943/1000  -  Loss: 29.633734334309892\n",
            "Epoch: 944/1000  -  Loss: 29.63379679361979\n",
            "Epoch: 945/1000  -  Loss: 29.63383900960287\n",
            "Epoch: 946/1000  -  Loss: 29.633696746826175\n",
            "Epoch: 947/1000  -  Loss: 29.63383916219076\n",
            "Epoch: 948/1000  -  Loss: 29.633587239583324\n",
            "Epoch: 949/1000  -  Loss: 29.633574600219728\n",
            "Epoch: 950/1000  -  Loss: 29.633733011881514\n",
            "Epoch: 951/1000  -  Loss: 29.63372439066568\n",
            "Epoch: 952/1000  -  Loss: 29.633582305908202\n",
            "Epoch: 953/1000  -  Loss: 29.6338693745931\n",
            "Epoch: 954/1000  -  Loss: 29.633719787597652\n",
            "Epoch: 955/1000  -  Loss: 29.63382609049479\n",
            "Epoch: 956/1000  -  Loss: 29.63377649943034\n",
            "Epoch: 957/1000  -  Loss: 29.633723449707027\n",
            "Epoch: 958/1000  -  Loss: 29.633835093180327\n",
            "Epoch: 959/1000  -  Loss: 29.63386199951172\n",
            "Epoch: 960/1000  -  Loss: 29.633788426717125\n",
            "Epoch: 961/1000  -  Loss: 29.633852437337246\n",
            "Epoch: 962/1000  -  Loss: 29.633669484456377\n",
            "Epoch: 963/1000  -  Loss: 29.633779093424483\n",
            "Epoch: 964/1000  -  Loss: 29.63377695719401\n",
            "Epoch: 965/1000  -  Loss: 29.633656311035157\n",
            "Epoch: 966/1000  -  Loss: 29.633737284342452\n",
            "Epoch: 967/1000  -  Loss: 29.633743286132812\n",
            "Epoch: 968/1000  -  Loss: 29.633776143391923\n",
            "Epoch: 969/1000  -  Loss: 29.633701883951815\n",
            "Epoch: 970/1000  -  Loss: 29.63378631591797\n",
            "Epoch: 971/1000  -  Loss: 29.633825174967452\n",
            "Epoch: 972/1000  -  Loss: 29.633867441813162\n",
            "Epoch: 973/1000  -  Loss: 29.633784128824868\n",
            "Epoch: 974/1000  -  Loss: 29.63367472330729\n",
            "Epoch: 975/1000  -  Loss: 29.633715769449868\n",
            "Epoch: 976/1000  -  Loss: 29.633773905436197\n",
            "Epoch: 977/1000  -  Loss: 29.63370295206706\n",
            "Epoch: 978/1000  -  Loss: 29.633814748128255\n",
            "Epoch: 979/1000  -  Loss: 29.633769734700525\n",
            "Epoch: 980/1000  -  Loss: 29.63386027018229\n",
            "Epoch: 981/1000  -  Loss: 29.63378189086914\n",
            "Epoch: 982/1000  -  Loss: 29.633753509521483\n",
            "Epoch: 983/1000  -  Loss: 29.6337819925944\n",
            "Epoch: 984/1000  -  Loss: 29.633689880371094\n",
            "Epoch: 985/1000  -  Loss: 29.633764750162758\n",
            "Epoch: 986/1000  -  Loss: 29.63368220011392\n",
            "Epoch: 987/1000  -  Loss: 29.633809865315765\n",
            "Epoch: 988/1000  -  Loss: 29.633794148763016\n",
            "Epoch: 989/1000  -  Loss: 29.633822479248053\n",
            "Epoch: 990/1000  -  Loss: 29.633795166015624\n",
            "Epoch: 991/1000  -  Loss: 29.633817189534508\n",
            "Epoch: 992/1000  -  Loss: 29.633689778645834\n",
            "Epoch: 993/1000  -  Loss: 29.633899281819662\n",
            "Epoch: 994/1000  -  Loss: 29.633822580973316\n",
            "Epoch: 995/1000  -  Loss: 29.633824564615892\n",
            "Epoch: 996/1000  -  Loss: 29.63370646158854\n",
            "Epoch: 997/1000  -  Loss: 29.63371897379558\n",
            "Epoch: 998/1000  -  Loss: 29.633746541341146\n",
            "Epoch: 999/1000  -  Loss: 29.633862508138023\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}