{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook_replication.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riOxinNHJcIB",
        "outputId": "a0136247-432c-43a0-a661-a8ba4c3b0a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'replication-topic-modelling-in-embedding-space'...\n",
            "remote: Enumerating objects: 137, done.\u001b[K\n",
            "remote: Counting objects: 100% (137/137), done.\u001b[K\n",
            "remote: Compressing objects: 100% (108/108), done.\u001b[K\n",
            "remote: Total 137 (delta 64), reused 83 (delta 25), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (137/137), 3.09 MiB | 10.19 MiB/s, done.\n",
            "Resolving deltas: 100% (64/64), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hanhluukim/replication-topic-modelling-in-embedding-space.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/replication-topic-modelling-in-embedding-space"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_6em-5qJg5e",
        "outputId": "5e790450-6218-40f2-d61f-60a7e4903ca4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/replication-topic-modelling-in-embedding-space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Datensatz, Vorverarbeitung und BOW-Repräsentationen**\n",
        "1. Vocabular erstellen\n",
        "2. BOW-Repräsentationen für allen Teildatensätzen"
      ],
      "metadata": {
        "id": "QzWqQhPQdJWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.preprare_dataset import TextDataLoader"
      ],
      "metadata": {
        "id": "1OCULr82pfgk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init TextDataLoader für die Datenquelle 20 News Groups\n",
        "textsloader = TextDataLoader(source=\"20newsgroups\", train_size=None, test_size=None)"
      ],
      "metadata": {
        "id": "cy0PpjxEpbrR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Daten abrufen vom Sklearn, tokenisieren und besondere Charaktern entfernen\n",
        "textsloader.load_tokenize_texts(\"20newsgroups\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCqh_f-7piyA",
        "outputId": "587d88ae-f980-427e-bdc6-74fd2ee7268e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading texts: ...\n",
            "finished load!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Beispiel von Textdaten\n",
        "textsloader.show_example_raw_texts(n_docs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHrl1vpZqAtL",
        "outputId": "846451de-e610-41a2-c432-bd96ad5999fb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check some sample texts of the dataset\n",
            "['From', ':', 'lerxst', '@', 'wam', '.', 'umd', '.', 'edu', '(', \"where's\", 'my', 'thing', ')', 'Subject', ':', 'WHAT', 'car', 'is', 'this', '!', '?', 'Nntp', 'Posting', 'Host', ':', 'rac3', '.', 'wam', '.', 'umd', '.', 'edu', 'Organization', ':', 'University', 'of', 'Maryland', ',', 'College', 'Park', 'Lines', ':', '15', 'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.', 'It', 'was', 'a', '2', 'door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s', '/', 'early', '70s', '.', 'It', 'was', 'called', 'a', 'Bricklin', '.', 'The', 'doors', 'were', 'really', 'small', '.', 'In', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'This', 'is', 'all', 'I', 'know', '.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e', 'mail', '.', 'Thanks', ',', 'IL', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst']\n",
            "====================================================================================================\n",
            "['From', ':', 'guykuo', '@', 'carson', '.', 'u', '.', 'washington', '.', 'edu', '(', 'Guy', 'Kuo', ')', 'Subject', ':', 'SI', 'Clock', 'Poll', 'Final', 'Call', 'Summary', ':', 'Final', 'call', 'for', 'SI', 'clock', 'reports', 'Keywords', ':', 'SI', ',', 'acceleration', ',', 'clock', ',', 'upgrade', 'Article', 'I', '.', 'D', '.', ':', 'shelley', '.', '1qvfo9INNc3s', 'Organization', ':', 'University', 'of', 'Washington', 'Lines', ':', '11', 'NNTP', 'Posting', 'Host', ':', 'carson', '.', 'u', '.', 'washington', '.', 'edu', 'A', 'fair', 'number', 'of', 'brave', 'souls', 'who', 'upgraded', 'their', 'SI', 'clock', 'oscillator', 'have', 'shared', 'their', 'experiences', 'for', 'this', 'poll', '.', 'Please', 'send', 'a', 'brief', 'message', 'detailing', 'your', 'experiences', 'with', 'the', 'procedure', '.', 'Top', 'speed', 'attained', ',', 'CPU', 'rated', 'speed', ',', 'add', 'on', 'cards', 'and', 'adapters', ',', 'heat', 'sinks', ',', 'hour', 'of', 'usage', 'per', 'day', ',', 'floppy', 'disk', 'functionality', 'with', '800', 'and', '1', '.', '4', 'm', 'floppies', 'are', 'especially', 'requested', '.', 'I', 'will', 'be', 'summarizing', 'in', 'the', 'next', 'two', 'days', ',', 'so', 'please', 'add', 'to', 'the', 'network', 'knowledge', 'base', 'if', 'you', 'have', 'done', 'the', 'clock', 'upgrade', 'and', \"haven't\", 'answered', 'this', 'poll', '.', 'Thanks', '.', 'Guy', 'Kuo', '<', 'guykuo', '@', 'u', '.', 'washington', '.', 'edu', '>']\n",
            "====================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vorverarbeitung von Daten mit folgenden Schritten:\n",
        "textsloader.preprocess_texts(length_one_remove=True, punctuation_lower = True, stopwords_filter = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5odpQDJ7qPTt",
        "outputId": "8b96ef10-f8d2-4647-fd6f-0ea593f0da1a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start: preprocessing: ...\n",
            "finised: preprocessing!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Daten zerlegen für Train, Test und Validation. Erstellen Vocabular aus dem Trainset\n",
        "textsloader.split_and_create_voca_from_trainset(max_df=0.7, min_df=10, stopwords_remove_from_voca=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRRCNPa9qXfq",
        "outputId": "9e71bc87-ac04-4841-b0ab-6fdad8464853"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "permuted indices for the train set: [137 104 206 136 108  83 124 126 246 140 132 103 232 102  97]\n",
            "start creating vocabulary ...\n",
            "length of the vocabulary: 348\n",
            "sample ten words of the vocabulary: ['university', 'mark', 'international', 'assuming', 'game', 'remember', 'fine', 'related', 'due', 'power']\n",
            "length word2id list: 348\n",
            "length id2word list: 348\n",
            "finished: creating vocabulary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Erstellen BOW-Repräsentation für ETM Modell\n",
        "for_lda_model = False \n",
        "word2id, id2word, train, test, val = textsloader.create_bow_and_savebow_for_each_set(for_lda_model=for_lda_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etzyjh_nqi19",
        "outputId": "b748c720-33bd-4c1f-81ff-746809d1dee2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length train-documents-indices : 4356\n",
            "length of the vocabulary: 348\n",
            "\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 149\n",
            "min doc-id: 0\n",
            "all docs: 4356\n",
            "all words: 4356\n",
            "docidx unique 150\n",
            "words unique: 348\n",
            "ndocs: 150\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 49\n",
            "min doc-id: 0\n",
            "all docs: 1465\n",
            "all words: 1465\n",
            "docidx unique 50\n",
            "words unique: 325\n",
            "ndocs: 50\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 3, 4, 7, 8, 9, 11, 13]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 49\n",
            "min doc-id: 0\n",
            "all docs: 719\n",
            "all words: 719\n",
            "docidx unique 50\n",
            "words unique: 240\n",
            "ndocs: 50\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 4, 5, 6, 9, 10, 11, 12, 13]\n",
            "max word-id: 346\n",
            "min word-id: 0\n",
            "max doc-id: 49\n",
            "min doc-id: 0\n",
            "all docs: 746\n",
            "all words: 746\n",
            "docidx unique 50\n",
            "words unique: 269\n",
            "ndocs: 50\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 99\n",
            "min doc-id: 0\n",
            "all docs: 3293\n",
            "all words: 3293\n",
            "docidx unique 100\n",
            "words unique: 348\n",
            "ndocs: 100\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kontrollieren die Größen von verschiedenen Datensätzen\n",
        "print(f'Size of the vocabulary after prprocessing ist: {len(textsloader.vocabulary)}')\n",
        "print(f'Size of train set: {len(train[\"tokens\"])}')\n",
        "print(f'Size of val set: {len(val[\"tokens\"])}')\n",
        "print(f'Size of test set: {len(test[\"test\"][\"tokens\"])}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1d-5ji3qwE8",
        "outputId": "1645516d-4bcb-412e-9ec9-996b134b594e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the vocabulary after prprocessing ist: 348\n",
            "Size of train set: 150\n",
            "Size of val set: 100\n",
            "Size of test set: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# re-erstellen von Dokumenten nach der Vorverarbeitungen. Die Dokumenten sind in Wörtern und werden für Word-Embedding Training benutzt\n",
        "docs_tr, docs_t, docs_v = textsloader.get_docs_in_words_for_each_set()\n",
        "print(docs_tr[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDXEEBHfq3Cy",
        "outputId": "e90fae28-d17c-4fb2-ecd7-052b47390ca0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['reply', 'nntp', 'posting', 'host', 'card', 'ram', 'appreciated', 'card']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word-Embedding-Trainieren aus dem Train-documents**"
      ],
      "metadata": {
        "id": "Ds_KuUTQrK5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "a81FTVFsrPog"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run main.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y4xDyP3Kv2_",
        "outputId": "36b2e41c-3123-4454-d3e9-b41051b91e42"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA\n",
            "\n",
            "\n",
            "loading texts: ...\n",
            "finished load!\n",
            "\n",
            "\n",
            "check some sample texts of the dataset\n",
            "['From', ':', 'lerxst', '@', 'wam', '.', 'umd', '.', 'edu', '(', \"where's\", 'my', 'thing', ')', 'Subject', ':', 'WHAT', 'car', 'is', 'this', '!', '?', 'Nntp', 'Posting', 'Host', ':', 'rac3', '.', 'wam', '.', 'umd', '.', 'edu', 'Organization', ':', 'University', 'of', 'Maryland', ',', 'College', 'Park', 'Lines', ':', '15', 'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.', 'It', 'was', 'a', '2', 'door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s', '/', 'early', '70s', '.', 'It', 'was', 'called', 'a', 'Bricklin', '.', 'The', 'doors', 'were', 'really', 'small', '.', 'In', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'This', 'is', 'all', 'I', 'know', '.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e', 'mail', '.', 'Thanks', ',', 'IL', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst']\n",
            "====================================================================================================\n",
            "['From', ':', 'guykuo', '@', 'carson', '.', 'u', '.', 'washington', '.', 'edu', '(', 'Guy', 'Kuo', ')', 'Subject', ':', 'SI', 'Clock', 'Poll', 'Final', 'Call', 'Summary', ':', 'Final', 'call', 'for', 'SI', 'clock', 'reports', 'Keywords', ':', 'SI', ',', 'acceleration', ',', 'clock', ',', 'upgrade', 'Article', 'I', '.', 'D', '.', ':', 'shelley', '.', '1qvfo9INNc3s', 'Organization', ':', 'University', 'of', 'Washington', 'Lines', ':', '11', 'NNTP', 'Posting', 'Host', ':', 'carson', '.', 'u', '.', 'washington', '.', 'edu', 'A', 'fair', 'number', 'of', 'brave', 'souls', 'who', 'upgraded', 'their', 'SI', 'clock', 'oscillator', 'have', 'shared', 'their', 'experiences', 'for', 'this', 'poll', '.', 'Please', 'send', 'a', 'brief', 'message', 'detailing', 'your', 'experiences', 'with', 'the', 'procedure', '.', 'Top', 'speed', 'attained', ',', 'CPU', 'rated', 'speed', ',', 'add', 'on', 'cards', 'and', 'adapters', ',', 'heat', 'sinks', ',', 'hour', 'of', 'usage', 'per', 'day', ',', 'floppy', 'disk', 'functionality', 'with', '800', 'and', '1', '.', '4', 'm', 'floppies', 'are', 'especially', 'requested', '.', 'I', 'will', 'be', 'summarizing', 'in', 'the', 'next', 'two', 'days', ',', 'so', 'please', 'add', 'to', 'the', 'network', 'knowledge', 'base', 'if', 'you', 'have', 'done', 'the', 'clock', 'upgrade', 'and', \"haven't\", 'answered', 'this', 'poll', '.', 'Thanks', '.', 'Guy', 'Kuo', '<', 'guykuo', '@', 'u', '.', 'washington', '.', 'edu', '>']\n",
            "====================================================================================================\n",
            "\n",
            "\n",
            "total documents 300\n",
            "start: preprocessing: ...\n",
            "finised: preprocessing!\n",
            "\n",
            "\n",
            "permuted indices for the train set: [231 245  85  37 179 247  65 145 119 176 122 226 218 164  59]\n",
            "start creating vocabulary ...\n",
            "length of the vocabulary: 348\n",
            "sample ten words of the vocabulary: ['university', 'mark', 'international', 'assuming', 'game', 'remember', 'fine', 'related', 'due', 'power']\n",
            "length word2id list: 348\n",
            "length id2word list: 348\n",
            "finished: creating vocabulary\n",
            "\n",
            "\n",
            "length train-documents-indices : 4947\n",
            "length of the vocabulary: 348\n",
            "\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 149\n",
            "min doc-id: 0\n",
            "all docs: 4947\n",
            "all words: 4947\n",
            "docidx unique 150\n",
            "words unique: 348\n",
            "ndocs: 150\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 49\n",
            "min doc-id: 0\n",
            "all docs: 1465\n",
            "all words: 1465\n",
            "docidx unique 50\n",
            "words unique: 325\n",
            "ndocs: 50\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 3, 4, 7, 8, 9, 11, 13]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 49\n",
            "min doc-id: 0\n",
            "all docs: 719\n",
            "all words: 719\n",
            "docidx unique 50\n",
            "words unique: 240\n",
            "ndocs: 50\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 4, 5, 6, 9, 10, 11, 12, 13]\n",
            "max word-id: 346\n",
            "min word-id: 0\n",
            "max doc-id: 49\n",
            "min doc-id: 0\n",
            "all docs: 746\n",
            "all words: 746\n",
            "docidx unique 50\n",
            "words unique: 269\n",
            "ndocs: 50\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 99\n",
            "min doc-id: 0\n",
            "all docs: 2702\n",
            "all words: 2702\n",
            "docidx unique 100\n",
            "words unique: 348\n",
            "ndocs: 100\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "compact representation for LDA\n",
            "\n",
            "\n",
            "train representation for LDA\n",
            "id2word for LDA: {0: 'university', 1: 'mark', 2: 'international', 3: 'assuming', 4: 'game', 5: 'remember', 6: 'fine', 7: 'related', 8: 'due', 9: 'power', 10: 'pay', 11: 'distribution', 12: 'wondering', 13: 'department', 14: 'stop', 15: 'college', 16: 'advance', 17: 'sort', 18: 'simple', 19: 'error', 20: 'thought', 21: 'steve', 22: 'dept', 23: 'speed', 24: 'cs', 25: 'light', 26: 'front', 27: 'summary', 28: 'technical', 29: 'including', 30: 'idea', 31: 'friend', 32: 'dod', 33: 'thing', 34: 'reading', 35: 'support', 36: 'center', 37: 'hand', 38: 'higher', 39: 'design', 40: 'access', 41: 'job', 42: 'big', 43: 'single', 44: 'takes', 45: 'christian', 46: 'technology', 47: 'change', 48: 'high', 49: 'article', 50: 'jim', 51: 'car', 52: 'au', 53: 'phone', 54: 'strong', 55: 'years', 56: 'called', 57: 'lot', 58: 'care', 59: 'minutes', 60: 'org', 61: 'hold', 62: 'white', 63: 'usa', 64: 'laboratory', 65: 'washington', 66: 'posted', 67: 'posting', 68: 'sci', 69: 'possibly', 70: 'matter', 71: 'general', 72: 'sound', 73: 'receive', 74: 'law', 75: 'group', 76: 'late', 77: 'east', 78: 'gun', 79: 'months', 80: 'engineering', 81: 'close', 82: 'work', 83: 'sell', 84: 'deleted', 85: 'nice', 86: 'form', 87: 'considered', 88: 'appreciated', 89: 'wrong', 90: 'drive', 91: 'guess', 92: 'hp', 93: 'list', 94: 'top', 95: 'stuff', 96: 'data', 97: 'program', 98: 'claim', 99: 'small', 100: 'write', 101: 'corporation', 102: 'religion', 103: 'agree', 104: 'windows', 105: 'short', 106: 'source', 107: 'email', 108: 'feel', 109: 'run', 110: 'current', 111: 'nntp', 112: 'rest', 113: 'net', 114: 'gov', 115: 'nasa', 116: 'buy', 117: 'expressed', 118: 'written', 119: 'keywords', 120: 'ago', 121: 'project', 122: 'box', 123: 'card', 124: 'couple', 125: 'pretty', 126: 'calls', 127: 'large', 128: 'science', 129: 'disk', 130: 'copy', 131: 'today', 132: 'fax', 133: 'mike', 134: 'give', 135: 'important', 136: 'ram', 137: 'heard', 138: 'true', 139: 'hell', 140: 'situation', 141: 'word', 142: 'canada', 143: 'place', 144: 'easy', 145: 'originator', 146: 'image', 147: 'ac', 148: 'reason', 149: 'computer', 150: 'truth', 151: 'open', 152: 'news', 153: 'opinion', 154: 'history', 155: 'result', 156: 'find', 157: 'means', 158: 'final', 159: 'case', 160: 'post', 161: 'class', 162: 'figure', 163: 'opinions', 164: 'order', 165: 'california', 166: 'taking', 167: 'things', 168: 'asked', 169: 'city', 170: 'major', 171: 'de', 172: 'chip', 173: 'newsreader', 174: 'hope', 175: 'sense', 176: 'religious', 177: 'problems', 178: 'children', 179: 'uk', 180: 'issue', 181: 'contact', 182: 'world', 183: 'type', 184: 'book', 185: 'early', 186: 'required', 187: 'ibm', 188: 'questions', 189: 'understand', 190: 'james', 191: 'worth', 192: 'makes', 193: 'texas', 194: 'home', 195: 'radio', 196: 'company', 197: 'file', 198: 'mind', 199: 'free', 200: 'toronto', 201: 'year', 202: 'pc', 203: 'told', 204: 'making', 205: 'view', 206: 'call', 207: 'information', 208: 'public', 209: 'bit', 210: 'good', 211: 'interested', 212: 'guy', 213: 'thinking', 214: 'problem', 215: 'rate', 216: 'part', 217: 'sale', 218: 'time', 219: 'robert', 220: 'board', 221: 'continue', 222: 'start', 223: 'machine', 224: 'found', 225: 'bill', 226: 'simply', 227: 'host', 228: 'sun', 229: 'read', 230: 'cc', 231: 'hardware', 232: 'gas', 233: 'reply', 234: 'ms', 235: 'disclaimer', 236: 'software', 237: 'made', 238: 'real', 239: 'fact', 240: 'day', 241: 'memory', 242: 'long', 243: 'internet', 244: 'western', 245: 'person', 246: 'similar', 247: 'days', 248: 'worse', 249: 'info', 250: 'full', 251: 'service', 252: 'writes', 253: 'network', 254: 'rights', 255: 'dos', 256: 'great', 257: 'set', 258: 'love', 259: 'bike', 260: 'hard', 261: 'apr', 262: 'make', 263: 'mail', 264: 'state', 265: 'key', 266: 'mac', 267: 'money', 268: 'put', 269: 'mine', 270: 'date', 271: 'faster', 272: 'folks', 273: 'results', 274: 'parts', 275: 'number', 276: 'division', 277: 'end', 278: 'effect', 279: 'question', 280: 'response', 281: 'answer', 282: 'note', 283: 'national', 284: 'experience', 285: 'bible', 286: 'times', 287: 'total', 288: 'display', 289: 'wanted', 290: 'hear', 291: 'week', 292: 'numbers', 293: 'school', 294: 'tin', 295: 'live', 296: 'god', 297: 'tom', 298: 'coming', 299: 'space', 300: 'systems', 301: 'research', 302: 'population', 303: 'left', 304: 'low', 305: 'man', 306: 'side', 307: 'people', 308: 'miles', 309: 'lost', 310: 'interesting', 311: 'computing', 312: 'david', 313: 'cost', 314: 'code', 315: 'wrote', 316: 'system', 317: 'based', 318: 'address', 319: 'control', 320: 'government', 321: 'life', 322: 'send', 323: 'mentioned', 324: 'point', 325: 'kind', 326: 'bad', 327: 'ca', 328: 'references', 329: 'interest', 330: 'common', 331: 'understanding', 332: 'jews', 333: 'line', 334: 'exists', 335: 'application', 336: 'talking', 337: 'original', 338: 'message', 339: 'add', 340: 'version', 341: 'clear', 342: 'price', 343: 'turn', 344: 'john', 345: 'back', 346: 'show', 347: 'institute'}\n",
            "example bow-representation for lda: \n",
            "[4, 5, 6, 9, 14, 16, 21, 22, 25, 26, 30, 34, 41, 42, 45, 46, 49, 51, 52, 54, 55, 56, 57, 58, 59, 65, 66, 69, 70, 72, 73, 77, 78, 82, 85, 87, 90, 91, 92, 93, 94, 96, 98, 99, 101, 105, 107, 110, 111, 112, 114, 118, 120, 122, 125, 127, 128, 131, 134, 138, 139, 143, 145, 147]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "length train-documents-indices : 4947\n",
            "length of the vocabulary: 348\n",
            "\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 149\n",
            "min doc-id: 0\n",
            "all docs: 4947\n",
            "all words: 4947\n",
            "docidx unique 150\n",
            "words unique: 348\n",
            "ndocs: 150\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 49\n",
            "min doc-id: 0\n",
            "all docs: 1465\n",
            "all words: 1465\n",
            "docidx unique 50\n",
            "words unique: 325\n",
            "ndocs: 50\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 3, 4, 7, 8, 9, 11, 13]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 49\n",
            "min doc-id: 0\n",
            "all docs: 719\n",
            "all words: 719\n",
            "docidx unique 50\n",
            "words unique: 240\n",
            "ndocs: 50\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 4, 5, 6, 9, 10, 11, 12, 13]\n",
            "max word-id: 346\n",
            "min word-id: 0\n",
            "max doc-id: 49\n",
            "min doc-id: 0\n",
            "all docs: 746\n",
            "all words: 746\n",
            "docidx unique 50\n",
            "words unique: 269\n",
            "ndocs: 50\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "top 10 - word-id of the doc: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "max word-id: 347\n",
            "min word-id: 0\n",
            "max doc-id: 99\n",
            "min doc-id: 0\n",
            "all docs: 2702\n",
            "all words: 2702\n",
            "docidx unique 100\n",
            "words unique: 348\n",
            "ndocs: 100\n",
            "vocab-size: 348\n",
            "finised creating bow input!\n",
            "\n",
            "train-bow-representation for ETM: \n",
            "\n",
            "id2word for ETM: {0: 'university', 1: 'mark', 2: 'international', 3: 'assuming', 4: 'game', 5: 'remember', 6: 'fine', 7: 'related', 8: 'due', 9: 'power', 10: 'pay', 11: 'distribution', 12: 'wondering', 13: 'department', 14: 'stop', 15: 'college', 16: 'advance', 17: 'sort', 18: 'simple', 19: 'error', 20: 'thought', 21: 'steve', 22: 'dept', 23: 'speed', 24: 'cs', 25: 'light', 26: 'front', 27: 'summary', 28: 'technical', 29: 'including', 30: 'idea', 31: 'friend', 32: 'dod', 33: 'thing', 34: 'reading', 35: 'support', 36: 'center', 37: 'hand', 38: 'higher', 39: 'design', 40: 'access', 41: 'job', 42: 'big', 43: 'single', 44: 'takes', 45: 'christian', 46: 'technology', 47: 'change', 48: 'high', 49: 'article', 50: 'jim', 51: 'car', 52: 'au', 53: 'phone', 54: 'strong', 55: 'years', 56: 'called', 57: 'lot', 58: 'care', 59: 'minutes', 60: 'org', 61: 'hold', 62: 'white', 63: 'usa', 64: 'laboratory', 65: 'washington', 66: 'posted', 67: 'posting', 68: 'sci', 69: 'possibly', 70: 'matter', 71: 'general', 72: 'sound', 73: 'receive', 74: 'law', 75: 'group', 76: 'late', 77: 'east', 78: 'gun', 79: 'months', 80: 'engineering', 81: 'close', 82: 'work', 83: 'sell', 84: 'deleted', 85: 'nice', 86: 'form', 87: 'considered', 88: 'appreciated', 89: 'wrong', 90: 'drive', 91: 'guess', 92: 'hp', 93: 'list', 94: 'top', 95: 'stuff', 96: 'data', 97: 'program', 98: 'claim', 99: 'small', 100: 'write', 101: 'corporation', 102: 'religion', 103: 'agree', 104: 'windows', 105: 'short', 106: 'source', 107: 'email', 108: 'feel', 109: 'run', 110: 'current', 111: 'nntp', 112: 'rest', 113: 'net', 114: 'gov', 115: 'nasa', 116: 'buy', 117: 'expressed', 118: 'written', 119: 'keywords', 120: 'ago', 121: 'project', 122: 'box', 123: 'card', 124: 'couple', 125: 'pretty', 126: 'calls', 127: 'large', 128: 'science', 129: 'disk', 130: 'copy', 131: 'today', 132: 'fax', 133: 'mike', 134: 'give', 135: 'important', 136: 'ram', 137: 'heard', 138: 'true', 139: 'hell', 140: 'situation', 141: 'word', 142: 'canada', 143: 'place', 144: 'easy', 145: 'originator', 146: 'image', 147: 'ac', 148: 'reason', 149: 'computer', 150: 'truth', 151: 'open', 152: 'news', 153: 'opinion', 154: 'history', 155: 'result', 156: 'find', 157: 'means', 158: 'final', 159: 'case', 160: 'post', 161: 'class', 162: 'figure', 163: 'opinions', 164: 'order', 165: 'california', 166: 'taking', 167: 'things', 168: 'asked', 169: 'city', 170: 'major', 171: 'de', 172: 'chip', 173: 'newsreader', 174: 'hope', 175: 'sense', 176: 'religious', 177: 'problems', 178: 'children', 179: 'uk', 180: 'issue', 181: 'contact', 182: 'world', 183: 'type', 184: 'book', 185: 'early', 186: 'required', 187: 'ibm', 188: 'questions', 189: 'understand', 190: 'james', 191: 'worth', 192: 'makes', 193: 'texas', 194: 'home', 195: 'radio', 196: 'company', 197: 'file', 198: 'mind', 199: 'free', 200: 'toronto', 201: 'year', 202: 'pc', 203: 'told', 204: 'making', 205: 'view', 206: 'call', 207: 'information', 208: 'public', 209: 'bit', 210: 'good', 211: 'interested', 212: 'guy', 213: 'thinking', 214: 'problem', 215: 'rate', 216: 'part', 217: 'sale', 218: 'time', 219: 'robert', 220: 'board', 221: 'continue', 222: 'start', 223: 'machine', 224: 'found', 225: 'bill', 226: 'simply', 227: 'host', 228: 'sun', 229: 'read', 230: 'cc', 231: 'hardware', 232: 'gas', 233: 'reply', 234: 'ms', 235: 'disclaimer', 236: 'software', 237: 'made', 238: 'real', 239: 'fact', 240: 'day', 241: 'memory', 242: 'long', 243: 'internet', 244: 'western', 245: 'person', 246: 'similar', 247: 'days', 248: 'worse', 249: 'info', 250: 'full', 251: 'service', 252: 'writes', 253: 'network', 254: 'rights', 255: 'dos', 256: 'great', 257: 'set', 258: 'love', 259: 'bike', 260: 'hard', 261: 'apr', 262: 'make', 263: 'mail', 264: 'state', 265: 'key', 266: 'mac', 267: 'money', 268: 'put', 269: 'mine', 270: 'date', 271: 'faster', 272: 'folks', 273: 'results', 274: 'parts', 275: 'number', 276: 'division', 277: 'end', 278: 'effect', 279: 'question', 280: 'response', 281: 'answer', 282: 'note', 283: 'national', 284: 'experience', 285: 'bible', 286: 'times', 287: 'total', 288: 'display', 289: 'wanted', 290: 'hear', 291: 'week', 292: 'numbers', 293: 'school', 294: 'tin', 295: 'live', 296: 'god', 297: 'tom', 298: 'coming', 299: 'space', 300: 'systems', 301: 'research', 302: 'population', 303: 'left', 304: 'low', 305: 'man', 306: 'side', 307: 'people', 308: 'miles', 309: 'lost', 310: 'interesting', 311: 'computing', 312: 'david', 313: 'cost', 314: 'code', 315: 'wrote', 316: 'system', 317: 'based', 318: 'address', 319: 'control', 320: 'government', 321: 'life', 322: 'send', 323: 'mentioned', 324: 'point', 325: 'kind', 326: 'bad', 327: 'ca', 328: 'references', 329: 'interest', 330: 'common', 331: 'understanding', 332: 'jews', 333: 'line', 334: 'exists', 335: 'application', 336: 'talking', 337: 'original', 338: 'message', 339: 'add', 340: 'version', 341: 'clear', 342: 'price', 343: 'turn', 344: 'john', 345: 'back', 346: 'show', 347: 'institute'}\n",
            "compare lda and etm representation: \n",
            "\n",
            "[4, 5, 6, 9, 14, 16, 21, 22, 25, 26, 30, 34, 41, 42, 45, 46, 49, 51, 52, 54, 55, 56, 57, 58, 59, 65, 66, 69, 70, 72, 73, 77, 78, 82, 85, 87, 90, 91, 92, 93, 94, 96, 98, 99, 101, 105, 107, 110, 111, 112, 114, 118, 120, 122, 125, 127, 128, 131, 134, 138, 139, 143, 145, 147]\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "[  6  46  49  72  94  99 104 113 117 122 163 182 188 214 218 234 243 252\n",
            " 255 281 303 319 347]\n",
            "[1 2 2 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 3 2 1 1 2]\n",
            "64\n",
            "23\n",
            "====================================================================================================\n",
            "Size of the vocabulary after prprocessing ist: 348\n",
            "Size of train set: 150\n",
            "Size of val set: 100\n",
            "Size of test set: 50\n",
            "====================================================================================================\n",
            "['dos', 'windows', 'institute', 'technology', 'article', 'writes', 'small', 'dos', 'small', 'dos', 'answer', 'ms', 'windows', 'world', 'sound', 'answer', 'fine', 'questions', 'net', 'article', 'time', 'problem', 'box', 'control', 'top', 'left', 'institute', 'technology', 'opinions', 'expressed', 'internet']\n"
          ]
        }
      ]
    }
  ]
}