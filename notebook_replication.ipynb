{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hanhluukim/replication-topic-modelling-in-embedding-space/blob/main/notebook_replication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ7P852F7yzU"
      },
      "source": [
        "# **Das Projekt aus dem Github klonen und in den Projektsordner**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riOxinNHJcIB",
        "outputId": "1484c168-f9bf-4efe-f8b7-d9a6f2f8b593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'replication-topic-modelling-in-embedding-space'...\n",
            "remote: Enumerating objects: 445, done.\u001b[K\n",
            "remote: Counting objects: 100% (220/220), done.\u001b[K\n",
            "remote: Compressing objects: 100% (159/159), done.\u001b[K\n",
            "remote: Total 445 (delta 137), reused 132 (delta 58), pack-reused 225\u001b[K\n",
            "Receiving objects: 100% (445/445), 4.98 MiB | 5.26 MiB/s, done.\n",
            "Resolving deltas: 100% (247/247), done.\n"
          ]
        }
      ],
      "source": [
        "#wenn die Ordner noch nicht geklont ist, soll dieser Fehler zuerst durchgeführt werden.\n",
        "!git clone https://github.com/hanhluukim/replication-topic-modelling-in-embedding-space.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_6em-5qJg5e",
        "outputId": "2c6cd297-f85e-4f8d-875c-79f167cfeaf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/replication-topic-modelling-in-embedding-space\n"
          ]
        }
      ],
      "source": [
        "cd /content/replication-topic-modelling-in-embedding-space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AAG98vV1JCg"
      },
      "source": [
        "#**Die benötige Paketen für das Projekt mittels requirements.txt installieren**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcBay625sD5D",
        "outputId": "75a745f6-13fa-4881-a474-9875601d0a28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 1)) (3.6.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 2)) (3.2.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 3)) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 4)) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 6)) (1.11.0+cu113)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (4.18.0)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 8)) (0.5.3)\n",
            "Requirement already satisfied: plotly==5.7.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 9)) (5.7.0)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 10)) (1.0.1)\n",
            "Requirement already satisfied: pyyaml==5.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 11)) (5.4.1)\n",
            "Requirement already satisfied: kaleido in /usr/local/lib/python3.7/dist-packages (from -r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 12)) (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly==5.7.0->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 9)) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly==5.7.0->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 9)) (8.0.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 1)) (6.0.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 6)) (4.2.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (0.5.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (0.0.53)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (3.0.8)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 8)) (0.5.6)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 8)) (0.51.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 8)) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 8)) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r /content/replication-topic-modelling-in-embedding-space/requirements.txt (line 7)) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Falls die Packages noch nicht installiert wurden, \n",
        "!pip install -r \"/content/replication-topic-modelling-in-embedding-space/requirements.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7xiPgja8eZe"
      },
      "source": [
        "# **Gebrauchte Paketen importieren**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uV7KZhGq1P7g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import umap.umap_ as umap\n",
        "import time\n",
        "import plotly.express as px\n",
        "from sklearn import cluster\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzWqQhPQdJWV"
      },
      "source": [
        "# **Vorverarbeitung und BOW-Repräsentationen für Textdaten durchführen**\n",
        "1. Vocabular erstellen\n",
        "2. BOW-Repräsentationen für allen Teildatensätzen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1OCULr82pfgk"
      },
      "outputs": [],
      "source": [
        "from src.prepare_dataset import TextDataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy0PpjxEpbrR",
        "outputId": "5f9eab28-6cc7-402f-febc-d866bc0d967f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading texts: ...\n",
            "finished load!\n",
            "check some sample texts of the dataset\n",
            "['From', ':', 'lerxst', '@', 'wam', '.', 'umd', '.', 'edu', '(', \"where's\", 'my', 'thing', ')', 'Subject', ':', 'WHAT', 'car', 'is', 'this', '!', '?', 'Nntp', 'Posting', 'Host', ':', 'rac3', '.', 'wam', '.', 'umd', '.', 'edu', 'Organization', ':', 'University', 'of', 'Maryland', ',', 'College', 'Park', 'Lines', ':', '15', 'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.', 'It', 'was', 'a', '2', 'door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s', '/', 'early', '70s', '.', 'It', 'was', 'called', 'a', 'Bricklin', '.', 'The', 'doors', 'were', 'really', 'small', '.', 'In', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'This', 'is', 'all', 'I', 'know', '.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e', 'mail', '.', 'Thanks', ',', 'IL', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst']\n",
            "====================================================================================================\n",
            "['From', ':', 'guykuo', '@', 'carson', '.', 'u', '.', 'washington', '.', 'edu', '(', 'Guy', 'Kuo', ')', 'Subject', ':', 'SI', 'Clock', 'Poll', 'Final', 'Call', 'Summary', ':', 'Final', 'call', 'for', 'SI', 'clock', 'reports', 'Keywords', ':', 'SI', ',', 'acceleration', ',', 'clock', ',', 'upgrade', 'Article', 'I', '.', 'D', '.', ':', 'shelley', '.', '1qvfo9INNc3s', 'Organization', ':', 'University', 'of', 'Washington', 'Lines', ':', '11', 'NNTP', 'Posting', 'Host', ':', 'carson', '.', 'u', '.', 'washington', '.', 'edu', 'A', 'fair', 'number', 'of', 'brave', 'souls', 'who', 'upgraded', 'their', 'SI', 'clock', 'oscillator', 'have', 'shared', 'their', 'experiences', 'for', 'this', 'poll', '.', 'Please', 'send', 'a', 'brief', 'message', 'detailing', 'your', 'experiences', 'with', 'the', 'procedure', '.', 'Top', 'speed', 'attained', ',', 'CPU', 'rated', 'speed', ',', 'add', 'on', 'cards', 'and', 'adapters', ',', 'heat', 'sinks', ',', 'hour', 'of', 'usage', 'per', 'day', ',', 'floppy', 'disk', 'functionality', 'with', '800', 'and', '1', '.', '4', 'm', 'floppies', 'are', 'especially', 'requested', '.', 'I', 'will', 'be', 'summarizing', 'in', 'the', 'next', 'two', 'days', ',', 'so', 'please', 'add', 'to', 'the', 'network', 'knowledge', 'base', 'if', 'you', 'have', 'done', 'the', 'clock', 'upgrade', 'and', \"haven't\", 'answered', 'this', 'poll', '.', 'Thanks', '.', 'Guy', 'Kuo', '<', 'guykuo', '@', 'u', '.', 'washington', '.', 'edu', '>']\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "# init TextDataLoader für die Datenquelle 20 News Groups\n",
        "# Daten abrufen vom Sklearn, tokenisieren und besondere Charaktern entfernen\n",
        "textsloader = TextDataLoader(source=\"20newsgroups\", train_size=None, test_size=None)\n",
        "textsloader.load_tokenize_texts(\"20newsgroups\")\n",
        "# Beispiel von Textdaten\n",
        "textsloader.show_example_raw_texts(n_docs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5odpQDJ7qPTt",
        "outputId": "6b8c1383-2b0f-4891-8511-df4575e66b9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start: preprocessing: ...\n",
            "finised: preprocessing!\n"
          ]
        }
      ],
      "source": [
        "# Vorverarbeitung von Daten mit folgenden Schritten:\n",
        "textsloader.preprocess_texts(length_one_remove=True, punctuation_lower = True, stopwords_filter = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRRCNPa9qXfq",
        "outputId": "916169a7-6e8b-4a7e-8b0c-8b890b6cd360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test-document-frequency: \n",
            "[[ 15  17  12  18  11  16  14  20  12  17  19 135  16  10  15  36  15  19\n",
            "   11  21  11  10  35  10  13  17  21  54  10  30  24  10  10  15  13  12\n",
            "   31  29  17  14  10  14  10  10  11  12  12  14  10  17  13  11  13  57\n",
            "   10  13  12  10  27  12  11  22  19  45  18  20  13  19  21  15  14  13\n",
            "   19  17  12  16  11  11  62  12  10  10  17  13  11  10  14  11  28  23\n",
            "   14  12  10  11  11  30  10  19  16  12  14  10  35  12  13  11  21  17\n",
            "   12  13  13  10  10  15  22  19  46  14  13  28  26  16  10  17  16  25\n",
            "   11  10  23  10  17  10  12  10  10  13 136  13  11  17  13  17  11  22\n",
            "   23  12  10  14  11  11  19  11  17  11  10  12  22  12  29  18  11  15\n",
            "   11  14  18  21  11  21  10  14  29  10  21  13  14  12  12  26  31  17\n",
            "   48  14  10  13  16  14  21  16  12  21  12  10  17  16  10  18  18  11\n",
            "   14  21  18  14  32  19  14 135  14  30  13  14  12  23  12  14  11  10\n",
            "   25  12  10  13  67  11  21  23  37  10  10  25  16 141  21  13  16  39\n",
            "   26  22  12  16  24  43  10  14  10  10  27  11  33  17  10  10  10  15\n",
            "   10  17  71  11  19  14  11  11  12  10  15  26  11  10  14  24  10  15\n",
            "   13  13  16  12  12  16  11  16  10  15  12  16  26  13  10  14  16  18\n",
            "   14  36  16  10  14  18  19  18  14  35  19  10  10  14  10  24  12  26\n",
            "   28  11  22  67  21  15  21  12  11  10  13  12  27  11  17  10  20  18\n",
            "   11 125  43  27  17  11  11  13  12  12  21  12  15  32  52  10  10  14\n",
            "  149  11  15  25  23  29]]\n",
            "vocab-size in df: 348\n",
            "start creating vocabulary ...\n",
            "length of the vocabulary: 348\n",
            "sample ten words of the vocabulary: ['days', 'feel', 'thing', 'early', 'kind', 'de', 'cost', 'late', 'higher', 'power']\n",
            "length word2id list: 348\n",
            "length id2word list: 348\n",
            "finished: creating vocabulary\n"
          ]
        }
      ],
      "source": [
        "# Daten zerlegen für Train, Test und Validation. Erstellen Vocabular aus dem Trainset\n",
        "textsloader.split_and_create_voca_from_trainset(max_df=0.7, min_df=10, stopwords_remove_from_voca=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etzyjh_nqi19",
        "outputId": "7b9d79f1-bacb-4e28-e08c-70090e802fbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length train-documents-indices : 4275\n",
            "length of the vocabulary: 348\n",
            "\n",
            "\n",
            "start: creating bow representation...\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "finised creating bow input!\n",
            "\n",
            "start: creating bow representation...\n",
            "finised creating bow input!\n",
            "\n",
            "id2word befor saving: dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347])\n"
          ]
        }
      ],
      "source": [
        "# Erstellen BOW-Repräsentation für ETM Modell\n",
        "for_lda_model = False \n",
        "word2id, id2word, train_set, test_set, val_set = textsloader.create_bow_and_savebow_for_each_set(for_lda_model=for_lda_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-DXUMguC8zM"
      },
      "source": [
        "# **Vocabular und IDs anzeigen als Beispiel**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "6RBJYhLHCfwy",
        "outputId": "6227b3e7-09fe-4544-d563-e9d6018edecf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        word  id\n",
              "0       days   0\n",
              "1       feel   1\n",
              "2      thing   2\n",
              "3      early   3\n",
              "4       kind   4\n",
              "..       ...  ..\n",
              "95   support  95\n",
              "96      line  96\n",
              "97  assuming  97\n",
              "98   general  98\n",
              "99  thinking  99\n",
              "\n",
              "[100 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2cc23c82-f297-41d9-a15b-785f9114343a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>days</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>feel</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>thing</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>early</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>kind</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>support</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>line</td>\n",
              "      <td>96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>assuming</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>general</td>\n",
              "      <td>98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>thinking</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2cc23c82-f297-41d9-a15b-785f9114343a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2cc23c82-f297-41d9-a15b-785f9114343a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2cc23c82-f297-41d9-a15b-785f9114343a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# show for samples: 100 word2id and id2 word\n",
        "word2id_df_100 = pd.DataFrame()\n",
        "word2id_df_100['word'] = list(word2id.keys())[:100]\n",
        "word2id_df_100['id'] = list(word2id.values())[:100]\n",
        "word2id_df_100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tupeI6Pw85_L"
      },
      "source": [
        "# **Die Größe von Datensätzen kontrollieren**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1d-5ji3qwE8",
        "outputId": "f881652c-86ff-4475-8571-1581c72b315a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the vocabulary after prprocessing ist: 348\n",
            "Size of train set: 139\n",
            "Size of val set: 101\n",
            "Size of test set: 60\n"
          ]
        }
      ],
      "source": [
        "# Kontrollieren die Größen von verschiedenen Datensätzen\n",
        "print(f'Size of the vocabulary after prprocessing ist: {len(textsloader.vocabulary)}')\n",
        "print(f'Size of train set: {len(train_set[\"tokens\"])}')\n",
        "print(f'Size of val set: {len(val_set[\"tokens\"])}')\n",
        "print(f'Size of test set: {len(test_set[\"test\"][\"tokens\"])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxQL5jQtDb1c"
      },
      "source": [
        "# **Dokumenten wiederstellen für Word2Vec Embedding**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "PDXEEBHfq3Cy",
        "outputId": "c34260c0-ad32-42cd-8f2c-4d38b7e76dc3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             text-after-preprocessing\n",
              "0   reply college disclaimer care info robert writ...\n",
              "1   cs university department computer science figu...\n",
              "2   card questions reply bit ram bit true general ...\n",
              "3   article institute nntp posting host article wr...\n",
              "4   call originator ca institute send internationa...\n",
              "..                                                ...\n",
              "95  nasa gov laboratory distribution world nntp po...\n",
              "96  wanted nntp posting host institute distributio...\n",
              "97  article apr writes expressed major university ...\n",
              "98  computer systems division distribution world n...\n",
              "99  james read nntp posting host engineering compu...\n",
              "\n",
              "[100 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-78154304-a32e-4186-b4d0-ee84c2ee0982\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text-after-preprocessing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>reply college disclaimer care info robert writ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cs university department computer science figu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>card questions reply bit ram bit true general ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>article institute nntp posting host article wr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>call originator ca institute send internationa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>nasa gov laboratory distribution world nntp po...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>wanted nntp posting host institute distributio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>article apr writes expressed major university ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>computer systems division distribution world n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>james read nntp posting host engineering compu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 1 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-78154304-a32e-4186-b4d0-ee84c2ee0982')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-78154304-a32e-4186-b4d0-ee84c2ee0982 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-78154304-a32e-4186-b4d0-ee84c2ee0982');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# re-erstellen von Dokumenten nach der Vorverarbeitungen. Die Dokumenten sind in Wörtern und werden für Word-Embedding Training benutzt\n",
        "docs_tr, docs_t, docs_v = textsloader.get_docs_in_words_for_each_set()\n",
        "train_docs_df = pd.DataFrame()\n",
        "train_docs_df['text-after-preprocessing'] = [' '.join(doc) for doc in docs_tr[:100]]\n",
        "train_docs_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds_KuUTQrK5P"
      },
      "source": [
        "# **Word-Embedding trainieren mit dem Traindatensatz**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KBexKIVf8Qs5",
        "outputId": "f723ae32-0b93-4942-8797-ba54c463b655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word-embedding train begins\n",
            "word-embedding train finished\n",
            "length of vocabulary from word-embedding model 348\n",
            "length of the vocabulary of prepraring-dataset-vocabulary: 348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 348/348 [00:00<00:00, 15801.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster id labels for inputted data\n",
            "[5 4 3 6 0 3 5 8 1 2 3 7 2 1 7 1 7 6 2 5 9 8 5 9 0 1 5 9 7 0 9 8 7 3 7 2 0\n",
            " 0 2 6 7 1 6 3 5 3 1 0 5 8 5 7 7 0 2 2 7 4 8 6 1 0 7 9 1 6 9 7 7 4 9 3 3 9\n",
            " 7 8 9 6 3 2 7 7 2 8 7 3 4 2 8 9 0 5 8 7 6 0 4 7 0 0 2 3 2 9 3 8 3 0 4 1 6\n",
            " 8 0 6 6 6 6 7 0 7 0 0 9 7 5 7 7 0 6 6 9 2 0 9 8 7 8 2 0 9 6 8 9 8 6 9 5 1\n",
            " 6 2 9 2 8 7 5 6 3 0 7 9 4 2 0 9 0 1 8 6 7 2 0 7 9 1 1 5 5 4 9 8 1 5 0 3 1\n",
            " 5 6 8 9 8 3 2 5 7 5 4 9 4 7 7 1 1 6 0 3 4 1 0 3 3 7 4 8 9 1 4 9 5 0 4 8 3\n",
            " 1 5 1 3 1 2 9 9 6 2 5 7 4 2 1 4 8 2 1 9 6 5 6 5 8 4 7 9 5 3 7 4 1 1 7 4 3\n",
            " 8 1 8 1 6 9 4 1 3 0 7 6 8 9 1 9 9 0 1 1 9 5 8 9 3 1 8 7 9 4 9 1 6 8 2 0 8\n",
            " 3 0 7 1 0 5 4 6 0 0 3 4 1 4 6 1 7 1 3 0 7 7 6 7 9 6 4 2 7 0 9 8 9 1 3 2 8\n",
            " 8 4 4 2 5 9 0 8 4 0 3 8 4 4 6]\n",
            "Centroids data\n",
            "[[-0.00015533 -0.03818461 -0.02884949 -0.05768432 -0.00089922 -0.04229567\n",
            "   0.00056679 -0.02797605  0.02412334 -0.00528557]\n",
            " [-0.04297236 -0.04285543 -0.02629198 -0.03397148  0.03292746 -0.02319951\n",
            "   0.01222118 -0.00977055  0.01475015 -0.03388542]\n",
            " [ 0.0065378  -0.0101162  -0.0122005  -0.05937604  0.0457892  -0.03695637\n",
            "  -0.00313018  0.02329175 -0.01479102  0.00321804]\n",
            " [-0.01337276 -0.05684344 -0.02288348 -0.02787767  0.00756714 -0.03155638\n",
            "  -0.02603648  0.03365957  0.01098453 -0.00710044]\n",
            " [-0.01863271 -0.02097239 -0.0040989  -0.0083971  -0.00010862 -0.01295733\n",
            "   0.00486305 -0.0142475  -0.01579511  0.00736274]\n",
            " [-0.04349829 -0.07467752 -0.05369901 -0.09098532  0.04476547 -0.05685251\n",
            "   0.00781686 -0.00503018  0.01731001 -0.00269798]\n",
            " [-0.01079555 -0.04746333 -0.03881958 -0.03758012  0.02137942 -0.00156011\n",
            "   0.01482335  0.00454361  0.03378056  0.01891667]\n",
            " [-0.01173711 -0.06865444 -0.03931483 -0.10270645  0.0340522  -0.01956763\n",
            "   0.01162111  0.02706629  0.02568569 -0.0336346 ]\n",
            " [-0.00734533 -0.00105737 -0.02202904 -0.0364768  -0.00777376 -0.0145476\n",
            "  -0.01403223  0.02198618  0.02348616 -0.02823266]\n",
            " [-0.02131951 -0.02327591 -0.00599794 -0.08573775  0.03252946  0.00335109\n",
            "   0.00816861 -0.01384456  0.00916131 -0.00749653]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  warnings.warn(problem)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.11.1.min.js\"></script>                <div id=\"251347ec-6ac4-449b-a816-e35e53bd73dd\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"251347ec-6ac4-449b-a816-e35e53bd73dd\")) {                    Plotly.newPlot(                        \"251347ec-6ac4-449b-a816-e35e53bd73dd\",                        [{\"hovertemplate\":\"cluster=cluster 5<br>x=%{x}<br>y=%{y}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 5\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 5\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[\"reply\",\"writes\",\"cs\",\"computer\",\"place\",\"price\",\"article\",\"nntp\",\"question\",\"things\",\"time\",\"nasa\",\"hard\",\"children\",\"real\",\"buy\",\"world\",\"power\",\"making\",\"sun\",\"center\",\"ac\",\"days\",\"put\",\"speed\",\"design\",\"start\"],\"x\":[1.9639824628829956,1.5783113241195679,2.144157886505127,1.8557934761047363,2.087235450744629,3.4200828075408936,0.9633592367172241,-0.6816681623458862,1.5968637466430664,1.8468323945999146,2.22346568107605,0.6652584671974182,-1.0449694395065308,0.836847186088562,2.299436092376709,0.1213088110089302,0.07859467715024948,1.1286771297454834,0.36972764134407043,1.992509126663208,0.5404238104820251,1.5470069646835327,0.35331040620803833,0.3228796720504761,-1.543369174003601,2.19792103767395,0.31171005964279175],\"xaxis\":\"x\",\"y\":[1.6304506063461304,1.6909865140914917,1.9911023378372192,2.5567963123321533,2.9973344802856445,5.404976844787598,1.8007785081863403,3.481208324432373,2.8629605770111084,2.3683788776397705,1.7762272357940674,2.139645576477051,4.095253944396973,1.9097843170166016,2.189309597015381,2.1704230308532715,2.4196417331695557,2.4374849796295166,2.46645188331604,1.7725787162780762,2.258638858795166,2.842388391494751,2.992704153060913,2.041877031326294,5.796121120452881,3.247285842895508,2.1749162673950195],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"cluster=cluster 4<br>x=%{x}<br>y=%{y}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 4\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 4\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[\"college\",\"mine\",\"originator\",\"problems\",\"kind\",\"interest\",\"school\",\"top\",\"makes\",\"government\",\"sci\",\"considered\",\"possibly\",\"truth\",\"original\",\"worse\",\"fax\",\"sale\",\"technical\",\"nice\",\"disk\",\"love\",\"total\",\"related\",\"exists\",\"sell\",\"copy\",\"hear\",\"jim\",\"interested\"],\"x\":[0.42747893929481506,-0.21718131005764008,2.0371153354644775,-0.620103657245636,0.04208001866936684,0.5690614581108093,-1.1787980794906616,0.443864107131958,-0.3516724705696106,-0.5272284150123596,0.33084532618522644,0.6807640194892883,1.1056495904922485,0.444652795791626,1.6536768674850464,0.7395321726799011,-0.7866154909133911,0.5373460650444031,0.5092962384223938,0.598143994808197,0.3586170971393585,0.3093484044075012,0.4803164601325989,2.3792691230773926,2.6848959922790527,0.6520385146141052,2.4291017055511475,0.21062123775482178,0.9095193147659302,0.08228996396064758],\"xaxis\":\"x\",\"y\":[4.9869704246521,5.314878940582275,5.651787281036377,4.5948805809021,5.561951160430908,5.326291084289551,4.659198760986328,5.647376537322998,5.0002875328063965,4.900578498840332,5.435352802276611,5.215939044952393,6.334959983825684,6.013979434967041,6.001391887664795,5.1055707931518555,4.6849894523620605,5.707695960998535,5.871650695800781,4.870500087738037,4.955966472625732,5.689868450164795,5.238314628601074,6.106987953186035,6.076469898223877,5.954854965209961,5.872574329376221,4.756507396697998,4.940854549407959,5.0995774269104],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"cluster=cluster 3<br>x=%{x}<br>y=%{y}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 3\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 3\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[\"disclaimer\",\"robert\",\"give\",\"ram\",\"opinion\",\"pay\",\"send\",\"international\",\"part\",\"called\",\"hand\",\"written\",\"person\",\"keywords\",\"car\",\"class\",\"company\",\"david\",\"mark\",\"list\",\"wrong\",\"address\",\"short\",\"form\",\"continue\",\"image\",\"full\",\"deleted\",\"sort\",\"reading\"],\"x\":[-0.05614689737558365,-1.2047733068466187,-0.4961085021495819,1.6255414485931396,-0.282210111618042,1.4112892150878906,-0.09284531325101852,-0.37269288301467896,2.9386773109436035,1.2121158838272095,-0.02796962670981884,0.12913775444030762,-1.0274016857147217,0.6578155159950256,0.21931226551532745,0.2936153709888458,-1.2374439239501953,-1.41025710105896,1.525956153869629,-0.2879451513290405,-0.05328835919499397,-1.543874979019165,0.03243179991841316,-0.7742758989334106,-1.0414949655532837,-0.5724592208862305,-0.16786596179008484,-0.05373043194413185,0.32253292202949524,2.351651668548584],\"xaxis\":\"x\",\"y\":[6.705934047698975,6.019787788391113,6.166889667510986,6.187698841094971,5.547030448913574,6.557744026184082,5.748095512390137,6.364375114440918,2.9142773151397705,5.824690341949463,6.7278828620910645,5.780121326446533,5.445294380187988,6.345611095428467,2.465789794921875,6.4246015548706055,5.317927360534668,5.044714450836182,5.480835914611816,6.4526801109313965,6.146060466766357,5.432057857513428,6.6824259757995605,6.1713972091674805,6.303089141845703,6.26833438873291,6.378800868988037,6.376184940338135,7.2699737548828125,5.951656818389893],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"cluster=cluster 6<br>x=%{x}<br>y=%{y}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 6\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 6\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[\"care\",\"mac\",\"problem\",\"strong\",\"machine\",\"stuff\",\"week\",\"life\",\"end\",\"home\",\"distribution\",\"cc\",\"org\",\"email\",\"summary\",\"rate\",\"sound\",\"software\",\"hp\",\"service\",\"late\",\"gas\",\"guy\",\"thing\",\"minutes\",\"bible\",\"point\",\"light\",\"based\",\"word\",\"months\",\"source\",\"ms\"],\"x\":[-0.5680135488510132,0.6912697553634644,-0.0024996378924697638,2.121338129043579,-1.0669925212860107,-0.9901085495948792,-1.1250731945037842,-0.5545064806938171,-1.3455547094345093,-0.3847489655017853,-0.058061808347702026,-1.0665968656539917,-0.9635012149810791,-0.7683612704277039,1.5448644161224365,-1.2708135843276978,-0.0016292817890644073,-0.8212447166442871,-0.8614465594291687,-0.8160811066627502,-0.03666405379772186,0.6751846075057983,1.7661138772964478,-1.4184608459472656,-0.799251914024353,-1.2517699003219604,1.8884522914886475,0.03493857756257057,0.06685152649879456,0.5321734547615051,1.3623650074005127,-0.10294204205274582,-0.9107458591461182],\"xaxis\":\"x\",\"y\":[5.03255033493042,3.970874786376953,3.387028932571411,3.9689018726348877,5.143050670623779,5.646829605102539,5.570528507232666,4.689548015594482,5.5491862297058105,4.856110572814941,3.8081629276275635,5.513924598693848,5.347096920013428,5.933043479919434,3.6738314628601074,4.946547031402588,4.343115329742432,5.68299674987793,5.3838911056518555,5.368211269378662,3.725088596343994,4.012173175811768,3.702744245529175,5.527500629425049,5.783758640289307,5.7829179763793945,3.7931411266326904,3.545952558517456,3.395301342010498,4.054022312164307,4.345813274383545,4.541240692138672,5.478293418884277],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"cluster=cluster 0<br>x=%{x}<br>y=%{y}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 0\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 0\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[\"info\",\"figure\",\"set\",\"heard\",\"run\",\"early\",\"wrote\",\"long\",\"asked\",\"live\",\"advance\",\"appreciated\",\"common\",\"means\",\"single\",\"toronto\",\"radio\",\"high\",\"gov\",\"hardware\",\"dept\",\"hope\",\"display\",\"windows\",\"money\",\"agree\",\"steve\",\"public\",\"religious\",\"key\",\"man\",\"references\",\"uk\",\"board\",\"higher\",\"dod\",\"including\",\"city\",\"friend\"],\"x\":[-1.0984852313995361,-1.0364667177200317,-0.961067795753479,-1.532275676727295,-1.228978157043457,-1.4473707675933838,1.274391770362854,0.8243876099586487,0.375224769115448,2.871337652206421,-1.295603632926941,-0.4463643729686737,-1.3231325149536133,0.9020398259162903,0.7225569486618042,-0.08010746538639069,1.206796407699585,-1.1152081489562988,0.9049670100212097,2.7742602825164795,1.0375319719314575,0.9034359455108643,-1.4087413549423218,0.4580889642238617,0.8043579459190369,0.7403398752212524,-1.1948330402374268,-1.2157318592071533,2.549006223678589,0.08553484827280045,-1.346612572669983,1.305117130279541,3.120497941970825,1.1172493696212769,0.35138288140296936,-1.291850209236145,1.1391046047210693,0.8493355512619019,0.15246674418449402],\"xaxis\":\"x\",\"y\":[6.235599994659424,4.019726753234863,4.801090717315674,4.48966646194458,4.315837383270264,4.593899250030518,2.340468168258667,2.8168797492980957,3.568044900894165,6.235671520233154,4.424557209014893,6.020977020263672,4.52567195892334,2.940629482269287,2.8765580654144287,4.12013578414917,3.372555732727051,4.491026401519775,3.2318665981292725,4.156561851501465,3.117063283920288,3.220632314682007,4.352969646453857,3.287006378173828,6.713693141937256,2.9694905281066895,4.013117790222168,4.254019737243652,6.3791022300720215,4.002467155456543,4.462428092956543,3.287158727645874,5.892453193664551,4.630023002624512,2.6327295303344727,4.812483787536621,3.3648483753204346,3.724830150604248,4.108778476715088],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"cluster=cluster 8<br>x=%{x}<br>y=%{y}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 8\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 8\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[\"understand\",\"department\",\"questions\",\"institute\",\"close\",\"contact\",\"interesting\",\"mind\",\"show\",\"final\",\"open\",\"day\",\"expressed\",\"easy\",\"folks\",\"numbers\",\"bill\",\"rights\",\"order\",\"type\",\"clear\",\"corporation\",\"stop\",\"guess\",\"canada\",\"due\",\"effect\",\"lost\",\"required\",\"james\",\"christian\",\"takes\",\"de\",\"claim\",\"experience\",\"chip\"],\"x\":[2.2696783542633057,0.5529180765151978,-0.9433590173721313,2.474905252456665,0.6833004951477051,0.32380014657974243,0.6328008770942688,2.5055036544799805,0.3068661093711853,2.868375778198242,2.560023784637451,0.29291921854019165,2.2752010822296143,0.9102343916893005,0.16450490057468414,2.450721502304077,1.33955979347229,0.284819632768631,2.257208824157715,1.4417402744293213,2.2075514793395996,0.2952771782875061,2.365628719329834,2.6637284755706787,1.4809409379959106,0.42753565311431885,0.733511745929718,0.6672900915145874,2.4028677940368652,0.316101998090744,0.4843440651893616,1.2313580513000488,0.45826420187950134,2.6518664360046387,1.7325513362884521,1.4147204160690308],\"xaxis\":\"x\",\"y\":[6.519789218902588,7.253093242645264,6.255582809448242,7.16338586807251,6.179361820220947,3.3495302200317383,7.344351291656494,6.518992900848389,6.7266082763671875,6.345136642456055,7.06608772277832,6.877704620361328,7.1514177322387695,7.300487041473389,4.3107709884643555,7.045943737030029,5.439257621765137,4.585192680358887,7.206357479095459,7.146414756774902,6.610021591186523,4.464565753936768,6.682858943939209,6.585036754608154,5.831775665283203,6.736098766326904,7.235166072845459,6.787487983703613,7.029919624328613,7.076840877532959,6.789436340332031,7.188920974731445,6.427217960357666,6.251994609832764,6.7481842041015625,7.132170677185059],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"cluster=cluster 1<br>x=%{x}<br>y=%{y}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 1\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 1\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[\"current\",\"memory\",\"today\",\"make\",\"important\",\"lot\",\"file\",\"message\",\"job\",\"code\",\"texas\",\"ago\",\"feel\",\"low\",\"thinking\",\"find\",\"support\",\"tom\",\"error\",\"systems\",\"network\",\"matter\",\"national\",\"jews\",\"mike\",\"apr\",\"east\",\"religion\",\"large\",\"parts\",\"game\",\"gun\",\"view\",\"front\",\"told\",\"hell\",\"date\",\"fine\",\"note\"],\"x\":[1.0468322038650513,1.2134171724319458,2.050790309906006,0.7965803146362305,1.4690091609954834,-0.20381993055343628,1.9866114854812622,0.8260522484779358,1.379492163658142,2.1746444702148438,1.724688172340393,1.8668403625488281,1.3165743350982666,3.28542160987854,1.1479583978652954,3.2463998794555664,2.9918205738067627,-0.3960861563682556,0.8926711082458496,1.7728408575057983,2.6915948390960693,1.6287428140640259,2.954819917678833,3.16701340675354,-1.1743428707122803,0.23590421676635742,2.8806324005126953,1.7786805629730225,1.465916633605957,1.6194331645965576,1.938337802886963,1.3782567977905273,1.4992142915725708,1.7737969160079956,0.9932295083999634,-0.9468516111373901,0.5629689693450928,1.2780406475067139,2.5786211490631104],\"xaxis\":\"x\",\"y\":[4.876675605773926,4.330798149108887,5.651379585266113,2.190079689025879,4.40307092666626,5.254297256469727,5.612438201904297,3.950700044631958,5.076791286468506,5.556708335876465,5.547398567199707,4.101866245269775,4.3073344230651855,4.296570301055908,4.420095443725586,5.5131940841674805,5.557315826416016,5.140125751495361,4.095998287200928,4.717427730560303,5.673821926116943,5.0443525314331055,5.52641487121582,5.8211846351623535,6.124125957489014,4.317746639251709,6.02239465713501,4.251994609832764,4.059476852416992,4.540750980377197,5.302262783050537,4.510324954986572,4.143945217132568,5.440976142883301,2.220869541168213,6.345615386962891,4.555136680603027,4.188863277435303,5.232670307159424],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"cluster=cluster 2<br>x=%{x}<br>y=%{y}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 2\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 2\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[\"situation\",\"add\",\"answer\",\"general\",\"write\",\"thought\",\"net\",\"research\",\"mentioned\",\"computing\",\"coming\",\"calls\",\"post\",\"opinions\",\"wanted\",\"california\",\"idea\",\"phone\",\"year\",\"newsreader\",\"response\",\"ibm\",\"similar\",\"hold\",\"engineering\",\"dos\",\"posted\"],\"x\":[1.3254369497299194,-0.8224020600318909,-0.6383351683616638,-0.1454032063484192,-0.8202580809593201,-0.3432976007461548,0.910995364189148,-0.20193874835968018,1.655503511428833,1.2557157278060913,1.8446601629257202,1.0370349884033203,0.9408749341964722,0.8023865818977356,1.4701292514801025,1.1484309434890747,1.4855111837387085,0.6567755937576294,1.0070221424102783,1.7342286109924316,-0.007585565559566021,0.840089738368988,2.380958318710327,-0.7665843963623047,2.1152167320251465,1.1522504091262817,0.9690268039703369],\"xaxis\":\"x\",\"y\":[5.646396636962891,3.0074453353881836,2.8108811378479004,2.8173611164093018,2.9266066551208496,2.740018129348755,6.737311363220215,2.638432025909424,6.465897083282471,5.996652603149414,6.336963653564453,6.41755485534668,7.032430171966553,5.281892776489258,5.882804870605469,6.466805458068848,6.357578754425049,6.676270484924316,3.641822338104248,5.052394866943359,2.9801571369171143,6.066988468170166,4.5391974449157715,2.9044525623321533,4.669241905212402,6.538844108581543,6.4840826988220215],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"cluster=cluster 7<br>x=%{x}<br>y=%{y}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 7\",\"marker\":{\"color\":\"#FF97FF\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 7\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[\"years\",\"faster\",\"system\",\"reason\",\"bit\",\"true\",\"read\",\"posting\",\"host\",\"mail\",\"people\",\"usa\",\"call\",\"information\",\"project\",\"news\",\"great\",\"side\",\"understanding\",\"case\",\"small\",\"work\",\"big\",\"control\",\"simple\",\"laboratory\",\"line\",\"found\",\"division\",\"john\",\"access\",\"result\",\"bad\",\"space\",\"group\",\"major\",\"version\",\"times\",\"taking\",\"simply\",\"talking\",\"law\",\"au\",\"bike\",\"program\"],\"x\":[3.5496082305908203,1.1466764211654663,2.9939334392547607,1.2194950580596924,2.1810755729675293,2.2161765098571777,2.1957061290740967,3.009126901626587,3.5151636600494385,3.446568250656128,2.7236292362213135,2.8333656787872314,1.6895594596862793,2.9083495140075684,2.1308562755584717,2.3591930866241455,2.6685543060302734,3.359361171722412,1.3294012546539307,2.795527219772339,2.847168207168579,2.854154109954834,3.160987138748169,2.2562685012817383,3.0543670654296875,2.3186254501342773,2.5686652660369873,1.2781442403793335,0.16242772340774536,1.9864681959152222,1.4634711742401123,2.5764975547790527,3.409494638442993,2.033071517944336,2.0380566120147705,3.3466060161590576,1.3878201246261597,2.6625256538391113,2.9304096698760986,1.0554146766662598,1.7170031070709229,2.242748737335205,2.245696783065796,1.3126314878463745,1.8575793504714966],\"xaxis\":\"x\",\"y\":[3.8678553104400635,2.2268834114074707,2.512328863143921,2.2454185485839844,3.4387967586517334,1.8177422285079956,2.245671033859253,3.973813533782959,3.516561508178711,3.291553020477295,2.0771195888519287,2.793424606323242,1.6959160566329956,3.2140564918518066,2.7650504112243652,3.175492763519287,2.6160411834716797,3.159961223602295,2.7441508769989014,3.2910284996032715,3.4085323810577393,2.4902331829071045,3.476952075958252,1.9103307723999023,2.7979700565338135,2.0354130268096924,2.3036112785339355,2.719292640686035,3.0007171630859375,2.435293197631836,1.648866057395935,2.7610907554626465,3.391167163848877,2.2270069122314453,2.840719699859619,3.1025819778442383,1.7819442749023438,2.7105274200439453,3.236238956451416,6.850518703460693,1.819726586341858,3.041316032409668,2.8570709228515625,1.9418059587478638,1.999260425567627],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"hovertemplate\":\"cluster=cluster 9<br>x=%{x}<br>y=%{y}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 9\",\"marker\":{\"color\":\"#FECB52\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 9\",\"orientation\":\"v\",\"showlegend\":true,\"text\":[\"university\",\"science\",\"application\",\"card\",\"receive\",\"drive\",\"ca\",\"issue\",\"number\",\"technology\",\"rest\",\"back\",\"state\",\"good\",\"data\",\"pc\",\"pretty\",\"box\",\"remember\",\"internet\",\"miles\",\"free\",\"fact\",\"cost\",\"wondering\",\"western\",\"tin\",\"god\",\"population\",\"change\",\"book\",\"results\",\"history\",\"made\",\"assuming\",\"sense\",\"washington\",\"left\",\"worth\",\"turn\",\"couple\",\"white\"],\"x\":[2.687567710876465,2.574681043624878,2.9885783195495605,0.18424388766288757,2.71740984916687,3.31937575340271,3.51945161819458,2.00848388671875,1.8276658058166504,1.8225805759429932,2.7246031761169434,2.500577211380005,3.2535014152526855,2.801213264465332,3.351578712463379,3.0391597747802734,1.7798808813095093,1.1914480924606323,2.6579670906066895,2.5206868648529053,2.035083055496216,1.982460856437683,1.927260398864746,3.249406099319458,2.5281474590301514,2.5086777210235596,3.0971946716308594,3.537100076675415,1.9370520114898682,2.4621026515960693,2.1500589847564697,2.5915544033050537,2.0836188793182373,3.174849271774292,1.7215129137039185,2.308176279067993,3.100914478302002,2.6399383544921875,1.9697030782699585,2.953777313232422,2.4919986724853516,1.0323811769485474],\"xaxis\":\"x\",\"y\":[3.5776097774505615,4.207818508148193,6.161562919616699,4.268104553222656,4.312382698059082,4.487287998199463,4.004073143005371,3.544703722000122,2.8758139610290527,3.521740674972534,5.034550666809082,3.8725173473358154,4.564871311187744,3.705657720565796,3.8291523456573486,4.76814079284668,3.3844945430755615,5.825338363647461,4.258199691772461,5.129220008850098,4.2388529777526855,4.769068717956543,3.34167218208313,4.588359832763672,4.896155834197998,4.780501842498779,5.8683857917785645,4.043365478515625,4.917318820953369,4.436119079589844,4.031554698944092,4.774003028869629,3.4142370223999023,4.030444622039795,4.630501747131348,4.926098823547363,4.743772983551025,5.29600715637207,4.95681095123291,3.825409173965454,3.5324220657348633,5.753753185272217],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"title\":{\"text\":\"cluster\"},\"tracegroupgap\":0},\"title\":{\"text\":\"word embedding samples\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('251347ec-6ac4-449b-a816-e35e53bd73dd');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from src.embedding import WordEmbeddingCreator\n",
        "from pathlib import Path\n",
        "save_path = Path.joinpath(Path.cwd(), \"vocab_embedding.txt\")\n",
        "wb_creator = WordEmbeddingCreator(model_name=\"cbow\", documents = docs_tr, save_path= save_path)\n",
        "wb_creator.train(min_count=0, embedding_size= 10)\n",
        "vocab = list(word2id.keys())\n",
        "wb_creator.create_and_save_vocab_embedding(vocab, save_path)\n",
        "wb_creator.cluster_words(embedding_save_path = save_path, fig_path = Path('figures'), n_components=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f23xipx7MSV4",
        "outputId": "992d7259-3da6-44ac-ca55-32299d77093c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word-embedding of the word-- reply: \n",
            "vector: [-0.055647593, -0.08603645, -0.050223187, -0.07851673, 0.06465466, -0.058570564, -0.018530225, 0.051920176, 0.016091611, -0.019912569]\n",
            "dim of vector: 10\n"
          ]
        }
      ],
      "source": [
        "v = list(wb_creator.model.wv.vocab)[0]\n",
        "vec = list(wb_creator.model.wv.__getitem__(v))\n",
        "print(f'word-embedding of the word-- {v}: ')\n",
        "print(f'vector: {vec}')\n",
        "print(f'dim of vector: {len(vec)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l53_jkUS-hl-"
      },
      "source": [
        "# **Word-Embeddings visualieren als Beispiel**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "o96LsIWkNrZS"
      },
      "outputs": [],
      "source": [
        "# read word-embedding files\n",
        "with open(save_path) as f:\n",
        "  lines = f.readlines()\n",
        "embedding_data = []\n",
        "words_data = []\n",
        "for t in lines:\n",
        "  w = t.split(\"\\t\")[0]\n",
        "  v = [float(e) for e in t.split(\"\\t\")[1].split(\" \")]\n",
        "  words_data.append(w)\n",
        "  embedding_data.append(v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99hYOKPwB5aw",
        "outputId": "c5ff224b-ecd8-475b-9b37-20df52715091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster id labels for inputted data\n",
            "[2 8 9 0 6 7 2 6 6 4 7 2 9 6 3 1 3 0 9 2 5 4 5 5 7 2 2 4 3 6 8 6 3 6 3 9 8\n",
            " 9 9 9 2 0 7 1 2 4 6 8 2 4 2 3 3 3 9 4 3 1 0 7 6 9 3 5 0 7 5 3 3 1 5 7 4 5\n",
            " 2 8 5 7 3 9 5 5 4 4 3 4 1 4 6 5 8 9 4 3 0 6 1 3 9 0 4 9 1 0 1 4 7 8 1 6 7\n",
            " 4 9 7 8 7 7 3 8 3 8 9 5 3 2 2 3 0 7 0 5 4 9 3 4 3 4 1 4 3 7 4 5 8 0 5 2 6\n",
            " 7 4 4 4 4 2 2 7 1 9 3 5 1 4 9 0 8 6 6 7 3 4 5 9 0 0 6 6 2 1 5 8 2 2 4 9 6\n",
            " 2 8 4 5 4 4 9 9 5 2 1 5 1 3 3 9 6 0 9 7 1 7 9 9 6 2 1 1 5 0 1 5 9 7 1 8 7\n",
            " 0 2 0 6 1 1 4 5 5 9 2 2 1 1 6 1 6 5 6 5 7 9 7 9 6 8 5 0 2 7 3 1 6 8 3 1 4\n",
            " 4 6 4 5 7 9 5 2 7 6 3 0 4 0 2 5 5 8 0 0 0 7 4 5 6 6 6 3 5 1 0 6 4 4 4 8 4\n",
            " 7 5 4 6 6 5 1 8 0 9 7 1 6 1 0 6 2 6 1 9 3 3 0 3 5 0 1 5 2 9 3 4 5 0 7 4 4\n",
            " 6 0 1 4 2 0 0 1 8 8 4 4 0 0 7]\n",
            "Centroids data\n",
            "[[-9.42228474e-03 -3.67209761e-02 -5.73488731e-03 -3.87772186e-02\n",
            "   2.44073065e-02 -1.89386000e-05  2.30586422e-02 -2.06212033e-02\n",
            "   3.14045513e-02 -1.14246240e-02]\n",
            " [-1.49679829e-02 -1.82984932e-02 -6.93341969e-03 -6.80353166e-03\n",
            "   1.16521866e-02 -1.68958250e-02 -8.79386674e-03 -2.36652657e-03\n",
            "  -1.89083666e-02 -5.58753314e-04]\n",
            " [-5.06864378e-02 -6.37611709e-02 -5.96270745e-02 -9.53497546e-02\n",
            "   4.11108197e-02 -4.42677841e-02  1.01775215e-02 -1.48350668e-03\n",
            "   1.65752002e-02 -2.32970708e-02]\n",
            " [ 3.70091960e-03 -7.25445602e-02 -3.47403858e-02 -1.00661071e-01\n",
            "   4.09635137e-02 -1.41256559e-02  3.61503606e-03  1.94581855e-02\n",
            "   3.31682877e-02 -3.40849554e-02]\n",
            " [ 4.89368751e-03 -6.04765836e-03 -6.80933977e-03 -5.52748226e-02\n",
            "   1.21340619e-02 -1.47917999e-02 -1.37820327e-02  2.91031880e-02\n",
            "   1.25253141e-02 -1.46214010e-02]\n",
            " [-3.46095938e-02 -3.14937237e-02 -1.02781352e-02 -9.28605224e-02\n",
            "   2.85418854e-02 -6.60202231e-03  1.79843203e-02 -8.31299077e-04\n",
            "   2.54746046e-03  1.40766915e-03]\n",
            " [-4.25844702e-02 -4.07581231e-02 -2.74215985e-02 -2.98965561e-02\n",
            "   1.37485979e-02 -3.24366421e-02 -1.05797537e-02  1.28841542e-03\n",
            "   1.94959037e-02 -3.97101160e-02]\n",
            " [-2.82173945e-02 -5.83803195e-02 -4.42357927e-02 -3.23330010e-02\n",
            "   1.12120838e-02 -1.31845112e-02 -9.91605428e-03  1.56436762e-02\n",
            "   3.20297887e-02  1.74881157e-02]\n",
            " [-4.05558118e-03 -6.83289091e-03 -5.37941321e-02 -3.14123577e-02\n",
            "  -5.04808759e-03 -4.12349402e-02  1.78588865e-02 -1.79056451e-02\n",
            "   2.58223369e-02  6.32029909e-04]\n",
            " [ 1.00357486e-02 -5.69036823e-02 -2.83759249e-02 -6.89141070e-02\n",
            "   2.40656952e-02 -5.18965592e-02  5.04792353e-03 -4.12587265e-03\n",
            "  -5.68718368e-03  5.17861791e-03]]\n"
          ]
        }
      ],
      "source": [
        "# clustering words with KMeans and Words-Vectors\n",
        "kmeans = cluster.KMeans(n_clusters=10)\n",
        "kmeans.fit(embedding_data)\n",
        " \n",
        "labels = kmeans.labels_\n",
        "centroids = kmeans.cluster_centers_\n",
        " \n",
        "print (\"Cluster id labels for inputted data\")\n",
        "print (labels)\n",
        "print (\"Centroids data\")\n",
        "print (centroids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAMgZ9aIE9A6",
        "outputId": "ea1fdde0-6dd7-4ba4-f6db-0a4606e21fc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duration: 2.1788387298583984 seconds\n"
          ]
        }
      ],
      "source": [
        "# dimension reduction with umap\n",
        "start = time.time()\n",
        "reducer = umap.UMAP(random_state=42,n_components=3)\n",
        "embedding = reducer.fit_transform(embedding_data)\n",
        "print('Duration: {} seconds'.format(time.time() - start))\n",
        "\n",
        "# show samples after dim-reduction in dataframe\n",
        "wb = pd.DataFrame(embedding, columns=['x', 'y', 'z'])\n",
        "wb['word'] = words_data\n",
        "wb['cluster'] = ['cluster ' + str(c) for c in labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "spomMOt_yy0W",
        "outputId": "4a451dc6-0083-4754-8ef1-7175f5e15690"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.11.1.min.js\"></script>                <div id=\"224a68cc-3553-4b28-827b-e15fc8338b75\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"224a68cc-3553-4b28-827b-e15fc8338b75\")) {                    Plotly.newPlot(                        \"224a68cc-3553-4b28-827b-e15fc8338b75\",                        [{\"hovertemplate\":\"cluster=cluster 2<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 2\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 2\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"reply\",\"writes\",\"years\",\"cs\",\"make\",\"place\",\"read\",\"price\",\"article\",\"nntp\",\"information\",\"things\",\"big\",\"time\",\"laboratory\",\"nasa\",\"children\",\"low\",\"real\",\"buy\",\"power\",\"bad\",\"sun\",\"center\",\"space\",\"put\",\"large\",\"parts\",\"talking\",\"program\",\"start\"],\"x\":[8.469653129577637,8.891770362854004,8.984050750732422,8.999683380126953,8.813676834106445,9.72326374053955,9.332722663879395,8.991739273071289,9.081926345825195,9.217175483703613,9.001551628112793,9.361574172973633,8.998323440551758,9.00867748260498,9.016593933105469,8.838726997375488,8.619458198547363,9.930219650268555,9.188122749328613,9.114497184753418,8.956521987915039,8.904085159301758,8.928711891174316,8.920981407165527,9.127795219421387,9.019680976867676,10.827031135559082,11.129720687866211,8.699132919311523,8.856744766235352,8.824148178100586],\"y\":[10.11330795288086,9.57571029663086,7.494811534881592,9.52833366394043,10.172618865966797,8.511847496032715,9.280488967895508,7.416162967681885,10.481670379638672,10.739538192749023,8.401158332824707,9.755206108093262,8.061652183532715,9.291529655456543,9.318680763244629,10.329228401184082,10.290424346923828,7.511422157287598,9.150070190429688,10.709781646728516,9.91724967956543,8.243924140930176,9.129327774047852,10.339214324951172,9.50219440460205,10.644491195678711,8.608304023742676,8.344828605651855,9.673311233520508,9.604612350463867,10.34566593170166],\"z\":[10.675250053405762,9.105443954467773,10.364067077636719,9.28018569946289,10.356822967529297,9.674104690551758,9.290816307067871,11.07769775390625,10.64113712310791,11.387324333190918,9.453778266906738,9.625056266784668,9.553561210632324,9.126093864440918,9.33272933959961,10.394342422485352,10.950429916381836,10.189717292785645,9.281582832336426,11.239290237426758,10.108802795410156,9.738274574279785,9.121346473693848,10.822454452514648,9.305469512939453,11.044879913330078,10.010637283325195,10.543848991394043,9.524246215820312,9.469062805175781,11.160289764404297],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 8<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 8\",\"marker\":{\"color\":\"#EF553B\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 8\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"college\",\"card\",\"heard\",\"early\",\"contact\",\"asked\",\"common\",\"distribution\",\"single\",\"toronto\",\"folks\",\"display\",\"rights\",\"late\",\"corporation\",\"fax\",\"apr\",\"key\",\"man\",\"based\",\"hear\",\"friend\"],\"x\":[11.454142570495605,11.142655372619629,10.297025680541992,10.389307975769043,10.376479148864746,10.558858871459961,10.301019668579102,10.504313468933105,8.915696144104004,11.232574462890625,11.428271293640137,10.415356636047363,11.322402954101562,10.40829849243164,11.379905700683594,10.811909675598145,11.342192649841309,11.134354591369629,10.034431457519531,10.236852645874023,11.36478042602539,11.31314754486084],\"y\":[9.151715278625488,9.379999160766602,10.2431001663208,10.417284965515137,10.179408073425293,9.994790077209473,10.404592514038086,10.118484497070312,9.708087921142578,9.890517234802246,9.483064651489258,10.282025337219238,9.156026840209961,10.195037841796875,9.128674507141113,10.05250358581543,9.371390342712402,9.79571533203125,10.356890678405762,10.25191593170166,9.336297035217285,9.673032760620117],\"z\":[11.959458351135254,10.752945899963379,13.26978588104248,13.101821899414062,10.473145484924316,10.472867012023926,13.167366027832031,10.90334415435791,10.85743522644043,11.183412551879883,11.043581008911133,13.204998016357422,11.075348854064941,10.838773727416992,11.114365577697754,12.638890266418457,10.923492431640625,10.951942443847656,12.957817077636719,10.611077308654785,11.799154281616211,10.948753356933594],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 9<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 9\",\"marker\":{\"color\":\"#00cc96\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 9\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"disclaimer\",\"add\",\"answer\",\"general\",\"run\",\"write\",\"problem\",\"thought\",\"long\",\"research\",\"question\",\"advance\",\"hand\",\"means\",\"radio\",\"gov\",\"dept\",\"hope\",\"division\",\"car\",\"year\",\"world\",\"find\",\"agree\",\"steve\",\"david\",\"making\",\"response\",\"ac\",\"days\",\"book\",\"higher\",\"dod\",\"including\"],\"x\":[9.466029167175293,8.480607986450195,8.433350563049316,8.8584623336792,9.977935791015625,8.487059593200684,9.922192573547363,8.517349243164062,8.97792911529541,8.628412246704102,9.190169334411621,10.154205322265625,9.087068557739258,8.922503471374512,9.252386093139648,8.818455696105957,9.378893852233887,8.86759090423584,9.702879905700684,8.783864974975586,9.651004791259766,8.897435188293457,8.920433044433594,8.873034477233887,9.845407485961914,10.115036964416504,8.758183479309082,9.58310604095459,9.44581127166748,9.796981811523438,9.342988967895508,8.8516263961792,10.256797790527344,9.310189247131348],\"y\":[9.328200340270996,9.018963813781738,9.162284851074219,9.5015230178833,10.561331748962402,8.997653007507324,10.0683012008667,9.236353874206543,9.700117111206055,9.719022750854492,9.486833572387695,10.483173370361328,9.509565353393555,9.568655014038086,9.123869895935059,9.158675193786621,9.618668556213379,9.10935115814209,10.167825698852539,10.143956184387207,9.13209056854248,10.27250862121582,7.728891849517822,9.425232887268066,10.647847175598145,10.23851203918457,10.128971099853516,10.073671340942383,9.651936531066895,9.997390747070312,8.63033676147461,9.983909606933594,10.231871604919434,9.210360527038574],\"z\":[12.302082061767578,12.653412818908691,12.207856178283691,11.598114967346191,12.84995174407959,12.599279403686523,10.51962947845459,11.89127254486084,10.695209503173828,11.460465431213379,10.189440727233887,13.039971351623535,12.075181007385254,10.802152633666992,10.897356986999512,11.182774543762207,10.616340637207031,11.199424743652344,10.507896423339844,11.179245948791504,11.475484848022461,11.271893501281738,11.238508224487305,10.975212097167969,12.933209419250488,12.581396102905273,11.128792762756348,10.952329635620117,10.26259708404541,10.37736701965332,11.044830322265625,11.05762004852295,12.75253677368164,11.064934730529785],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 0<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 0\",\"marker\":{\"color\":\"#ab63fa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 0\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"care\",\"mac\",\"important\",\"close\",\"message\",\"life\",\"appreciated\",\"rest\",\"high\",\"summary\",\"sound\",\"internet\",\"miles\",\"ago\",\"gas\",\"error\",\"systems\",\"network\",\"change\",\"point\",\"results\",\"game\",\"gun\",\"assuming\",\"left\",\"board\",\"word\",\"months\",\"source\",\"note\",\"sell\",\"white\",\"city\",\"jim\",\"interested\"],\"x\":[10.830869674682617,10.52489948272705,10.818138122558594,10.875049591064453,10.549079895019531,10.706645011901855,9.758976936340332,10.509839057922363,9.936026573181152,10.052596092224121,10.640864372253418,10.53731632232666,9.526607513427734,9.779594421386719,10.478890419006348,10.808402061462402,10.934910774230957,10.220248222351074,9.390917778015137,9.534029960632324,9.666354179382324,9.417624473571777,10.616047859191895,10.770072937011719,10.451187133789062,10.625336647033691,10.333357810974121,10.441507339477539,10.616890907287598,10.489704132080078,11.014396667480469,11.030219078063965,10.358011245727539,11.323554039001465,11.023959159851074],\"y\":[9.850008010864258,9.362433433532715,8.692842483520508,8.495720863342285,9.232773780822754,10.23229694366455,9.795357704162598,7.3224196434021,10.448197364807129,9.259922981262207,9.914213180541992,7.406117916107178,8.519906044006348,8.775717735290527,9.495038032531738,9.116284370422363,8.317683219909668,7.208003520965576,8.049195289611816,8.946127891540527,7.810098171234131,8.42492961883545,8.75564193725586,8.370624542236328,7.3562798500061035,8.946111679077148,9.671016693115234,9.001566886901855,9.869129180908203,7.402730941772461,8.619534492492676,8.309784889221191,9.51204776763916,8.821807861328125,9.311965942382812],\"z\":[12.30067253112793,10.799043655395508,10.460941314697266,11.961615562438965,10.71701431274414,12.110371589660645,12.654587745666504,11.409293174743652,12.93305492401123,10.608920097351074,11.179244995117188,11.404596328735352,11.1476411819458,10.855761528015137,10.982808113098145,10.453386306762695,10.745065689086914,11.52431869506836,11.174150466918945,10.83253288269043,11.23774242401123,11.564505577087402,10.626571655273438,10.941977500915527,11.549845695495605,11.315095901489258,10.881611824035645,10.764351844787598,11.284013748168945,11.447972297668457,11.946004867553711,11.616424560546875,10.714132308959961,11.61754035949707,12.161806106567383],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 6<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 6\",\"marker\":{\"color\":\"#FFA15A\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 6\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"info\",\"understand\",\"current\",\"memory\",\"set\",\"questions\",\"ram\",\"lot\",\"file\",\"mind\",\"live\",\"job\",\"code\",\"texas\",\"bill\",\"feel\",\"hard\",\"thinking\",\"support\",\"mark\",\"wrong\",\"national\",\"stop\",\"jews\",\"guess\",\"mike\",\"east\",\"religious\",\"continue\",\"view\",\"required\",\"front\",\"told\",\"uk\",\"hell\",\"date\",\"fine\",\"claim\"],\"x\":[9.181851387023926,9.995587348937988,11.362495422363281,10.781379699707031,9.653093338012695,9.307978630065918,9.30469036102295,10.465810775756836,9.238029479980469,9.46302318572998,9.270153999328613,9.363903045654297,9.427553176879883,9.301880836486816,9.33709716796875,10.9798583984375,9.41089153289795,11.035771369934082,9.820103645324707,8.935585021972656,9.498616218566895,9.9203462600708,9.83175277709961,9.1568021774292,9.504287719726562,9.224936485290527,9.602245330810547,9.364176750183105,9.12423324584961,10.729944229125977,9.823591232299805,9.085317611694336,8.493345260620117,9.167792320251465,9.102737426757812,11.290022850036621,10.84985065460205,9.640823364257812],\"y\":[10.028824806213379,7.535635471343994,8.486077308654785,8.985304832458496,9.921581268310547,9.701162338256836,8.565571784973145,9.452370643615723,8.017374992370605,7.179819583892822,7.264801979064941,8.40745735168457,7.788166522979736,8.203783988952637,8.37483024597168,8.784640312194824,10.757367134094238,8.800273895263672,7.219795227050781,8.475207328796387,9.000307083129883,7.182168960571289,7.118625640869141,7.3255462646484375,7.097126007080078,9.989740371704102,7.180356979370117,7.21810245513916,9.949040412902832,8.529985427856445,7.094213962554932,8.36439323425293,9.727239608764648,7.234505653381348,9.882189750671387,8.961556434631348,8.75148868560791,7.091902256011963],\"z\":[12.749029159545898,12.52551555633545,10.807544708251953,10.080506324768066,13.02281379699707,12.784966468811035,12.365330696105957,12.285470962524414,11.749783515930176,12.135986328125,11.864049911499023,12.033698081970215,11.721254348754883,12.015384674072266,12.237915992736816,10.269430160522461,12.12047290802002,10.270350456237793,10.984827995300293,11.751522064208984,12.550406455993652,11.00450611114502,12.372989654541016,11.369017601013184,12.195516586303711,12.654025077819824,11.612167358398438,12.151069641113281,12.79078197479248,10.13805866241455,12.60360336303711,11.858457565307617,10.204404830932617,11.397592544555664,12.798966407775879,10.932385444641113,10.13123607635498,12.059968948364258],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 7<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 7\",\"marker\":{\"color\":\"#19d3f3\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 7\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"robert\",\"give\",\"figure\",\"strong\",\"machine\",\"stuff\",\"send\",\"week\",\"person\",\"end\",\"home\",\"cc\",\"org\",\"email\",\"rate\",\"software\",\"hp\",\"service\",\"company\",\"tom\",\"public\",\"list\",\"thing\",\"minutes\",\"address\",\"bible\",\"form\",\"speed\",\"image\",\"full\",\"sort\",\"ms\"],\"x\":[9.282540321350098,9.36966609954834,9.536800384521484,9.359704971313477,10.109859466552734,10.148673057556152,10.292169570922852,9.797261238098145,9.150713920593262,9.586986541748047,10.724567413330078,9.945051193237305,10.144488334655762,9.846805572509766,10.023774147033691,9.961502075195312,10.111198425292969,10.011149406433105,9.279964447021484,10.67402458190918,9.665328025817871,9.813749313354492,9.461636543273926,10.133183479309082,9.419687271118164,9.570046424865723,9.471290588378906,9.15796184539795,9.501100540161133,9.65112018585205,9.63174057006836,9.952749252319336],\"y\":[10.16080093383789,8.817540168762207,10.767227172851562,8.617854118347168,10.461640357971191,9.901739120483398,9.253826141357422,10.23273754119873,10.325796127319336,10.302112579345703,10.049551010131836,10.196803092956543,10.379561424255371,9.644954681396484,10.423187255859375,9.95051383972168,10.154047966003418,10.042654991149902,10.342647552490234,9.796636581420898,10.656612396240234,9.082500457763672,10.436452865600586,9.6062650680542,10.479762077331543,10.250399589538574,9.69074535369873,10.398526191711426,9.5128755569458,9.066777229309082,8.923567771911621,10.136067390441895],\"z\":[12.562505722045898,11.93727970123291,12.055744171142578,10.993700981140137,12.188505172729492,12.440778732299805,12.453607559204102,12.492086410522461,11.855756759643555,12.165995597839355,11.73748779296875,12.371614456176758,12.145904541015625,12.884100914001465,12.685229301452637,11.844918251037598,11.982277870178223,12.242560386657715,11.789804458618164,12.035112380981445,12.426850318908691,12.952322959899902,11.864585876464844,12.306645393371582,12.281563758850098,12.454292297363281,12.75001049041748,11.920429229736328,12.678251266479492,12.771554946899414,13.179886817932129,11.974411010742188],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 4<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 4\",\"marker\":{\"color\":\"#FF6692\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 4\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"situation\",\"department\",\"application\",\"pay\",\"institute\",\"net\",\"international\",\"mentioned\",\"interesting\",\"called\",\"computing\",\"show\",\"coming\",\"final\",\"open\",\"post\",\"day\",\"expressed\",\"hardware\",\"easy\",\"wanted\",\"box\",\"california\",\"numbers\",\"idea\",\"phone\",\"money\",\"order\",\"type\",\"class\",\"tin\",\"short\",\"canada\",\"due\",\"effect\",\"lost\",\"light\",\"james\",\"hold\",\"christian\",\"simply\",\"takes\",\"dos\",\"de\",\"posted\",\"reading\",\"chip\"],\"x\":[11.169927597045898,9.758657455444336,9.600497245788574,8.89426040649414,9.844803810119629,9.67322826385498,9.94442367553711,8.872748374938965,9.601985931396484,9.039932250976562,11.117466926574707,10.121722221374512,8.992424964904785,9.525812149047852,9.839384078979492,9.357141494750977,10.025323867797852,10.140547752380371,8.911959648132324,9.892508506774902,11.155856132507324,11.06714916229248,10.127522468566895,9.863171577453613,8.892525672912598,9.504830360412598,9.97800064086914,9.708016395568848,10.475617408752441,9.536566734313965,9.377691268920898,9.749695777893066,11.1655855178833,10.257752418518066,9.562122344970703,9.9642915725708,10.028903007507324,9.800209999084473,8.57839584350586,10.059670448303223,9.383013725280762,10.358288764953613,10.171856880187988,10.47298526763916,10.095422744750977,9.39624309539795,10.439156532287598],\"y\":[7.990437984466553,8.815923690795898,7.111471176147461,8.304582595825195,7.010167121887207,8.144068717956543,9.27700424194336,8.23748779296875,8.641430854797363,8.371228218078613,7.9692535400390625,8.675692558288574,8.039071083068848,7.1165056228637695,6.985866069793701,8.270808219909668,8.691454887390137,7.247700214385986,8.051851272583008,8.311275482177734,7.894567489624023,8.220688819885254,8.062236785888672,7.027207374572754,8.680944442749023,8.343655586242676,8.423789978027344,7.120372772216797,7.715869426727295,8.67977237701416,7.159007549285889,8.771721839904785,7.943814754486084,8.398741722106934,8.583499908447266,8.258597373962402,10.038110733032227,8.790019035339355,8.812618255615234,8.457612991333008,8.12919807434082,7.885946750640869,7.962230205535889,8.494128227233887,8.06089973449707,7.607931613922119,7.748189449310303],\"z\":[11.449036598205566,13.372912406921387,11.628666877746582,12.556147575378418,12.686760902404785,12.732087135314941,12.794730186462402,12.516568183898926,13.325230598449707,11.691736221313477,11.657556533813477,13.249710083007812,12.463786125183105,11.795184135437012,12.530914306640625,12.888895988464355,13.173437118530273,12.852360725402832,10.936529159545898,13.37474536895752,11.487212181091309,11.501648902893066,12.464190483093262,12.58410930633545,12.200821876525879,11.753859519958496,12.898447036743164,12.75904655456543,13.01627254486084,12.640795707702637,11.496953964233398,12.706686973571777,11.535953521728516,12.95902156829834,13.2483491897583,12.993393898010254,10.705727577209473,13.192047119140625,12.53520679473877,13.088763236999512,12.798433303833008,13.118738174438477,12.579310417175293,12.899247169494629,12.603878021240234,11.846479415893555,12.965944290161133],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 3<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 3\",\"marker\":{\"color\":\"#B6E880\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 3\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"faster\",\"system\",\"reason\",\"bit\",\"true\",\"posting\",\"host\",\"wrote\",\"mail\",\"people\",\"usa\",\"call\",\"part\",\"great\",\"side\",\"understanding\",\"case\",\"small\",\"work\",\"control\",\"good\",\"simple\",\"data\",\"line\",\"found\",\"access\",\"result\",\"major\",\"version\",\"times\",\"taking\",\"law\",\"au\",\"bike\",\"turn\"],\"x\":[8.455902099609375,8.756244659423828,8.777932167053223,9.130446434020996,8.622505187988281,8.820033073425293,8.60871696472168,8.916804313659668,8.692059516906738,8.82679557800293,8.892768859863281,8.516009330749512,8.669903755187988,9.412464141845703,8.544600486755371,9.003911018371582,8.733208656311035,8.91002082824707,8.618379592895508,8.8253173828125,9.199981689453125,8.631575584411621,8.792481422424316,9.037158012390137,8.967702865600586,8.449020385742188,9.336265563964844,8.490592002868652,8.422135353088379,8.646259307861328,8.91726016998291,9.521478652954102,9.011394500732422,8.411993026733398,8.88899040222168],\"y\":[9.560256004333496,8.653739929199219,10.039937973022461,8.709761619567871,9.342246055603027,8.237561225891113,8.099507331848145,9.979682922363281,8.113009452819824,8.897116661071777,8.653849601745605,9.656752586364746,8.538989067077637,8.961115837097168,8.240747451782227,9.404240608215332,8.36526870727539,8.194019317626953,8.74412727355957,9.384263038635254,8.004953384399414,8.43678092956543,7.87693452835083,9.229735374450684,9.321098327636719,9.694284439086914,8.933271408081055,8.397026062011719,9.75167179107666,8.791874885559082,8.418848991394043,8.898669242858887,9.108115196228027,9.658428192138672,7.970432281494141],\"z\":[10.017260551452637,9.756086349487305,9.923444747924805,10.278736114501953,9.333527565002441,10.567486763000488,9.849209785461426,10.095541954040527,9.720300674438477,9.46133804321289,10.13587474822998,9.760730743408203,10.655951499938965,10.0323486328125,9.791936874389648,10.297266960144043,9.606277465820312,9.712026596069336,10.24875259399414,9.316259384155273,9.955740928649902,10.393045425415039,10.466570854187012,9.533181190490723,10.372088432312012,9.666070938110352,10.046781539916992,9.85438060760498,9.825862884521484,10.048907279968262,9.415689468383789,9.966917037963867,10.347844123840332,10.0135498046875,10.588839530944824],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 1<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 1\",\"marker\":{\"color\":\"#FF97FF\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 1\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"today\",\"opinion\",\"mine\",\"originator\",\"problems\",\"kind\",\"calls\",\"written\",\"interest\",\"opinions\",\"keywords\",\"school\",\"top\",\"makes\",\"government\",\"sci\",\"considered\",\"clear\",\"possibly\",\"truth\",\"matter\",\"newsreader\",\"original\",\"ibm\",\"worse\",\"sale\",\"technical\",\"disk\",\"love\",\"total\",\"related\",\"deleted\",\"exists\",\"copy\",\"experience\"],\"x\":[9.510586738586426,10.648397445678711,10.632357597351074,9.620630264282227,10.641857147216797,10.769503593444824,10.234053611755371,10.676347732543945,11.476730346679688,10.028626441955566,10.081076622009277,10.608041763305664,11.291251182556152,10.837630271911621,10.854220390319824,10.728130340576172,11.490164756774902,9.96495246887207,10.274991989135742,10.851330757141113,11.182940483093262,10.484903335571289,10.952231407165527,10.667794227600098,11.47735595703125,11.052206039428711,10.980047225952148,11.244244575500488,10.97492504119873,10.442561149597168,10.193899154663086,9.725385665893555,9.944195747375488,10.411946296691895,10.15870475769043],\"y\":[7.832780838012695,9.264885902404785,9.461462020874023,7.921757698059082,10.180997848510742,9.244053840637207,8.08847713470459,8.847846031188965,8.766082763671875,8.858657836914062,8.31393051147461,10.253473281860352,8.657495498657227,9.71664810180664,9.912704467773438,9.159494400024414,8.774433135986328,7.327728748321533,8.135047912597656,8.680840492248535,8.252608299255371,8.026562690734863,7.8604302406311035,8.444533348083496,8.856698036193848,8.776673316955566,8.760743141174316,9.175930976867676,8.96541690826416,9.011853218078613,7.143551826477051,8.970077514648438,7.027583599090576,7.402419090270996,7.919983386993408],\"z\":[11.927567481994629,12.744447708129883,12.54532527923584,11.900984764099121,12.270716667175293,12.75869369506836,12.613401412963867,12.777599334716797,12.125768661499023,11.82845687866211,12.622552871704102,12.928643226623535,12.530019760131836,12.113873481750488,12.196534156799316,12.395837783813477,11.855951309204102,12.334848403930664,12.42774486541748,12.756040573120117,10.759512901306152,11.077561378479004,12.27415657043457,12.621736526489258,11.860508918762207,12.496243476867676,12.652056694030762,11.900253295898438,12.745139122009277,11.901771545410156,12.096075057983398,12.75808048248291,11.825532913208008,12.014988899230957,12.943528175354004],\"type\":\"scatter3d\"},{\"hovertemplate\":\"cluster=cluster 5<br>x=%{x}<br>y=%{y}<br>z=%{z}<br>word=%{text}<extra></extra>\",\"legendgroup\":\"cluster 5\",\"marker\":{\"color\":\"#FECB52\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"cluster 5\",\"scene\":\"scene\",\"showlegend\":true,\"text\":[\"university\",\"computer\",\"science\",\"receive\",\"drive\",\"ca\",\"issue\",\"number\",\"project\",\"news\",\"technology\",\"back\",\"state\",\"pc\",\"pretty\",\"remember\",\"windows\",\"free\",\"fact\",\"john\",\"cost\",\"wondering\",\"western\",\"god\",\"guy\",\"similar\",\"population\",\"group\",\"religion\",\"nice\",\"history\",\"made\",\"sense\",\"washington\",\"references\",\"design\",\"worth\",\"engineering\",\"couple\"],\"x\":[9.440964698791504,9.569351196289062,9.697331428527832,9.461730003356934,10.10339641571045,9.011920928955078,10.120332717895508,9.873331069946289,9.516900062561035,9.541219711303711,10.00400161743164,9.8574800491333,10.173579216003418,10.360041618347168,9.791019439697266,10.020715713500977,10.16896915435791,9.876747131347656,10.004158020019531,9.418145179748535,9.576251029968262,10.637606620788574,10.48773193359375,9.1254243850708,10.304656028747559,10.549208641052246,10.820883750915527,9.618574142456055,10.494161605834961,11.284037590026855,9.8490629196167,8.901702880859375,10.646300315856934,10.017578125,9.659198760986328,9.793289184570312,10.63821792602539,10.660419464111328,9.596137046813965],\"y\":[8.107051849365234,9.673531532287598,7.957826137542725,8.179678916931152,7.335978984832764,7.586815357208252,8.83018684387207,9.400391578674316,9.007356643676758,8.678384780883789,9.08702564239502,8.144695281982422,7.331610679626465,7.3536553382873535,9.076035499572754,7.986596584320068,10.079086303710938,7.817335605621338,9.172184944152832,9.6245698928833,7.275974750518799,7.561193943023682,7.664373397827148,7.486717700958252,8.9425048828125,7.677242279052734,7.861347675323486,9.110273361206055,8.338945388793945,9.02185344696045,8.859217643737793,7.897493839263916,7.724800109863281,7.311204433441162,9.440336227416992,8.589337348937988,8.015122413635254,7.786713123321533,8.011270523071289],\"z\":[9.754883766174316,9.767903327941895,10.69891357421875,10.870794296264648,10.367636680603027,10.402612686157227,10.332780838012695,9.896122932434082,9.70962905883789,9.382868766784668,10.423964500427246,10.117634773254395,10.382804870605469,10.571394920349121,10.360689163208008,10.452445030212402,10.323773384094238,10.999876022338867,9.94018268585205,9.740668296813965,10.49638843536377,10.998745918273926,10.547547340393066,10.235246658325195,10.960068702697754,10.614709854125977,10.80698013305664,9.456708908081055,10.235993385314941,11.679051399230957,10.177207946777344,10.68677806854248,11.136127471923828,10.539567947387695,10.594785690307617,9.804967880249023,10.922595024108887,10.741156578063965,9.914264678955078],\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"x\"}},\"yaxis\":{\"title\":{\"text\":\"y\"}},\"zaxis\":{\"title\":{\"text\":\"z\"}}},\"legend\":{\"title\":{\"text\":\"cluster\"},\"tracegroupgap\":0},\"title\":{\"text\":\"word-embedding-samples\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('224a68cc-3553-4b28-827b-e15fc8338b75');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# visualization\n",
        "fig = px.scatter_3d(wb, \n",
        "                    text = wb['word'],\n",
        "                    x='x', y='y', z='z',\n",
        "                    color = wb['cluster'],\n",
        "                    title =\"word-embedding-samples\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9jSI12r9zqu"
      },
      "source": [
        "# **ETM Model**\n",
        "\n",
        "ETM hat die Architektur eines Variational Autoencoders. \n",
        "ETM wird mit den pretrainierten Embedding kombiniert. Die Embeddings für Topics werden als Gewichten eines Teiles des Netzes aktualiert mittels der ELBO Loss\n",
        "\n",
        "![ETM.drawio.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwiU9zWCIUh_",
        "outputId": "ca3289c7-4549-4cd2-883f-0c135715cafd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "139\n",
            "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0476, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0476, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0476, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0476, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0476, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0476,\n",
            "        0.0000, 0.0000, 0.0000, 0.0952, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0476,\n",
            "        0.0000, 0.0000, 0.0000, 0.0476, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0476,\n",
            "        0.0476, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0476,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0476, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0476, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0476,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0476, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0952,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0476, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "        0.0476, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000])\n"
          ]
        }
      ],
      "source": [
        "# using DocSet to use easier the modul DataSet from torch\n",
        "from src.train_etm import DocSet, TrainETM\n",
        "from src.etm import ETM\n",
        "\n",
        "vocab_size = len(list(word2id.keys()))\n",
        "tr_set = DocSet(\"train\", vocab_size, train_set, normalize_data=True)\n",
        "print(len(tr_set))\n",
        "print(tr_set.__getitem__(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kbJIWLZpJRnG",
        "outputId": "57e6868f-bf51-4dab-b60d-9224440a26ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "adam\n",
            "ETM(\n",
            "  (t_drop): Dropout(p=0.5, inplace=False)\n",
            "  (theta_act): ReLU()\n",
            "  (topic_embeddings_alphas): Linear(in_features=10, out_features=5, bias=False)\n",
            "  (q_theta): Sequential(\n",
            "    (0): Linear(in_features=348, out_features=100, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (mu_q_theta): Linear(in_features=100, out_features=5, bias=True)\n",
            "  (logsigma_q_theta): Linear(in_features=100, out_features=5, bias=True)\n",
            ")\n",
            "number of batches: 23\n",
            "Epoch: 0/1000  -  Loss: 5.8624982833862305 \t Rec: 5.85275936126709 \t KL: 0.009738001972436905\n",
            "Epoch: 1/1000  -  Loss: 5.855328559875488 \t Rec: 5.852765083312988 \t KL: 0.002563350135460496\n",
            "Epoch: 2/1000  -  Loss: 5.853658199310303 \t Rec: 5.852509021759033 \t KL: 0.0011481072288006544\n",
            "Epoch: 3/1000  -  Loss: 5.853189945220947 \t Rec: 5.852572917938232 \t KL: 0.0006167730898596346\n",
            "Epoch: 4/1000  -  Loss: 5.8526930809021 \t Rec: 5.852432727813721 \t KL: 0.0002603085886221379\n",
            "Epoch: 5/1000  -  Loss: 5.85248327255249 \t Rec: 5.852397918701172 \t KL: 8.502320997649804e-05\n",
            "Epoch: 6/1000  -  Loss: 5.852112293243408 \t Rec: 5.852093696594238 \t KL: 1.898472874017898e-05\n",
            "Epoch: 7/1000  -  Loss: 5.852108478546143 \t Rec: 5.8521037101745605 \t KL: 2.8763563477696152e-06\n",
            "Epoch: 8/1000  -  Loss: 5.8521809577941895 \t Rec: 5.8521809577941895 \t KL: 2.708123645334126e-07\n",
            "Epoch: 9/1000  -  Loss: 5.852109909057617 \t Rec: 5.852109909057617 \t KL: 1.7060749257780117e-08\n",
            "Epoch: 10/1000  -  Loss: 5.852035999298096 \t Rec: 5.852035999298096 \t KL: 6.478765857131918e-10\n",
            "Epoch: 11/1000  -  Loss: 5.851984977722168 \t Rec: 5.851984977722168 \t KL: 2.8074651492460134e-09\n",
            "Epoch: 12/1000  -  Loss: 5.851980686187744 \t Rec: 5.851980686187744 \t KL: 2.8074651492460134e-09\n",
            "Epoch: 13/1000  -  Loss: 5.8519439697265625 \t Rec: 5.8519439697265625 \t KL: 3.0234239556392595e-09\n",
            "Epoch: 14/1000  -  Loss: 5.8519287109375 \t Rec: 5.8519287109375 \t KL: 8.638354476175891e-10\n",
            "Epoch: 15/1000  -  Loss: 5.851941108703613 \t Rec: 5.851941108703613 \t KL: 0.0\n",
            "Epoch: 16/1000  -  Loss: 5.851874828338623 \t Rec: 5.851874828338623 \t KL: 0.0\n",
            "Epoch: 17/1000  -  Loss: 5.851869106292725 \t Rec: 5.851869106292725 \t KL: 0.0\n",
            "Epoch: 18/1000  -  Loss: 5.8518967628479 \t Rec: 5.8518967628479 \t KL: 0.0\n",
            "Epoch: 19/1000  -  Loss: 5.851864814758301 \t Rec: 5.851864814758301 \t KL: 0.0\n",
            "Epoch: 20/1000  -  Loss: 5.851834297180176 \t Rec: 5.851834297180176 \t KL: 0.0\n",
            "Epoch: 21/1000  -  Loss: 5.851833820343018 \t Rec: 5.851833820343018 \t KL: 0.0\n",
            "Epoch: 22/1000  -  Loss: 5.85183048248291 \t Rec: 5.85183048248291 \t KL: 0.0\n",
            "Epoch: 23/1000  -  Loss: 5.851831912994385 \t Rec: 5.851831912994385 \t KL: 0.0\n",
            "Epoch: 24/1000  -  Loss: 5.8518218994140625 \t Rec: 5.8518218994140625 \t KL: 0.0\n",
            "Epoch: 25/1000  -  Loss: 5.85181188583374 \t Rec: 5.85181188583374 \t KL: 0.0\n",
            "Epoch: 26/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 0.0\n",
            "Epoch: 27/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 0.0\n",
            "Epoch: 28/1000  -  Loss: 5.851813316345215 \t Rec: 5.851813316345215 \t KL: 0.0\n",
            "Epoch: 29/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 30/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 0.0\n",
            "Epoch: 31/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 32/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 33/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 34/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 35/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 36/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 37/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 38/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 39/1000  -  Loss: 5.851798057556152 \t Rec: 5.851798057556152 \t KL: 0.0\n",
            "Epoch: 40/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 0.0\n",
            "Epoch: 41/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 42/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 43/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 44/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 45/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 46/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 47/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 48/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 49/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 50/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 51/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 52/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 53/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 54/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 55/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 56/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 57/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 58/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 59/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 60/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 61/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 62/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 63/1000  -  Loss: 5.8517985343933105 \t Rec: 5.8517985343933105 \t KL: 0.0\n",
            "Epoch: 64/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 65/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 66/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 67/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 68/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 69/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 70/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 71/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 72/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 73/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 74/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 75/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 0.0\n",
            "Epoch: 76/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 77/1000  -  Loss: 5.851796627044678 \t Rec: 5.851796627044678 \t KL: 0.0\n",
            "Epoch: 78/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 79/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 80/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 81/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 82/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 83/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 84/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 85/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 86/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 87/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 88/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 89/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 90/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 91/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 92/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 93/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 94/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 95/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 96/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 97/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 98/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 99/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 100/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 101/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 102/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 103/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 104/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 105/1000  -  Loss: 5.8517985343933105 \t Rec: 5.8517985343933105 \t KL: 0.0\n",
            "Epoch: 106/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 107/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 108/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 109/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 110/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 111/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 0.0\n",
            "Epoch: 112/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 113/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 114/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 115/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 116/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 117/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 118/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 119/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 120/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 121/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 122/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 123/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 124/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 125/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 126/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 0.0\n",
            "Epoch: 127/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 128/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 129/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 130/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 131/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 132/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 133/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 134/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 135/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 136/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 137/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 138/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 139/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 140/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 0.0\n",
            "Epoch: 141/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 142/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 143/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 0.0\n",
            "Epoch: 144/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 145/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 0.0\n",
            "Epoch: 146/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 147/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 148/1000  -  Loss: 5.851797580718994 \t Rec: 5.851797580718994 \t KL: 0.0\n",
            "Epoch: 149/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 150/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 151/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 152/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 153/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 154/1000  -  Loss: 5.8517961502075195 \t Rec: 5.8517961502075195 \t KL: 0.0\n",
            "Epoch: 155/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 156/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 157/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 158/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 0.0\n",
            "Epoch: 159/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 160/1000  -  Loss: 5.851797580718994 \t Rec: 5.851797580718994 \t KL: 0.0\n",
            "Epoch: 161/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 162/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 163/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 164/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 165/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 166/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 167/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 0.0\n",
            "Epoch: 168/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 0.0\n",
            "Epoch: 169/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 170/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 171/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 172/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 173/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 174/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 175/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 176/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 177/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 178/1000  -  Loss: 5.851798057556152 \t Rec: 5.851798057556152 \t KL: 0.0\n",
            "Epoch: 179/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 180/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 0.0\n",
            "Epoch: 181/1000  -  Loss: 5.851797580718994 \t Rec: 5.851797580718994 \t KL: 0.0\n",
            "Epoch: 182/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 183/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 184/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 185/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 0.0\n",
            "Epoch: 186/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 187/1000  -  Loss: 5.851797580718994 \t Rec: 5.851797580718994 \t KL: 0.0\n",
            "Epoch: 188/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 189/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 190/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 0.0\n",
            "Epoch: 191/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 192/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 0.0\n",
            "Epoch: 193/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 194/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 195/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 196/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 197/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 0.0\n",
            "Epoch: 198/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 199/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 200/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 201/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 202/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 0.0\n",
            "Epoch: 203/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 204/1000  -  Loss: 5.851811408996582 \t Rec: 5.851811408996582 \t KL: 0.0\n",
            "Epoch: 205/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 206/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 207/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 208/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 0.0\n",
            "Epoch: 209/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 210/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 211/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 212/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 213/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 214/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 215/1000  -  Loss: 5.851796627044678 \t Rec: 5.851796627044678 \t KL: 0.0\n",
            "Epoch: 216/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 217/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 218/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 0.0\n",
            "Epoch: 219/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 220/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 221/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 222/1000  -  Loss: 5.8518147468566895 \t Rec: 5.8518147468566895 \t KL: 0.0\n",
            "Epoch: 223/1000  -  Loss: 5.8517985343933105 \t Rec: 5.8517985343933105 \t KL: 0.0\n",
            "Epoch: 224/1000  -  Loss: 5.851795196533203 \t Rec: 5.851795196533203 \t KL: 0.0\n",
            "Epoch: 225/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 226/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 227/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 0.0\n",
            "Epoch: 228/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 0.0\n",
            "Epoch: 229/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 230/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 231/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 0.0\n",
            "Epoch: 232/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 233/1000  -  Loss: 5.851811408996582 \t Rec: 5.851811408996582 \t KL: 0.0\n",
            "Epoch: 234/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 235/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 236/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 237/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 238/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 239/1000  -  Loss: 5.8517985343933105 \t Rec: 5.8517985343933105 \t KL: 0.0\n",
            "Epoch: 240/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 241/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 242/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 243/1000  -  Loss: 5.851796627044678 \t Rec: 5.851796627044678 \t KL: 0.0\n",
            "Epoch: 244/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 245/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 246/1000  -  Loss: 5.851811408996582 \t Rec: 5.851811408996582 \t KL: 0.0\n",
            "Epoch: 247/1000  -  Loss: 5.851798057556152 \t Rec: 5.851798057556152 \t KL: 0.0\n",
            "Epoch: 248/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 0.0\n",
            "Epoch: 249/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 250/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 251/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 252/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 253/1000  -  Loss: 5.85181188583374 \t Rec: 5.85181188583374 \t KL: 0.0\n",
            "Epoch: 254/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 255/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 256/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 257/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 258/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 259/1000  -  Loss: 5.851798057556152 \t Rec: 5.851798057556152 \t KL: 0.0\n",
            "Epoch: 260/1000  -  Loss: 5.851818561553955 \t Rec: 5.851818561553955 \t KL: 0.0\n",
            "Epoch: 261/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 262/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 263/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 264/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 265/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 266/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 267/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 268/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 0.0\n",
            "Epoch: 269/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 270/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 271/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 0.0\n",
            "Epoch: 272/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 273/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 274/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 275/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 0.0\n",
            "Epoch: 276/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 277/1000  -  Loss: 5.851797580718994 \t Rec: 5.851797580718994 \t KL: 0.0\n",
            "Epoch: 278/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 279/1000  -  Loss: 5.851815700531006 \t Rec: 5.851815700531006 \t KL: 0.0\n",
            "Epoch: 280/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 281/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 282/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 283/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 284/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 285/1000  -  Loss: 5.851813793182373 \t Rec: 5.851813793182373 \t KL: 0.0\n",
            "Epoch: 286/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 287/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 288/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 289/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 290/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 291/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 292/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 293/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 0.0\n",
            "Epoch: 294/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 295/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 296/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 297/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 0.0\n",
            "Epoch: 298/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 299/1000  -  Loss: 5.851789951324463 \t Rec: 5.851789951324463 \t KL: 0.0\n",
            "Epoch: 300/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 0.0\n",
            "Epoch: 301/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 302/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 303/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 304/1000  -  Loss: 5.851794242858887 \t Rec: 5.851794242858887 \t KL: 0.0\n",
            "Epoch: 305/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 306/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 0.0\n",
            "Epoch: 307/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 308/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 309/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 0.0\n",
            "Epoch: 310/1000  -  Loss: 5.851794242858887 \t Rec: 5.851794242858887 \t KL: 0.0\n",
            "Epoch: 311/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 312/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 313/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 314/1000  -  Loss: 5.851818561553955 \t Rec: 5.851818561553955 \t KL: 0.0\n",
            "Epoch: 315/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 316/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 317/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 318/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 319/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 320/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 321/1000  -  Loss: 5.8517985343933105 \t Rec: 5.8517985343933105 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 322/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 0.0\n",
            "Epoch: 323/1000  -  Loss: 5.851796627044678 \t Rec: 5.851796627044678 \t KL: 0.0\n",
            "Epoch: 324/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 0.0\n",
            "Epoch: 325/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 326/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 327/1000  -  Loss: 5.851811408996582 \t Rec: 5.851811408996582 \t KL: 0.0\n",
            "Epoch: 328/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 329/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 330/1000  -  Loss: 5.851813793182373 \t Rec: 5.851813793182373 \t KL: 0.0\n",
            "Epoch: 331/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 3.887259403256849e-09\n",
            "Epoch: 332/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 333/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 334/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 2.5915063428527674e-09\n",
            "Epoch: 335/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 336/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 337/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 338/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 0.0\n",
            "Epoch: 339/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 340/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 341/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 342/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 343/1000  -  Loss: 5.851813793182373 \t Rec: 5.851813793182373 \t KL: 0.0\n",
            "Epoch: 344/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 345/1000  -  Loss: 5.851813793182373 \t Rec: 5.851813793182373 \t KL: 0.0\n",
            "Epoch: 346/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 347/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 348/1000  -  Loss: 5.851794719696045 \t Rec: 5.851794719696045 \t KL: 0.0\n",
            "Epoch: 349/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 350/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 351/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 352/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 353/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 354/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 355/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 356/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 357/1000  -  Loss: 5.85181188583374 \t Rec: 5.85181188583374 \t KL: 0.0\n",
            "Epoch: 358/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 359/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 360/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 361/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 362/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 6.478765857131918e-10\n",
            "Epoch: 363/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 364/1000  -  Loss: 5.851811408996582 \t Rec: 5.851811408996582 \t KL: 0.0\n",
            "Epoch: 365/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 366/1000  -  Loss: 5.851796627044678 \t Rec: 5.851796627044678 \t KL: 0.0\n",
            "Epoch: 367/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 368/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 369/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 370/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 371/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 372/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 373/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 374/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 0.0\n",
            "Epoch: 375/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 376/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 377/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 378/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 379/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 380/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 381/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 382/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 383/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 384/1000  -  Loss: 5.851813793182373 \t Rec: 5.851813793182373 \t KL: 0.0\n",
            "Epoch: 385/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 0.0\n",
            "Epoch: 386/1000  -  Loss: 5.8517985343933105 \t Rec: 5.8517985343933105 \t KL: 0.0\n",
            "Epoch: 387/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 0.0\n",
            "Epoch: 388/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 389/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 390/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 391/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 392/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 393/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 394/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 395/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 396/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 397/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 398/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 399/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 400/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 401/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 402/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 403/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 0.0\n",
            "Epoch: 404/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 2.5915063428527674e-09\n",
            "Epoch: 405/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 406/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 0.0\n",
            "Epoch: 407/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 408/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 2.5915063428527674e-09\n",
            "Epoch: 409/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 410/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 411/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 412/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 413/1000  -  Loss: 5.85181999206543 \t Rec: 5.85181999206543 \t KL: 2.5915063428527674e-09\n",
            "Epoch: 414/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 6.478765857131918e-10\n",
            "Epoch: 415/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 416/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 417/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 6.478765857131918e-10\n",
            "Epoch: 418/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 419/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 420/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 421/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 422/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 423/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 424/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 0.0\n",
            "Epoch: 425/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 426/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 427/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 428/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 429/1000  -  Loss: 5.851813793182373 \t Rec: 5.851813793182373 \t KL: 0.0\n",
            "Epoch: 430/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 0.0\n",
            "Epoch: 431/1000  -  Loss: 5.851795196533203 \t Rec: 5.851795196533203 \t KL: 0.0\n",
            "Epoch: 432/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 433/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 0.0\n",
            "Epoch: 434/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 435/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 436/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 437/1000  -  Loss: 5.851794242858887 \t Rec: 5.851794242858887 \t KL: 0.0\n",
            "Epoch: 438/1000  -  Loss: 5.8518147468566895 \t Rec: 5.8518147468566895 \t KL: 0.0\n",
            "Epoch: 439/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 440/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 441/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 442/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 0.0\n",
            "Epoch: 443/1000  -  Loss: 5.8517985343933105 \t Rec: 5.8517985343933105 \t KL: 0.0\n",
            "Epoch: 444/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 445/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 0.0\n",
            "Epoch: 446/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 447/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 448/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 1.7276708952351783e-09\n",
            "Epoch: 449/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 450/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 451/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 0.0\n",
            "Epoch: 452/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 453/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 454/1000  -  Loss: 5.851797580718994 \t Rec: 5.851797580718994 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 455/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 456/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 457/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 6.478765857131918e-10\n",
            "Epoch: 458/1000  -  Loss: 5.851811408996582 \t Rec: 5.851811408996582 \t KL: 0.0\n",
            "Epoch: 459/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 460/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 461/1000  -  Loss: 5.851813793182373 \t Rec: 5.851813793182373 \t KL: 0.0\n",
            "Epoch: 462/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 463/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 2.5915063428527674e-09\n",
            "Epoch: 464/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 465/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 3.887259403256849e-09\n",
            "Epoch: 466/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 467/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 468/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 469/1000  -  Loss: 5.851811408996582 \t Rec: 5.851811408996582 \t KL: 0.0\n",
            "Epoch: 470/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 471/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 1.7276708952351783e-09\n",
            "Epoch: 472/1000  -  Loss: 5.851813316345215 \t Rec: 5.851813316345215 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 473/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 474/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 475/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 6.478765857131918e-10\n",
            "Epoch: 476/1000  -  Loss: 5.851813793182373 \t Rec: 5.851813793182373 \t KL: 0.0\n",
            "Epoch: 477/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 478/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 479/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 1.5117119778196297e-09\n",
            "Epoch: 480/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 481/1000  -  Loss: 5.851798057556152 \t Rec: 5.851798057556152 \t KL: 0.0\n",
            "Epoch: 482/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 2.3755473144149164e-09\n",
            "Epoch: 483/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 8.638354476175891e-10\n",
            "Epoch: 484/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 0.0\n",
            "Epoch: 485/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 486/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 2.8074651492460134e-09\n",
            "Epoch: 487/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 8.638354476175891e-10\n",
            "Epoch: 488/1000  -  Loss: 5.851813316345215 \t Rec: 5.851813316345215 \t KL: 0.0\n",
            "Epoch: 489/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 1.7276708952351783e-09\n",
            "Epoch: 490/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 6.478765857131918e-10\n",
            "Epoch: 491/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 2.1595887300662753e-09\n",
            "Epoch: 492/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 8.638354476175891e-10\n",
            "Epoch: 493/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 8.638354476175891e-10\n",
            "Epoch: 494/1000  -  Loss: 5.85181188583374 \t Rec: 5.85181188583374 \t KL: 0.0\n",
            "Epoch: 495/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 496/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 2.3755473144149164e-09\n",
            "Epoch: 497/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 498/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 499/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 500/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 8.638354476175891e-10\n",
            "Epoch: 501/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 502/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 503/1000  -  Loss: 5.8518147468566895 \t Rec: 5.8518147468566895 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 504/1000  -  Loss: 5.851798057556152 \t Rec: 5.851798057556152 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 505/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 506/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 507/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 8.638354476175891e-10\n",
            "Epoch: 508/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 2.5915063428527674e-09\n",
            "Epoch: 509/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 510/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 6.478765857131918e-10\n",
            "Epoch: 511/1000  -  Loss: 5.851812839508057 \t Rec: 5.851812839508057 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 512/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 1.7276707842128758e-09\n",
            "Epoch: 513/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 514/1000  -  Loss: 5.851813316345215 \t Rec: 5.851813316345215 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 515/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 516/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 517/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 6.478765857131918e-10\n",
            "Epoch: 518/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 8.638354476175891e-10\n",
            "Epoch: 519/1000  -  Loss: 5.8517961502075195 \t Rec: 5.8517961502075195 \t KL: 6.478765857131918e-10\n",
            "Epoch: 520/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 0.0\n",
            "Epoch: 521/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 1.5117120888419322e-09\n",
            "Epoch: 522/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 523/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 6.478765857131918e-10\n",
            "Epoch: 524/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 3.0234241776838644e-09\n",
            "Epoch: 525/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 1.9436297016284243e-09\n",
            "Epoch: 526/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 2.3755475364595213e-09\n",
            "Epoch: 527/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 6.478765857131918e-10\n",
            "Epoch: 528/1000  -  Loss: 5.8517985343933105 \t Rec: 5.8517985343933105 \t KL: 3.6713008189082075e-09\n",
            "Epoch: 529/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 530/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 3.0234241776838644e-09\n",
            "Epoch: 531/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 4.751094628829833e-09\n",
            "Epoch: 532/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 0.0\n",
            "Epoch: 533/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 8.638354476175891e-10\n",
            "Epoch: 534/1000  -  Loss: 5.851798057556152 \t Rec: 5.851798057556152 \t KL: 1.9436299236730292e-09\n",
            "Epoch: 535/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 536/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 537/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 8.638354476175891e-10\n",
            "Epoch: 538/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 3.887259403256849e-09\n",
            "Epoch: 539/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 540/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 0.0\n",
            "Epoch: 541/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 542/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 1.5117120888419322e-09\n",
            "Epoch: 543/1000  -  Loss: 5.85181188583374 \t Rec: 5.85181188583374 \t KL: 0.0\n",
            "Epoch: 544/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 0.0\n",
            "Epoch: 545/1000  -  Loss: 5.8517985343933105 \t Rec: 5.8517985343933105 \t KL: 0.0\n",
            "Epoch: 546/1000  -  Loss: 5.851798057556152 \t Rec: 5.851798057556152 \t KL: 1.9436297016284243e-09\n",
            "Epoch: 547/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 548/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 549/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 550/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 551/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 552/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 553/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 554/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 555/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 556/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 6.478765857131918e-10\n",
            "Epoch: 557/1000  -  Loss: 5.851813793182373 \t Rec: 5.851813793182373 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 558/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 559/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 6.478765857131918e-10\n",
            "Epoch: 560/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 4.3191774601325506e-09\n",
            "Epoch: 561/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 2.8074651492460134e-09\n",
            "Epoch: 562/1000  -  Loss: 5.851811408996582 \t Rec: 5.851811408996582 \t KL: 2.5915063428527674e-09\n",
            "Epoch: 563/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 564/1000  -  Loss: 5.851798057556152 \t Rec: 5.851798057556152 \t KL: 0.0\n",
            "Epoch: 565/1000  -  Loss: 5.851812839508057 \t Rec: 5.851812839508057 \t KL: 1.9436297016284243e-09\n",
            "Epoch: 566/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 567/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 1.7276708952351783e-09\n",
            "Epoch: 568/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 8.638354476175891e-10\n",
            "Epoch: 569/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 3.6713003748189976e-09\n",
            "Epoch: 570/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 571/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 2.8074651492460134e-09\n",
            "Epoch: 572/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 573/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 574/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 575/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 2.8074649272014085e-09\n",
            "Epoch: 576/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 8.638354476175891e-10\n",
            "Epoch: 577/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 8.638354476175891e-10\n",
            "Epoch: 578/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 579/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 580/1000  -  Loss: 5.8517985343933105 \t Rec: 5.8517985343933105 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 581/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 582/1000  -  Loss: 5.851813793182373 \t Rec: 5.851813793182373 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 583/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 584/1000  -  Loss: 5.8517985343933105 \t Rec: 5.8517985343933105 \t KL: 4.3191774601325506e-09\n",
            "Epoch: 585/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 3.4553417904703565e-09\n",
            "Epoch: 586/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 2.1595885080216703e-09\n",
            "Epoch: 587/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 588/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 8.638354476175891e-10\n",
            "Epoch: 589/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 1.5117119778196297e-09\n",
            "Epoch: 590/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 591/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 592/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 1.9436297016284243e-09\n",
            "Epoch: 593/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 594/1000  -  Loss: 5.851798057556152 \t Rec: 5.851798057556152 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 595/1000  -  Loss: 5.851811408996582 \t Rec: 5.851811408996582 \t KL: 8.638354476175891e-10\n",
            "Epoch: 596/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 0.0\n",
            "Epoch: 597/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 598/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 6.478765857131918e-10\n",
            "Epoch: 599/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 8.638354476175891e-10\n",
            "Epoch: 600/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 8.638354476175891e-10\n",
            "Epoch: 601/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 602/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 603/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 0.0\n",
            "Epoch: 604/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 0.0\n",
            "Epoch: 605/1000  -  Loss: 5.8517961502075195 \t Rec: 5.8517961502075195 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 606/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 2.8074651492460134e-09\n",
            "Epoch: 607/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 8.638354476175891e-10\n",
            "Epoch: 608/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 609/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 610/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 6.478765857131918e-10\n",
            "Epoch: 611/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 1.9436297016284243e-09\n",
            "Epoch: 612/1000  -  Loss: 5.851797580718994 \t Rec: 5.851797580718994 \t KL: 6.478765857131918e-10\n",
            "Epoch: 613/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 6.478765857131918e-10\n",
            "Epoch: 614/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 615/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 1.7276708952351783e-09\n",
            "Epoch: 616/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 8.638354476175891e-10\n",
            "Epoch: 617/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 618/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 6.478765857131918e-10\n",
            "Epoch: 619/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 620/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 621/1000  -  Loss: 5.85181188583374 \t Rec: 5.85181188583374 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 622/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 2.8074651492460134e-09\n",
            "Epoch: 623/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 8.638354476175891e-10\n",
            "Epoch: 624/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 625/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 8.638354476175891e-10\n",
            "Epoch: 626/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 6.478765857131918e-10\n",
            "Epoch: 627/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 8.638354476175891e-10\n",
            "Epoch: 628/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 629/1000  -  Loss: 5.851811408996582 \t Rec: 5.851811408996582 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 630/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 8.638354476175891e-10\n",
            "Epoch: 631/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 632/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 633/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 634/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 8.638354476175891e-10\n",
            "Epoch: 635/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 3.0234241776838644e-09\n",
            "Epoch: 636/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 0.0\n",
            "Epoch: 637/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 2.1595885080216703e-09\n",
            "Epoch: 638/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 639/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 6.478765857131918e-10\n",
            "Epoch: 640/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 8.638354476175891e-10\n",
            "Epoch: 641/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 8.638354476175891e-10\n",
            "Epoch: 642/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 8.638354476175891e-10\n",
            "Epoch: 643/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 644/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 645/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 1.7276708952351783e-09\n",
            "Epoch: 646/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 3.8872598473460585e-09\n",
            "Epoch: 647/1000  -  Loss: 5.851798057556152 \t Rec: 5.851798057556152 \t KL: 8.638354476175891e-10\n",
            "Epoch: 648/1000  -  Loss: 5.851798057556152 \t Rec: 5.851798057556152 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 649/1000  -  Loss: 5.851813316345215 \t Rec: 5.851813316345215 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 650/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 0.0\n",
            "Epoch: 651/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 1.5117120888419322e-09\n",
            "Epoch: 652/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 653/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 2.8074651492460134e-09\n",
            "Epoch: 654/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 3.0234239556392595e-09\n",
            "Epoch: 655/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 656/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 657/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 658/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 659/1000  -  Loss: 5.851813793182373 \t Rec: 5.851813793182373 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 660/1000  -  Loss: 5.851797580718994 \t Rec: 5.851797580718994 \t KL: 8.638354476175891e-10\n",
            "Epoch: 661/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 3.0234241776838644e-09\n",
            "Epoch: 662/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 0.0\n",
            "Epoch: 663/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 8.638354476175891e-10\n",
            "Epoch: 664/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 665/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 2.1595885080216703e-09\n",
            "Epoch: 666/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 2.3755475364595213e-09\n",
            "Epoch: 667/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 668/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 669/1000  -  Loss: 5.8517985343933105 \t Rec: 5.8517985343933105 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 670/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 6.478765857131918e-10\n",
            "Epoch: 671/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 8.638354476175891e-10\n",
            "Epoch: 672/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 673/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 674/1000  -  Loss: 5.851796627044678 \t Rec: 5.851796627044678 \t KL: 0.0\n",
            "Epoch: 675/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 2.8074649272014085e-09\n",
            "Epoch: 676/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 677/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 678/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 679/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 680/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 6.478765857131918e-10\n",
            "Epoch: 681/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 1.9436297016284243e-09\n",
            "Epoch: 682/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 8.638354476175891e-10\n",
            "Epoch: 683/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 684/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 8.638354476175891e-10\n",
            "Epoch: 685/1000  -  Loss: 5.851811408996582 \t Rec: 5.851811408996582 \t KL: 2.1595882859770654e-09\n",
            "Epoch: 686/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 6.478765857131918e-10\n",
            "Epoch: 687/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 688/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 8.638354476175891e-10\n",
            "Epoch: 689/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 2.8074651492460134e-09\n",
            "Epoch: 690/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 1.5117119778196297e-09\n",
            "Epoch: 691/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 8.638354476175891e-10\n",
            "Epoch: 692/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 6.478765857131918e-10\n",
            "Epoch: 693/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 694/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 2.3755473144149164e-09\n",
            "Epoch: 695/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 6.478765857131918e-10\n",
            "Epoch: 696/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 8.638354476175891e-10\n",
            "Epoch: 697/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 698/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 699/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 700/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 8.638354476175891e-10\n",
            "Epoch: 701/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 6.478765857131918e-10\n",
            "Epoch: 702/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 2.3755475364595213e-09\n",
            "Epoch: 703/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 2.1595887300662753e-09\n",
            "Epoch: 704/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 8.638354476175891e-10\n",
            "Epoch: 705/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 0.0\n",
            "Epoch: 706/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 707/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 708/1000  -  Loss: 5.851811408996582 \t Rec: 5.851811408996582 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 709/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 710/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 711/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 6.478765857131918e-10\n",
            "Epoch: 712/1000  -  Loss: 5.85181188583374 \t Rec: 5.85181188583374 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 713/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 2.8074651492460134e-09\n",
            "Epoch: 714/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 715/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 8.638354476175891e-10\n",
            "Epoch: 716/1000  -  Loss: 5.851796627044678 \t Rec: 5.851796627044678 \t KL: 1.5117120888419322e-09\n",
            "Epoch: 717/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 2.5915063428527674e-09\n",
            "Epoch: 718/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 2.5915063428527674e-09\n",
            "Epoch: 719/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 720/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 0.0\n",
            "Epoch: 721/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 1.5117120888419322e-09\n",
            "Epoch: 722/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 723/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 724/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 725/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 726/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 727/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 3.6713003748189976e-09\n",
            "Epoch: 728/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 729/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 730/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 731/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 732/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 733/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 1.9436297016284243e-09\n",
            "Epoch: 734/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 735/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 736/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 737/1000  -  Loss: 5.851794242858887 \t Rec: 5.851794242858887 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 738/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 1.7276708952351783e-09\n",
            "Epoch: 739/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 6.478765857131918e-10\n",
            "Epoch: 740/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 8.638354476175891e-10\n",
            "Epoch: 741/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 742/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 6.478765857131918e-10\n",
            "Epoch: 743/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 744/1000  -  Loss: 5.851798057556152 \t Rec: 5.851798057556152 \t KL: 0.0\n",
            "Epoch: 745/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 0.0\n",
            "Epoch: 746/1000  -  Loss: 5.851811408996582 \t Rec: 5.851811408996582 \t KL: 1.5117119778196297e-09\n",
            "Epoch: 747/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 748/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 749/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 2.5915063428527674e-09\n",
            "Epoch: 750/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 0.0\n",
            "Epoch: 751/1000  -  Loss: 5.8517985343933105 \t Rec: 5.8517985343933105 \t KL: 8.638354476175891e-10\n",
            "Epoch: 752/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 2.8074651492460134e-09\n",
            "Epoch: 753/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 754/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 8.638354476175891e-10\n",
            "Epoch: 755/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 1.7276708952351783e-09\n",
            "Epoch: 756/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 8.638354476175891e-10\n",
            "Epoch: 757/1000  -  Loss: 5.851811408996582 \t Rec: 5.851811408996582 \t KL: 4.103218209650095e-09\n",
            "Epoch: 758/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 6.478765857131918e-10\n",
            "Epoch: 759/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 760/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 8.638354476175891e-10\n",
            "Epoch: 761/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 762/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 2.5915063428527674e-09\n",
            "Epoch: 763/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 0.0\n",
            "Epoch: 764/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 765/1000  -  Loss: 5.85181188583374 \t Rec: 5.85181188583374 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 766/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 8.638354476175891e-10\n",
            "Epoch: 767/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 768/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 769/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 2.5915061208081624e-09\n",
            "Epoch: 770/1000  -  Loss: 5.851798057556152 \t Rec: 5.851798057556152 \t KL: 1.9436294795838194e-09\n",
            "Epoch: 771/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 3.0234239556392595e-09\n",
            "Epoch: 772/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 1.9436297016284243e-09\n",
            "Epoch: 773/1000  -  Loss: 5.8517985343933105 \t Rec: 5.8517985343933105 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 774/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 2.1595885080216703e-09\n",
            "Epoch: 775/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 776/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 0.0\n",
            "Epoch: 777/1000  -  Loss: 5.85181188583374 \t Rec: 5.85181188583374 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 778/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 6.478765857131918e-10\n",
            "Epoch: 779/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 0.0\n",
            "Epoch: 780/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 1.9436297016284243e-09\n",
            "Epoch: 781/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 782/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 8.638354476175891e-10\n",
            "Epoch: 783/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 2.3755473144149164e-09\n",
            "Epoch: 784/1000  -  Loss: 5.851795196533203 \t Rec: 5.851795196533203 \t KL: 0.0\n",
            "Epoch: 785/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 786/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 8.638354476175891e-10\n",
            "Epoch: 787/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 788/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 789/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 790/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 2.5915063428527674e-09\n",
            "Epoch: 791/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 792/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 793/1000  -  Loss: 5.851797580718994 \t Rec: 5.851797580718994 \t KL: 1.7276707842128758e-09\n",
            "Epoch: 794/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 1.7276708952351783e-09\n",
            "Epoch: 795/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 796/1000  -  Loss: 5.851811408996582 \t Rec: 5.851811408996582 \t KL: 1.5117119778196297e-09\n",
            "Epoch: 797/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 798/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 6.478765857131918e-10\n",
            "Epoch: 799/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 800/1000  -  Loss: 5.851812839508057 \t Rec: 5.851812839508057 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 801/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 802/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 6.046848355367729e-09\n",
            "Epoch: 803/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 6.478765857131918e-10\n",
            "Epoch: 804/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 6.478765857131918e-10\n",
            "Epoch: 805/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 806/1000  -  Loss: 5.85181188583374 \t Rec: 5.85181188583374 \t KL: 1.5117119778196297e-09\n",
            "Epoch: 807/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 808/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 1.7276708952351783e-09\n",
            "Epoch: 809/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 3.2393827620325055e-09\n",
            "Epoch: 810/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 2.5915061208081624e-09\n",
            "Epoch: 811/1000  -  Loss: 5.851813316345215 \t Rec: 5.851813316345215 \t KL: 1.5117120888419322e-09\n",
            "Epoch: 812/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 8.638354476175891e-10\n",
            "Epoch: 813/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 1.5117120888419322e-09\n",
            "Epoch: 814/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 815/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 816/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 817/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 818/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 6.478765857131918e-10\n",
            "Epoch: 819/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 2.5915063428527674e-09\n",
            "Epoch: 820/1000  -  Loss: 5.8518147468566895 \t Rec: 5.8518147468566895 \t KL: 2.1595885080216703e-09\n",
            "Epoch: 821/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 822/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 2.5915063428527674e-09\n",
            "Epoch: 823/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 1.5117119778196297e-09\n",
            "Epoch: 824/1000  -  Loss: 5.851795196533203 \t Rec: 5.851795196533203 \t KL: 1.9436297016284243e-09\n",
            "Epoch: 825/1000  -  Loss: 5.851812839508057 \t Rec: 5.851812839508057 \t KL: 2.3755475364595213e-09\n",
            "Epoch: 826/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 3.0234239556392595e-09\n",
            "Epoch: 827/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 8.638354476175891e-10\n",
            "Epoch: 828/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 8.638354476175891e-10\n",
            "Epoch: 829/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 830/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 831/1000  -  Loss: 5.851813316345215 \t Rec: 5.851813316345215 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 832/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 833/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 834/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 835/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 836/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 837/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 1.7276708952351783e-09\n",
            "Epoch: 838/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 6.478765857131918e-10\n",
            "Epoch: 839/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 2.3755475364595213e-09\n",
            "Epoch: 840/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 2.3755475364595213e-09\n",
            "Epoch: 841/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 842/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 843/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 844/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 8.638354476175891e-10\n",
            "Epoch: 845/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 6.478765857131918e-10\n",
            "Epoch: 846/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 8.638354476175891e-10\n",
            "Epoch: 847/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 8.638354476175891e-10\n",
            "Epoch: 848/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 1.7276707842128758e-09\n",
            "Epoch: 849/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 850/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 2.3755473144149164e-09\n",
            "Epoch: 851/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 852/1000  -  Loss: 5.8517961502075195 \t Rec: 5.8517961502075195 \t KL: 8.638354476175891e-10\n",
            "Epoch: 853/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 1.5117120888419322e-09\n",
            "Epoch: 854/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 6.478765857131918e-10\n",
            "Epoch: 855/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 856/1000  -  Loss: 5.851813793182373 \t Rec: 5.851813793182373 \t KL: 0.0\n",
            "Epoch: 857/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 858/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 6.478765857131918e-10\n",
            "Epoch: 859/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 860/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 2.8074651492460134e-09\n",
            "Epoch: 861/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 862/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 863/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 864/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 865/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 6.478765857131918e-10\n",
            "Epoch: 866/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 8.638354476175891e-10\n",
            "Epoch: 867/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 868/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 869/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 6.478765857131918e-10\n",
            "Epoch: 870/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 871/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 0.0\n",
            "Epoch: 872/1000  -  Loss: 5.85181188583374 \t Rec: 5.85181188583374 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 873/1000  -  Loss: 5.851791858673096 \t Rec: 5.851791858673096 \t KL: 2.8074651492460134e-09\n",
            "Epoch: 874/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 1.5117120888419322e-09\n",
            "Epoch: 875/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 3.0234241776838644e-09\n",
            "Epoch: 876/1000  -  Loss: 5.851813793182373 \t Rec: 5.851813793182373 \t KL: 1.7276708952351783e-09\n",
            "Epoch: 877/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 8.638354476175891e-10\n",
            "Epoch: 878/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 879/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 3.0234241776838644e-09\n",
            "Epoch: 880/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 881/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 6.478765857131918e-10\n",
            "Epoch: 882/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 8.638354476175891e-10\n",
            "Epoch: 883/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 884/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 8.638354476175891e-10\n",
            "Epoch: 885/1000  -  Loss: 5.851811408996582 \t Rec: 5.851811408996582 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 886/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 1.5117119778196297e-09\n",
            "Epoch: 887/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 0.0\n",
            "Epoch: 888/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 889/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 890/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 891/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 1.5117120888419322e-09\n",
            "Epoch: 892/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 8.638354476175891e-10\n",
            "Epoch: 893/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 3.2393827620325055e-09\n",
            "Epoch: 894/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 895/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 0.0\n",
            "Epoch: 896/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 2.1595887300662753e-09\n",
            "Epoch: 897/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 6.478765857131918e-10\n",
            "Epoch: 898/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 899/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 2.5915061208081624e-09\n",
            "Epoch: 900/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 1.7276708952351783e-09\n",
            "Epoch: 901/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 902/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 6.478765857131918e-10\n",
            "Epoch: 903/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 904/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 4.535135822436587e-09\n",
            "Epoch: 905/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 2.5915065648973723e-09\n",
            "Epoch: 906/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 1.9436297016284243e-09\n",
            "Epoch: 907/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 908/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 6.478765857131918e-10\n",
            "Epoch: 909/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 8.638354476175891e-10\n",
            "Epoch: 910/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 8.638354476175891e-10\n",
            "Epoch: 911/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 0.0\n",
            "Epoch: 912/1000  -  Loss: 5.8517961502075195 \t Rec: 5.8517961502075195 \t KL: 5.183012685705535e-09\n",
            "Epoch: 913/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 4.9670543234014986e-09\n",
            "Epoch: 914/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 1.5117120888419322e-09\n",
            "Epoch: 915/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 916/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 6.478765857131918e-10\n",
            "Epoch: 917/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 8.638354476175891e-10\n",
            "Epoch: 918/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 2.8074651492460134e-09\n",
            "Epoch: 919/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 920/1000  -  Loss: 5.85181188583374 \t Rec: 5.85181188583374 \t KL: 1.9436297016284243e-09\n",
            "Epoch: 921/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 6.478765857131918e-10\n",
            "Epoch: 922/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 1.5117120888419322e-09\n",
            "Epoch: 923/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 1.5117119778196297e-09\n",
            "Epoch: 924/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 2.1595887300662753e-09\n",
            "Epoch: 925/1000  -  Loss: 5.8518147468566895 \t Rec: 5.8518147468566895 \t KL: 2.8074651492460134e-09\n",
            "Epoch: 926/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 2.3755475364595213e-09\n",
            "Epoch: 927/1000  -  Loss: 5.8517985343933105 \t Rec: 5.8517985343933105 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 928/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 929/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 0.0\n",
            "Epoch: 930/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 931/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 932/1000  -  Loss: 5.851797580718994 \t Rec: 5.851797580718994 \t KL: 6.478765857131918e-10\n",
            "Epoch: 933/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 0.0\n",
            "Epoch: 934/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 8.638354476175891e-10\n",
            "Epoch: 935/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 1.5117119778196297e-09\n",
            "Epoch: 936/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 6.478765857131918e-10\n",
            "Epoch: 937/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 2.1595885080216703e-09\n",
            "Epoch: 938/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 939/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 8.638354476175891e-10\n",
            "Epoch: 940/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 8.638354476175891e-10\n",
            "Epoch: 941/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 1.5117120888419322e-09\n",
            "Epoch: 942/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 2.1595887300662753e-09\n",
            "Epoch: 943/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 3.4553417904703565e-09\n",
            "Epoch: 944/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 945/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 946/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 1.5117119778196297e-09\n",
            "Epoch: 947/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 1.5117120888419322e-09\n",
            "Epoch: 948/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 1.7276708952351783e-09\n",
            "Epoch: 949/1000  -  Loss: 5.851812839508057 \t Rec: 5.851812839508057 \t KL: 8.638354476175891e-10\n",
            "Epoch: 950/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 2.3755475364595213e-09\n",
            "Epoch: 951/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 952/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 953/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 954/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 8.638354476175891e-10\n",
            "Epoch: 955/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 8.638354476175891e-10\n",
            "Epoch: 956/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 8.638354476175891e-10\n",
            "Epoch: 957/1000  -  Loss: 5.851801872253418 \t Rec: 5.851801872253418 \t KL: 6.478765857131918e-10\n",
            "Epoch: 958/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 8.638354476175891e-10\n",
            "Epoch: 959/1000  -  Loss: 5.851815700531006 \t Rec: 5.851815700531006 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 960/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 961/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 0.0\n",
            "Epoch: 962/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 963/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 1.5117119778196297e-09\n",
            "Epoch: 964/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 8.638354476175891e-10\n",
            "Epoch: 965/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 2.1595887300662753e-09\n",
            "Epoch: 966/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 8.638354476175891e-10\n",
            "Epoch: 967/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 2.1595887300662753e-09\n",
            "Epoch: 968/1000  -  Loss: 5.851797580718994 \t Rec: 5.851797580718994 \t KL: 2.8074651492460134e-09\n",
            "Epoch: 969/1000  -  Loss: 5.851810455322266 \t Rec: 5.851810455322266 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 970/1000  -  Loss: 5.851803302764893 \t Rec: 5.851803302764893 \t KL: 2.5915063428527674e-09\n",
            "Epoch: 971/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 1.9436297016284243e-09\n",
            "Epoch: 972/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 973/1000  -  Loss: 5.851800441741943 \t Rec: 5.851800441741943 \t KL: 8.638354476175891e-10\n",
            "Epoch: 974/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 2.8074649272014085e-09\n",
            "Epoch: 975/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 8.638354476175891e-10\n",
            "Epoch: 976/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 977/1000  -  Loss: 5.851809501647949 \t Rec: 5.851809501647949 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 978/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 979/1000  -  Loss: 5.851805210113525 \t Rec: 5.851805210113525 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 980/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 6.478765857131918e-10\n",
            "Epoch: 981/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 982/1000  -  Loss: 5.851815223693848 \t Rec: 5.851815223693848 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 983/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 1.7276707842128758e-09\n",
            "Epoch: 984/1000  -  Loss: 5.851806163787842 \t Rec: 5.851806163787842 \t KL: 8.638354476175891e-10\n",
            "Epoch: 985/1000  -  Loss: 5.851806640625 \t Rec: 5.851806640625 \t KL: 0.0\n",
            "Epoch: 986/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 987/1000  -  Loss: 5.851802825927734 \t Rec: 5.851802825927734 \t KL: 1.0797943650331376e-09\n",
            "Epoch: 988/1000  -  Loss: 5.85180139541626 \t Rec: 5.85180139541626 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 989/1000  -  Loss: 5.851807117462158 \t Rec: 5.851807117462158 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 990/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 1.7276708952351783e-09\n",
            "Epoch: 991/1000  -  Loss: 5.851799964904785 \t Rec: 5.851799964904785 \t KL: 4.3191772380879456e-10\n",
            "Epoch: 992/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 1.9436297016284243e-09\n",
            "Epoch: 993/1000  -  Loss: 5.851808071136475 \t Rec: 5.851808071136475 \t KL: 6.478765857131918e-10\n",
            "Epoch: 994/1000  -  Loss: 5.851803779602051 \t Rec: 5.851803779602051 \t KL: 1.7276708952351783e-09\n",
            "Epoch: 995/1000  -  Loss: 5.851804733276367 \t Rec: 5.851804733276367 \t KL: 8.638354476175891e-10\n",
            "Epoch: 996/1000  -  Loss: 5.851799488067627 \t Rec: 5.851799488067627 \t KL: 1.7276708952351783e-09\n",
            "Epoch: 997/1000  -  Loss: 5.851809978485107 \t Rec: 5.851809978485107 \t KL: 2.1595886190439728e-10\n",
            "Epoch: 998/1000  -  Loss: 5.851811408996582 \t Rec: 5.851811408996582 \t KL: 1.2957531714263837e-09\n",
            "Epoch: 999/1000  -  Loss: 5.851808547973633 \t Rec: 5.851808547973633 \t KL: 8.638354476175891e-10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "num_topics = 5\n",
        "t_hidden_size = 100\n",
        "rho_size = len(embedding_data[0])\n",
        "emb_size = len(embedding_data[0])\n",
        "theta_act = \"relu\"\n",
        "\n",
        "class TrainArguments:\n",
        "      def __init__(self, epochs, batch_size, log_interval):\n",
        "          self.epochs = epochs\n",
        "          self.batch_size = batch_size\n",
        "          self.log_interval = log_interval\n",
        "\n",
        "class OptimizerArguments:\n",
        "      def __init__(self, optimizer_name, lr, wdecay):\n",
        "            self.optimizer = optimizer_name\n",
        "            self.lr = lr\n",
        "            self.wdecay = wdecay\n",
        "            \n",
        "train_args = TrainArguments(epochs=1000, batch_size=6, log_interval=None)\n",
        "optimizer_args = OptimizerArguments(optimizer_name=\"adam\", lr=0.001, wdecay=0.1)\n",
        "\n",
        "print(train_args.epochs)\n",
        "print(optimizer_args.optimizer)\n",
        "\n",
        "training_set = train_set\n",
        "\n",
        "# define the ETM-model with setting-parameters\n",
        "etm_model = ETM(\n",
        "      num_topics, \n",
        "      vocab_size, \n",
        "      t_hidden_size, rho_size, emb_size, theta_act, \n",
        "      embedding_data, \n",
        "      enc_drop=0.5)\n",
        "\n",
        "print(etm_model)\n",
        "\n",
        "# start training\n",
        "train_class = TrainETM().train(\n",
        "    etm_model,\n",
        "    vocab_size, \n",
        "    train_args, optimizer_args, training_set, \n",
        "    normalize_data = True) \n",
        "    #num_topics, t_hidden_size, rho_size, emb_size, theta_act, embedding_data, 0.5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "notebook_replication.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}