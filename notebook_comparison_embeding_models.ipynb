{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleichen und Kontrollieren die ähnlichen Wörter von unterschiedlichen Embeddings-Methoden\n",
    "\n",
    "1. Word2Vec-Methoden: CBOW und Skipgram möglich. Nach dem Paper benutzen wir Skipgram für unser Experiment\n",
    "2. Für das neue Experiment wird BERT-Modell verwendet, um Wortembeddings zu erstellen. Das ist ein mehrschritten-Prozess: Durchschnitt. Die genaue Berechnung wird in dem Bericht beschrieben\n",
    "3. Durch mehrere Schritten Durchschnitt sind die semantischen Ähnlichkeiten noch bebeihalten. \n",
    "4. Die genaue Implementierung für Embeddings in dem `src\\embedding.py` und `src/bert*` zu sehen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenvorbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da wir Skipgramm und Bert vergleichen möchten, stellen wir in dem textsloader.preprocess, da Bert später auch mit benutzt wird:\n",
    "1. use_bert_embedding = True\n",
    "2. Ziel: damit wir das Vocabular konsitent haben können"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading texts: ...\n",
      "train-size after loading: 11314\n",
      "test-size after loading: 7532\n",
      "finished load!\n",
      "start: preprocessing: ...\n",
      "preprocssing step: remove stopwords\n",
      "will use bert embedding, so delete words from not_in_bert_vocab.txt\n",
      "finised: preprocessing!\n",
      "vocab-size in df: 8473\n",
      "preprocessing remove stopwords from vocabulary\n",
      "start creating vocabulary ...\n",
      "length of the vocabulary: 8473\n",
      "length word2id list: 8473\n",
      "length id2word list: 8473\n",
      "finished: creating vocabulary\n",
      "train-size-after-all: 11214\n",
      "test-size-after-all: 7532\n",
      "validation-size-after-all: 100\n",
      "test-size-after-all: 11214\n",
      "test-indices-length: 11214\n",
      "test-size-after-all: 100\n",
      "test-indices-length: 100\n",
      "test-size-after-all: 7532\n",
      "test-indices-length: 7532\n",
      "length train-documents-indices : 1146648\n",
      "length of the vocabulary: 8473\n",
      "\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "from src.prepare_dataset import TextDataLoader\n",
    "# init TextDataLoader für die Datenquelle 20 News Groups\n",
    "# Daten abrufen vom Sklearn, tokenisieren und besondere Charaktern entfernen\n",
    "textsloader = TextDataLoader(source=\"20newsgroups\", train_size=None, test_size=None)\n",
    "textsloader.load_tokenize_texts(\"20newsgroups\")\n",
    "# Vorverarbeitung von Daten mit folgenden Schritten:\n",
    "textsloader.preprocess_texts(length_one_remove=True, \n",
    "                             punctuation_lower = True, \n",
    "                             stopwords_filter = True,\n",
    "                             use_bert_embedding = True)\n",
    "# Daten zerlegen für Train, Test und Validation. Erstellen Vocabular aus dem Trainset\n",
    "min_df= 30\n",
    "textsloader.split_and_create_voca_from_trainset(max_df=0.7, \n",
    "                                                min_df=min_df, \n",
    "                                                stopwords_remove_from_voca=True)\n",
    "\n",
    "# Erstellen BOW-Repräsentation für ETM Modell\n",
    "for_lda_model = False\n",
    "word2id, id2word, train_set, test_set, val_set = textsloader.create_bow_and_savebow_for_each_set(for_lda_model=for_lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary after prprocessing ist: 8473\n",
      "Size of train set: 11214\n",
      "Size of val set: 100\n",
      "Size of test set: 7532\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text-after-preprocessing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>biochem nwu jackson swimming pool defense nntp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apollo hp red herring police state usa nntp po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hades coos dartmouth brian hughes installing r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jaeger buphy bu gregg jaeger rushdie boston un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>king eng umd doug boom computer design lab mar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>physics ca campbell pc windows os unix reply p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ncr jim sharp parts information distribution w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>sera zuma serdar argic nazi germany armenians ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>chips astro temple charlie mathew bible resear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>loss ece cmu doug loss crazy electrical comput...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text-after-preprocessing\n",
       "0   biochem nwu jackson swimming pool defense nntp...\n",
       "1   apollo hp red herring police state usa nntp po...\n",
       "2   hades coos dartmouth brian hughes installing r...\n",
       "3   jaeger buphy bu gregg jaeger rushdie boston un...\n",
       "4   king eng umd doug boom computer design lab mar...\n",
       "..                                                ...\n",
       "95  physics ca campbell pc windows os unix reply p...\n",
       "96  ncr jim sharp parts information distribution w...\n",
       "97  sera zuma serdar argic nazi germany armenians ...\n",
       "98  chips astro temple charlie mathew bible resear...\n",
       "99  loss ece cmu doug loss crazy electrical comput...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kontrollieren die Größen von verschiedenen Datensätzen\n",
    "print(f'Size of the vocabulary after prprocessing ist: {len(textsloader.vocabulary)}')\n",
    "print(f'Size of train set: {len(train_set[\"tokens\"])}')\n",
    "print(f'Size of val set: {len(val_set[\"tokens\"])}')\n",
    "print(f'Size of test set: {len(test_set[\"test\"][\"tokens\"])}')\n",
    "\n",
    "# re-erstellen von Dokumenten nach der Vorverarbeitungen. Die Dokumenten sind in Wörtern und werden für Word-Embedding Training benutzt\n",
    "docs_tr, docs_t, docs_v = textsloader.get_docs_in_words_for_each_set()\n",
    "del docs_t\n",
    "del docs_v\n",
    "train_docs_df = pd.DataFrame()\n",
    "train_docs_df['text-after-preprocessing'] = [' '.join(doc) for doc in docs_tr[:100]]\n",
    "train_docs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordEmbedding mit Skipgramm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Trainieren Word-Embedding \n",
    "2. Speichern in txt File\n",
    "3. Wiederaufrufen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.embedding import WordEmbeddingCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train word-embedding with skipgram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 206/8473 [00:00<00:04, 2053.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vocabulary from word-embedding with skipgram: 8473\n",
      "length of vocabulary after creating BOW: 8473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8473/8473 [00:04<00:00, 1896.80it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = list(word2id.keys()) # vocabulary after preprocessing and creating bow\n",
    "word2vec_model = 'skipgram'\n",
    "\n",
    "save_path = Path.joinpath(Path.cwd(), f'prepared_data/min_df_{min_df}')\n",
    "figures_path = Path.joinpath(Path.cwd(), f'figures/min_df_{min_df}')\n",
    "Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(figures_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if word2vec_model!=\"bert\":\n",
    "    # only for cbow and skipgram model\n",
    "    wb_creator = WordEmbeddingCreator(model_name=word2vec_model, documents = docs_tr, save_path= save_path)\n",
    "    wb_creator.train(min_count=0, embedding_size= 300)\n",
    "    wb_creator.create_and_save_vocab_embedding(vocab, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: grip - vector: [-0.073809646, 0.04122088, -0.020229047, -0.03743437, 0.061036717] \n"
     ]
    }
   ],
   "source": [
    "print(f'word: {vocab[1]} - vector: {list(wb_creator.model.wv.__getitem__(vocab[0]))[:5]} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleichen ähnliche Wörter von Word2Vec (gensim und eigene Cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word-embedding of the word-- consumers: \n",
      "dim of vector: 300\n"
     ]
    }
   ],
   "source": [
    "v = vocab[11]\n",
    "vec = list(wb_creator.model.wv.__getitem__(v))\n",
    "print(f'word-embedding of the word-- {v}: ')\n",
    "print(f'dim of vector: {len(vec)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reviewed', 0.9035565853118896),\n",
       " ('customers', 0.9017565250396729),\n",
       " ('privately', 0.9014784693717957),\n",
       " ('fees', 0.8981140851974487),\n",
       " ('payment', 0.8965352773666382),\n",
       " ('overseas', 0.8964957594871521),\n",
       " ('licence', 0.8935616612434387),\n",
       " ('paperwork', 0.8847388029098511),\n",
       " ('invest', 0.8807591795921326),\n",
       " ('maintaining', 0.8789840936660767)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using gensim function\n",
    "wb_creator.find_most_similar_words(n_neighbor=10, word=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reviewed': 0.9035565,\n",
       " 'customers': 0.9017564,\n",
       " 'privately': 0.9014784,\n",
       " 'fees': 0.898114,\n",
       " 'payment': 0.8965352,\n",
       " 'overseas': 0.89649564,\n",
       " 'licence': 0.8935616,\n",
       " 'paperwork': 0.88473874,\n",
       " 'invest': 0.88075924,\n",
       " 'maintaining': 0.878984}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using self-implemented cosine\n",
    "wb_creator.find_similar_words_self_implemented(10, vocab, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleich ähnliche Wörter zwischen Word2Vec und Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104008\n"
     ]
    }
   ],
   "source": [
    "# only bert_vocab, not embeddings in this file\n",
    "with open('prepared_data/bert_vocab.txt') as f:\n",
    "    lines = f.readlines()\n",
    "readed_bert_vocab = [e.split(\"\\n\")[0] for e in lines]\n",
    "print(len(readed_bert_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read word-embeddings with bert from file...\n",
      "bert-embedding ready!\n"
     ]
    }
   ],
   "source": [
    "from src.embedding import BertEmbedding\n",
    "bert_eb = BertEmbedding('prepared_data') #directory, where the txt.file of bert_vocab_embedding.txt ist\n",
    "bert_eb.get_bert_embeddings(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8473, 768)\n"
     ]
    }
   ],
   "source": [
    "print(bert_eb.bert_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controll the consitence of vocabulary\n",
    "for w in vocab:\n",
    "    if w not in bert_eb.bert_vocab:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'consumer': 0.8973592162010428,\n",
       " 'customers': 0.8523259162439452,\n",
       " 'customer': 0.8231497132300408,\n",
       " 'owners': 0.8083975057445063,\n",
       " 'makers': 0.7980602967907122,\n",
       " 'buyer': 0.7979561896060481,\n",
       " 'users': 0.7912219846980949,\n",
       " 'citizens': 0.7895518600746335,\n",
       " 'employees': 0.789212372741686,\n",
       " 'people': 0.7877557850835473}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_eb.find_similar_words(v, 10, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
