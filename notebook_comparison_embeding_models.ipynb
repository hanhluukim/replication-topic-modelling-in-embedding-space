{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleichen und Kontrollieren die ähnlichen Wörter von unterschiedlichen Embeddings-Methoden\n",
    "\n",
    "1. Word2Vec-Methoden: CBOW und Skipgram möglich. Nach dem Paper benutzen wir Skipgram für unser Experiment\n",
    "2. Für das neue Experiment wird BERT-Modell verwendet, um Wortembeddings zu erstellen. Das ist ein mehrschritten-Prozess: Durchschnitt. Die genaue Berechnung wird in dem Bericht beschrieben\n",
    "3. Durch mehrere Schritten Durchschnitt sind die semantischen Ähnlichkeiten noch bebeihalten. \n",
    "4. Die genaue Implementierung für Embeddings in dem `src\\embedding.py` und `src/bert*` zu sehen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenvorbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da wir Skipgramm und Bert vergleichen möchten, stellen wir in dem textsloader.preprocess, da Bert später auch mit benutzt wird:\n",
    "1. use_bert_embedding = True\n",
    "2. Ziel: damit wir das Vocabular konsitent haben können"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading texts: ...\n",
      "train-size after loading: 11314\n",
      "test-size after loading: 7532\n",
      "finished load!\n",
      "start: preprocessing: ...\n",
      "preprocessing step: remove stopwords\n",
      "will use bert embedding, so delete words from not_in_bert_vocab.txt\n",
      "finised: preprocessing!\n",
      "vocab-size in df: 102981\n",
      "preprocessing remove stopwords from vocabulary\n",
      "start creating vocabulary ...\n",
      "length of the vocabulary: 82759\n",
      "length word2id list: 82759\n",
      "length id2word list: 82759\n",
      "finished: creating vocabulary\n",
      "save docs in txt...\n",
      "save docs finished\n",
      "train-size-after-all: 11214\n",
      "test-size-after-all: 7532\n",
      "validation-size-after-all: 100\n",
      "test-size-after-all: 11214\n",
      "test-indices-length: 11214\n",
      "test-size-after-all: 100\n",
      "test-indices-length: 100\n",
      "test-size-after-all: 7532\n",
      "test-indices-length: 7532\n",
      "length train-documents-indices : 1452962\n",
      "length of the vocabulary: 82759\n",
      "\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "from src.prepare_dataset import TextDataLoader\n",
    "# init TextDataLoader für die Datenquelle 20 News Groups\n",
    "# Daten abrufen vom Sklearn, tokenisieren und besondere Charaktern entfernen\n",
    "textsloader = TextDataLoader(source=\"20newsgroups\", train_size=None, test_size=None)\n",
    "textsloader.load_tokenize_texts(\"20newsgroups\")\n",
    "# Vorverarbeitung von Daten mit folgenden Schritten:\n",
    "textsloader.preprocess_texts(length_one_remove=True, \n",
    "                             punctuation_lower = True, \n",
    "                             stopwords_filter = True,\n",
    "                             use_bert_embedding = True)\n",
    "# Daten zerlegen für Train, Test und Validation. Erstellen Vocabular aus dem Trainset\n",
    "min_df= 0\n",
    "textsloader.split_and_create_voca_from_trainset(max_df=0.7, \n",
    "                                                min_df=min_df, \n",
    "                                                stopwords_remove_from_voca=True)\n",
    "\n",
    "# Erstellen BOW-Repräsentation für ETM Modell\n",
    "for_lda_model = False\n",
    "word2id, id2word, train_set, test_set, val_set = textsloader.create_bow_and_savebow_for_each_set(for_lda_model=for_lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary after prprocessing ist: 82759\n",
      "Size of train set: 11214\n",
      "Size of val set: 100\n",
      "Size of test set: 7532\n",
      "save docs in txt...\n",
      "save docs finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text-after-preprocessing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dowdy tochtli biochem nwu dowdy jackson swimmi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goykhman apollo hp red herring police state us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hades coos dartmouth brian hughes installing r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jaeger buphy bu gregg jaeger inimitable rushdi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sysmgr king eng umd doug mohney boom whoosh co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>rcampbel weejordy physics mun ca roderick camp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>jiml strauss ftcollinsco ncr jim sharp parts i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>sera zuma serdar argic nazi germany armenians ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>chips astro ocis temple charlie mathew interdi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>loss ece cmu doug loss crazy imaginitive elect...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text-after-preprocessing\n",
       "0   dowdy tochtli biochem nwu dowdy jackson swimmi...\n",
       "1   goykhman apollo hp red herring police state us...\n",
       "2   hades coos dartmouth brian hughes installing r...\n",
       "3   jaeger buphy bu gregg jaeger inimitable rushdi...\n",
       "4   sysmgr king eng umd doug mohney boom whoosh co...\n",
       "..                                                ...\n",
       "95  rcampbel weejordy physics mun ca roderick camp...\n",
       "96  jiml strauss ftcollinsco ncr jim sharp parts i...\n",
       "97  sera zuma serdar argic nazi germany armenians ...\n",
       "98  chips astro ocis temple charlie mathew interdi...\n",
       "99  loss ece cmu doug loss crazy imaginitive elect...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kontrollieren die Größen von verschiedenen Datensätzen\n",
    "print(f'Size of the vocabulary after prprocessing ist: {len(textsloader.vocabulary)}')\n",
    "print(f'Size of train set: {len(train_set[\"tokens\"])}')\n",
    "print(f'Size of val set: {len(val_set[\"tokens\"])}')\n",
    "print(f'Size of test set: {len(test_set[\"test\"][\"tokens\"])}')\n",
    "\n",
    "# re-erstellen von Dokumenten nach der Vorverarbeitungen. Die Dokumenten sind in Wörtern und werden für Word-Embedding Training benutzt\n",
    "docs_tr, docs_t, docs_v = textsloader.get_docs_in_words_for_each_set()\n",
    "del docs_t\n",
    "del docs_v\n",
    "train_docs_df = pd.DataFrame()\n",
    "train_docs_df['text-after-preprocessing'] = [' '.join(doc) for doc in docs_tr[:100]]\n",
    "train_docs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordEmbedding mit Skipgramm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Trainieren Word-Embedding \n",
    "2. Speichern in txt File\n",
    "3. Wiederaufrufen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.embedding import WordEmbeddingCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train word-embedding with skipgram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/82759 [00:00<09:39, 142.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vocabulary from word-embedding with skipgram: 82759\n",
      "length of vocabulary after creating BOW: 82759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82759/82759 [07:27<00:00, 184.87it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = list(word2id.keys()) # vocabulary after preprocessing and creating bow\n",
    "word2vec_model = 'skipgram'\n",
    "\n",
    "save_path = Path.joinpath(Path.cwd(), f'prepared_data/min_df_{min_df}')\n",
    "figures_path = Path.joinpath(Path.cwd(), f'figures/min_df_{min_df}')\n",
    "Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(figures_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if word2vec_model!=\"bert\":\n",
    "    # only for cbow and skipgram model\n",
    "    wb_creator = WordEmbeddingCreator(model_name=word2vec_model, documents = docs_tr, save_path= save_path)\n",
    "    wb_creator.train(min_count=0, embedding_size= 300)\n",
    "    wb_creator.create_and_save_vocab_embedding(vocab, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: wechsler - vector: [-0.03136295, -0.03053455, 0.0019862694, -0.040926985, -0.03296724] \n"
     ]
    }
   ],
   "source": [
    "print(f'word: {vocab[1]} - vector: {list(wb_creator.model.wv.__getitem__(vocab[0]))[:5]} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleichen ähnliche Wörter von Word2Vec (gensim und eigene Cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word-embedding of the word-- paladin: \n",
      "dim of vector: 300\n"
     ]
    }
   ],
   "source": [
    "v = vocab[9]\n",
    "vec = list(wb_creator.model.wv.__getitem__(v))\n",
    "print(f'word-embedding of the word-- {v}: ')\n",
    "print(f'dim of vector: {len(vec)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('daved', 0.9813060164451599),\n",
       " ('tombaker', 0.971382737159729),\n",
       " ('mkaye', 0.9695613980293274),\n",
       " ('lww', 0.9682618379592896),\n",
       " ('rsilver', 0.9671342372894287),\n",
       " ('wdm', 0.965248167514801),\n",
       " ('artc', 0.9612756967544556),\n",
       " ('aep', 0.959023118019104),\n",
       " ('eastgate', 0.9554911851882935),\n",
       " ('rogerw', 0.9544965028762817)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using gensim function\n",
    "wb_creator.find_most_similar_words(n_neighbor=10, word=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "Ähnliches Wort &  Cosinus-Ähnlichkeit \\\\\n",
      "\\midrule\n",
      "         daved &             0.981306 \\\\\n",
      "      tombaker &             0.971383 \\\\\n",
      "         mkaye &             0.969561 \\\\\n",
      "           lww &             0.968262 \\\\\n",
      "       rsilver &             0.967134 \\\\\n",
      "           wdm &             0.965248 \\\\\n",
      "          artc &             0.961276 \\\\\n",
      "           aep &             0.959023 \\\\\n",
      "      eastgate &             0.955491 \\\\\n",
      "        rogerw &             0.954497 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using self-implemented cosine\n",
    "sw = wb_creator.find_similar_words_self_implemented(10, vocab, v)\n",
    "df = pd.DataFrame()\n",
    "df['Ähnliches Wort'] = list(sw.keys())\n",
    "df['Cosinus-Ähnlichkeit'] = list(sw.values())\n",
    "print(df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del wb_creator\n",
    "del textsloader\n",
    "del word2id\n",
    "del id2word\n",
    "del train_set\n",
    "del test_set\n",
    "del val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleich ähnliche Wörter zwischen Word2Vec und Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104008\n"
     ]
    }
   ],
   "source": [
    "# only bert_vocab, not embeddings in this file\n",
    "with open('prepared_data/bert_vocab.txt') as f:\n",
    "    lines = f.readlines()\n",
    "readed_bert_vocab = [e.split(\"\\n\")[0] for e in lines]\n",
    "print(len(readed_bert_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read word-embeddings with bert from file...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-31923c541531>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbert_eb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'prepared_data'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#directory, where the txt.file of bert_vocab_embedding.txt ist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbert_eb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_bert_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_eb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Sommersemester_2022/Data-Mining-Project/replication-topic-modelling-in-embedding-space/src/embedding.py\u001b[0m in \u001b[0;36mget_bert_embeddings\u001b[0;34m(self, etm_vocab)\u001b[0m\n\u001b[1;32m    263\u001b[0m           \u001b[0;31m# filtering words by etm_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'read word-embeddings with bert from file...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m           \u001b[0mwords_in_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_prefitted_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metm_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_in_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Sommersemester_2022/Data-Mining-Project/replication-topic-modelling-in-embedding-space/src/embedding.py\u001b[0m in \u001b[0;36mread_prefitted_embedding\u001b[0;34m(model_name, vocab, save_path)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#remove \\n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# check again only word in the vocabulary, which was created at bow-creating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                   \u001b[0membedding_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.embedding import BertEmbedding\n",
    "bert_eb = BertEmbedding('prepared_data') #directory, where the txt.file of bert_vocab_embedding.txt ist\n",
    "bert_eb.get_bert_embeddings(vocab)\n",
    "print(bert_eb.bert_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controll the consitence of vocabulary\n",
    "\"\"\"\n",
    "for w in vocab:\n",
    "    if w not in bert_eb.bert_vocab:\n",
    "        print(w)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = bert_eb.find_similar_words(v, 10, vocab)\n",
    "df = pd.DataFrame()\n",
    "df['Ähnliches Wort'] = list(sw.keys())\n",
    "df['Cosinus-Ähnlichkeit'] = list(sw.values())\n",
    "print(df.to_latex(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
