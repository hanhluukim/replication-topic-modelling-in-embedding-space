{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleichen und Kontrollieren die ähnlichen Wörter von unterschiedlichen Embeddings-Methoden\n",
    "\n",
    "1. Word2Vec-Methoden: CBOW und Skipgram möglich. Nach dem Paper benutzen wir Skipgram für unser Experiment\n",
    "2. Für das neue Experiment wird BERT-Modell verwendet, um Wortembeddings zu erstellen. Das ist ein mehrschritten-Prozess: Durchschnitt. Die genaue Berechnung wird in dem Bericht beschrieben\n",
    "3. Durch mehrere Schritten Durchschnitt sind die semantischen Ähnlichkeiten noch bebeihalten. \n",
    "4. Die genaue Implementierung für Embeddings in dem `src\\embedding.py` und `src/bert*` zu sehen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenvorbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da wir Skipgramm und Bert vergleichen möchten, stellen wir in dem textsloader.preprocess, da Bert später auch mit benutzt wird:\n",
    "1. use_bert_embedding = True\n",
    "2. Ziel: damit wir das Vocabular konsitent haben können"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading texts: ...\n",
      "train-size after loading: 11314\n",
      "test-size after loading: 7532\n",
      "finished load!\n",
      "start: preprocessing: ...\n",
      "preprocessing step: remove stopwords\n",
      "will use bert embedding, so delete words from not_in_bert_vocab.txt\n",
      "finised: preprocessing!\n",
      "vocab-size in df: 3095\n",
      "preprocessing remove stopwords from vocabulary\n",
      "start creating vocabulary ...\n",
      "length of the vocabulary: 3095\n",
      "length word2id list: 3095\n",
      "length id2word list: 3095\n",
      "finished: creating vocabulary\n",
      "save docs in txt...\n",
      "save docs finished\n",
      "train-size-after-all: 11214\n",
      "test-size-after-all: 7532\n",
      "validation-size-after-all: 100\n",
      "test-size-after-all: 11214\n",
      "test-indices-length: 11214\n",
      "test-size-after-all: 100\n",
      "test-indices-length: 100\n",
      "test-size-after-all: 7532\n",
      "test-indices-length: 7532\n",
      "length train-documents-indices : 893379\n",
      "length of the vocabulary: 3095\n",
      "\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "from src.prepare_dataset import TextDataLoader\n",
    "# init TextDataLoader für die Datenquelle 20 News Groups\n",
    "# Daten abrufen vom Sklearn, tokenisieren und besondere Charaktern entfernen\n",
    "textsloader = TextDataLoader(source=\"20newsgroups\", train_size=None, test_size=None)\n",
    "textsloader.load_tokenize_texts(\"20newsgroups\")\n",
    "# Vorverarbeitung von Daten mit folgenden Schritten:\n",
    "textsloader.preprocess_texts(length_one_remove=True, \n",
    "                             punctuation_lower = True, \n",
    "                             stopwords_filter = True,\n",
    "                             use_bert_embedding = True)\n",
    "# Daten zerlegen für Train, Test und Validation. Erstellen Vocabular aus dem Trainset\n",
    "min_df= 100\n",
    "textsloader.split_and_create_voca_from_trainset(max_df=0.7, \n",
    "                                                min_df=min_df, \n",
    "                                                stopwords_remove_from_voca=True)\n",
    "\n",
    "# Erstellen BOW-Repräsentation für ETM Modell\n",
    "for_lda_model = False\n",
    "word2id, id2word, train_set, test_set, val_set = textsloader.create_bow_and_savebow_for_each_set(for_lda_model=for_lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary after prprocessing ist: 3095\n",
      "Size of train set: 11214\n",
      "Size of val set: 100\n",
      "Size of test set: 7532\n",
      "save docs in txt...\n",
      "save docs finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text-after-preprocessing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jackson defense nntp posting host university i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apollo hp red police state usa nntp posting ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dartmouth brian hughes installing ram quadra r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bu boston university physics department articl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>king eng umd doug computer design lab maryland...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>physics ca pc windows os unix reply physics ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ncr jim parts information distribution world n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>sera zuma serdar argic nazi germany armenians ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>chips astro temple bible research temple unive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>loss cmu doug loss crazy electrical computer e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text-after-preprocessing\n",
       "0   jackson defense nntp posting host university i...\n",
       "1   apollo hp red police state usa nntp posting ho...\n",
       "2   dartmouth brian hughes installing ram quadra r...\n",
       "3   bu boston university physics department articl...\n",
       "4   king eng umd doug computer design lab maryland...\n",
       "..                                                ...\n",
       "95  physics ca pc windows os unix reply physics ca...\n",
       "96  ncr jim parts information distribution world n...\n",
       "97  sera zuma serdar argic nazi germany armenians ...\n",
       "98  chips astro temple bible research temple unive...\n",
       "99  loss cmu doug loss crazy electrical computer e...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kontrollieren die Größen von verschiedenen Datensätzen\n",
    "print(f'Size of the vocabulary after prprocessing ist: {len(textsloader.vocabulary)}')\n",
    "print(f'Size of train set: {len(train_set[\"tokens\"])}')\n",
    "print(f'Size of val set: {len(val_set[\"tokens\"])}')\n",
    "print(f'Size of test set: {len(test_set[\"test\"][\"tokens\"])}')\n",
    "\n",
    "# re-erstellen von Dokumenten nach der Vorverarbeitungen. Die Dokumenten sind in Wörtern und werden für Word-Embedding Training benutzt\n",
    "docs_tr, docs_t, docs_v = textsloader.get_docs_in_words_for_each_set()\n",
    "del docs_t\n",
    "del docs_v\n",
    "train_docs_df = pd.DataFrame()\n",
    "train_docs_df['text-after-preprocessing'] = [' '.join(doc) for doc in docs_tr[:100]]\n",
    "train_docs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordEmbedding mit Skipgramm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Trainieren Word-Embedding \n",
    "2. Speichern in txt File\n",
    "3. Wiederaufrufen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.embedding import WordEmbeddingCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train word-embedding with skipgram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 401/3095 [00:00<00:00, 4000.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vocabulary from word-embedding with skipgram: 3095\n",
      "length of vocabulary after creating BOW: 3095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3095/3095 [00:01<00:00, 2778.57it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = list(word2id.keys()) # vocabulary after preprocessing and creating bow\n",
    "word2vec_model = 'skipgram'\n",
    "\n",
    "save_path = Path.joinpath(Path.cwd(), f'prepared_data/min_df_{min_df}')\n",
    "figures_path = Path.joinpath(Path.cwd(), f'figures/min_df_{min_df}')\n",
    "Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(figures_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if word2vec_model!=\"bert\":\n",
    "    # only for cbow and skipgram model\n",
    "    wb_creator = WordEmbeddingCreator(model_name=word2vec_model, documents = docs_tr, save_path= save_path)\n",
    "    wb_creator.train(min_count=0, embedding_size= 300)\n",
    "    wb_creator.create_and_save_vocab_embedding(vocab, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: report - vector: [0.09772752, -0.014075972, -0.2555263, -0.056844436, -0.21444546] \n"
     ]
    }
   ],
   "source": [
    "print(f'word: {vocab[1]} - vector: {list(wb_creator.model.wv.__getitem__(vocab[0]))[:5]} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleichen ähnliche Wörter von Word2Vec (gensim und eigene Cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word-embedding of the word-- supported: \n",
      "dim of vector: 300\n"
     ]
    }
   ],
   "source": [
    "v = vocab[9]\n",
    "vec = list(wb_creator.model.wv.__getitem__(v))\n",
    "print(f'word-embedding of the word-- {v}: ')\n",
    "print(f'dim of vector: {len(vec)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('supports', 0.8612833023071289),\n",
       " ('implemented', 0.7653976082801819),\n",
       " ('multi', 0.7261040210723877),\n",
       " ('provided', 0.7183036804199219),\n",
       " ('tools', 0.7166996598243713),\n",
       " ('implementation', 0.7062899470329285),\n",
       " ('implement', 0.6892890930175781),\n",
       " ('capabilities', 0.6522724628448486),\n",
       " ('requirements', 0.6459758281707764),\n",
       " ('multiple', 0.6458479762077332)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using gensim function\n",
    "wb_creator.find_most_similar_words(n_neighbor=10, word=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "Ähnliches Wort &  Cosinus-Ähnlichkeit \\\\\n",
      "\\midrule\n",
      "      supports &             0.861283 \\\\\n",
      "   implemented &             0.765398 \\\\\n",
      "         multi &             0.726104 \\\\\n",
      "      provided &             0.718304 \\\\\n",
      "         tools &             0.716700 \\\\\n",
      "implementation &             0.706290 \\\\\n",
      "     implement &             0.689289 \\\\\n",
      "  capabilities &             0.652272 \\\\\n",
      "  requirements &             0.645976 \\\\\n",
      "      multiple &             0.645848 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using self-implemented cosine\n",
    "sw = wb_creator.find_similar_words_self_implemented(10, vocab, v)\n",
    "df = pd.DataFrame()\n",
    "df['Ähnliches Wort'] = list(sw.keys())\n",
    "df['Cosinus-Ähnlichkeit'] = list(sw.values())\n",
    "print(df.to_latex(index=False))\n",
    "del sw\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del wb_creator\n",
    "del textsloader\n",
    "del word2id\n",
    "del id2word\n",
    "del train_set\n",
    "del test_set\n",
    "del val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleich ähnliche Wörter zwischen Word2Vec und Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104008\n"
     ]
    }
   ],
   "source": [
    "# only bert_vocab, not embeddings in this file\n",
    "with open('prepared_data/bert_vocab.txt') as f:\n",
    "    lines = f.readlines()\n",
    "readed_bert_vocab = [e.split(\"\\n\")[0] for e in lines]\n",
    "print(len(readed_bert_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read word-embeddings with bert from file...\n",
      "prepared_data/bert_vocab_embedding.txt\n",
      "bert-embedding ready!\n",
      "(3095, 768)\n"
     ]
    }
   ],
   "source": [
    "from src.embedding import BertEmbedding\n",
    "bert_eb = BertEmbedding('prepared_data') #directory, where the txt.file of bert_vocab_embedding.txt ist\n",
    "bert_eb.get_bert_embeddings(vocab)\n",
    "print(bert_eb.bert_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3095\n"
     ]
    }
   ],
   "source": [
    "#my vocabular\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find similar words for supported: \n",
      "\n",
      "\\begin{tabular}{lr}\n",
      "\\toprule\n",
      "Ähnliches Wort &  Cosinus-Ähnlichkeit \\\\\n",
      "\\midrule\n",
      "       support &             0.933962 \\\\\n",
      "    supporting &             0.917919 \\\\\n",
      "      supports &             0.881589 \\\\\n",
      "      provided &             0.860958 \\\\\n",
      "   implemented &             0.860854 \\\\\n",
      "      included &             0.856517 \\\\\n",
      "      accepted &             0.845091 \\\\\n",
      "     installed &             0.843016 \\\\\n",
      "     protected &             0.835109 \\\\\n",
      "      approved &             0.834749 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'find similar words for {v}: \\n')\n",
    "sw = bert_eb.find_similar_words(v, 10, vocab)\n",
    "df = pd.DataFrame()\n",
    "df['Ähnliches Wort'] = list(sw.keys())\n",
    "df['Cosinus-Ähnlichkeit'] = list(sw.values())\n",
    "print(df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controll the consitence of vocabulary\n",
    "for w in vocab:\n",
    "    if w not in bert_eb.bert_vocab:\n",
    "        print(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
