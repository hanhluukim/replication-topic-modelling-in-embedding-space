{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleichen und Kontrollieren die ähnlichen Wörter von unterschiedlichen Embeddings-Methoden\n",
    "\n",
    "1. Word2Vec-Methoden: CBOW und Skipgram möglich. Nach dem Paper benutzen wir Skipgram für unser Experiment\n",
    "2. Für das neue Experiment wird BERT-Modell verwendet, um Wortembeddings zu erstellen. Das ist ein mehrschritten-Prozess: Durchschnitt. Die genaue Berechnung wird in dem Bericht beschrieben\n",
    "3. Durch mehrere Schritten Durchschnitt sind die semantischen Ähnlichkeiten noch bebeihalten. \n",
    "4. Die genaue Implementierung für Embeddings in dem `src\\embedding.py` und `src/bert*` zu sehen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenvorbereitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da wir Skipgramm und Bert vergleichen möchten, stellen wir in dem textsloader.preprocess, da Bert später auch mit benutzt wird:\n",
    "1. use_bert_embedding = True\n",
    "2. Ziel: damit wir das Vocabular konsitent haben können"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading texts: ...\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "train-size after loading: 11314\n",
      "test-size after loading: 7532\n",
      "finished load!\n",
      "start: preprocessing: ...\n",
      "will use bert embedding, so delete words from not_in_bert_vocab.txt\n",
      "finised: preprocessing!\n",
      "vocab-size in df: 57723\n",
      "start creating vocabulary ...\n",
      "length of the vocabulary: 54715\n",
      "length word2id list: 54715\n",
      "length id2word list: 54715\n",
      "finished: creating vocabulary\n",
      "save docs in txt...\n",
      "save docs finished\n",
      "train-size-after-all: 11214\n",
      "test-size-after-all: 7532\n",
      "validation-size-after-all: 100\n",
      "test-size-after-all: 11214\n",
      "test-indices-length: 11214\n",
      "test-size-after-all: 100\n",
      "test-indices-length: 100\n",
      "test-size-after-all: 7532\n",
      "test-indices-length: 7532\n",
      "length train-documents-indices : 2379561\n",
      "length of the vocabulary: 54715\n",
      "\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n",
      "start: creating bow representation...\n",
      "finised creating bow input!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "from src.prepare_dataset import TextDataLoader\n",
    "# init TextDataLoader für die Datenquelle 20 News Groups\n",
    "# Daten abrufen vom Sklearn, tokenisieren und besondere Charaktern entfernen\n",
    "textsloader = TextDataLoader(source=\"20newsgroups\", train_size=None, test_size=None)\n",
    "textsloader.load_tokenize_texts(\"20newsgroups\")\n",
    "# Vorverarbeitung von Daten mit folgenden Schritten:\n",
    "textsloader.preprocess_texts(length_one_remove=True, \n",
    "                             punctuation_lower = True, \n",
    "                             stopwords_filter = False,\n",
    "                             use_bert_embedding = True)\n",
    "# Daten zerlegen für Train, Test und Validation. Erstellen Vocabular aus dem Trainset\n",
    "min_df= 2\n",
    "textsloader.split_and_create_voca_from_trainset(max_df=0.7, \n",
    "                                                min_df=min_df, \n",
    "                                                stopwords_remove_from_voca=False)\n",
    "\n",
    "# Erstellen BOW-Repräsentation für ETM Modell\n",
    "for_lda_model = False\n",
    "word2id, id2word, train_set, test_set, val_set = textsloader.create_bow_and_savebow_for_each_set(for_lda_model=for_lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary after prprocessing ist: 54715\n",
      "Size of train set: 11214\n",
      "Size of val set: 100\n",
      "Size of test set: 7532\n",
      "save docs in txt...\n",
      "save docs finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text-after-preprocessing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>biochem nwu edu jackson re swimming pool defen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>goykhman apollo hp com red herring re welcome ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hades coos dartmouth edu brian hughes re insta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jaeger buphy bu edu gregg jaeger re inimitable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sysmgr king eng umd edu doug mohney re boom wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>rcampbel weejordy physics mun ca roderick camp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>jiml strauss ftcollinsco ncr com jim need shar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>sera zuma uucp serdar argic nazi germany armen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>chips astro ocis temple edu charlie mathew bib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>loss ece cmu edu doug loss re crazy or just im...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             text-after-preprocessing\n",
       "0   biochem nwu edu jackson re swimming pool defen...\n",
       "1   goykhman apollo hp com red herring re welcome ...\n",
       "2   hades coos dartmouth edu brian hughes re insta...\n",
       "3   jaeger buphy bu edu gregg jaeger re inimitable...\n",
       "4   sysmgr king eng umd edu doug mohney re boom wh...\n",
       "..                                                ...\n",
       "95  rcampbel weejordy physics mun ca roderick camp...\n",
       "96  jiml strauss ftcollinsco ncr com jim need shar...\n",
       "97  sera zuma uucp serdar argic nazi germany armen...\n",
       "98  chips astro ocis temple edu charlie mathew bib...\n",
       "99  loss ece cmu edu doug loss re crazy or just im...\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kontrollieren die Größen von verschiedenen Datensätzen\n",
    "print(f'Size of the vocabulary after prprocessing ist: {len(textsloader.vocabulary)}')\n",
    "print(f'Size of train set: {len(train_set[\"tokens\"])}')\n",
    "print(f'Size of val set: {len(val_set[\"tokens\"])}')\n",
    "print(f'Size of test set: {len(test_set[\"test\"][\"tokens\"])}')\n",
    "\n",
    "# re-erstellen von Dokumenten nach der Vorverarbeitungen. Die Dokumenten sind in Wörtern und werden für Word-Embedding Training benutzt\n",
    "docs_tr, docs_t, docs_v = textsloader.get_docs_in_words_for_each_set()\n",
    "del docs_t\n",
    "del docs_v\n",
    "train_docs_df = pd.DataFrame()\n",
    "train_docs_df['text-after-preprocessing'] = [' '.join(doc) for doc in docs_tr[:100]]\n",
    "train_docs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordEmbedding mit Skipgramm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Trainieren Word-Embedding \n",
    "2. Speichern in txt File\n",
    "3. Wiederaufrufen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.embedding import WordEmbeddingCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train word-embedding with skipgram\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 48/54715 [00:00<01:56, 468.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vocabulary from word-embedding with skipgram: 54715\n",
      "length of vocabulary after creating BOW: 54715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4290/54715 [00:09<01:34, 533.00it/s]"
     ]
    }
   ],
   "source": [
    "vocab = list(word2id.keys()) # vocabulary after preprocessing and creating bow\n",
    "word2vec_model = 'skipgram'\n",
    "\n",
    "save_path = Path.joinpath(Path.cwd(), f'prepared_data/min_df_{min_df}')\n",
    "figures_path = Path.joinpath(Path.cwd(), f'figures/min_df_{min_df}')\n",
    "Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(figures_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if word2vec_model!=\"bert\":\n",
    "    # only for cbow and skipgram model\n",
    "    wb_creator = WordEmbeddingCreator(model_name=word2vec_model, documents = docs_tr, save_path= save_path)\n",
    "    wb_creator.train(min_count=0, embedding_size= 300)\n",
    "    wb_creator.create_and_save_vocab_embedding(vocab, save_path)\n",
    "del docs_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'word: {vocab[1]} - vector: {list(wb_creator.model.wv.__getitem__(vocab[0]))[:5]} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleichen ähnliche Wörter von Word2Vec (gensim und eigene Cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = vocab[9]\n",
    "vec = list(wb_creator.model.wv.__getitem__(v))\n",
    "print(f'word-embedding of the word-- {v}: ')\n",
    "print(f'dim of vector: {len(vec)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using gensim function\n",
    "wb_creator.find_most_similar_words(n_neighbor=10, word=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using self-implemented cosine\n",
    "sw = wb_creator.find_similar_words_self_implemented(10, vocab, v)\n",
    "df = pd.DataFrame()\n",
    "df['Ähnliches Wort'] = list(sw.keys())\n",
    "df['Cosinus-Ähnlichkeit'] = list(sw.values())\n",
    "print(df.to_latex(index=False))\n",
    "del sw\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del wb_creator\n",
    "del textsloader\n",
    "del word2id\n",
    "del id2word\n",
    "del train_set\n",
    "del test_set\n",
    "del val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleich ähnliche Wörter zwischen Word2Vec und Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only bert_vocab, not embeddings in this file\n",
    "with open('prepared_data/bert_vocab.txt') as f:\n",
    "    lines = f.readlines()\n",
    "readed_bert_vocab = [e.split(\"\\n\")[0] for e in lines]\n",
    "print(len(readed_bert_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.embedding import BertEmbedding\n",
    "bert_eb = BertEmbedding('prepared_data') #directory, where the txt.file of bert_vocab_embedding.txt ist\n",
    "bert_eb.get_bert_embeddings(vocab)\n",
    "print(bert_eb.bert_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my vocabular\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'find similar words for {v}: \\n')\n",
    "sw = bert_eb.find_similar_words(v, 10, vocab)\n",
    "df = pd.DataFrame()\n",
    "df['Ähnliches Wort'] = list(sw.keys())\n",
    "df['Cosinus-Ähnlichkeit'] = list(sw.values())\n",
    "print(df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controll the consitence of vocabulary\n",
    "for w in vocab:\n",
    "    if w not in bert_eb.bert_vocab:\n",
    "        print(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
